# Projects Overview

## Complete Learning Path: 17 Projects

This directory contains all projects for the ML to LLM learning journey, organized into three phases.

## Phase 1: Classical ML Foundation (Projects 1-11 + Bridges)
**Duration**: 9-14 weeks

Foundation of machine learning concepts:

1. **Linear Regression** - Gradient descent, MSE loss
2. **Logistic Regression** - Classification, sigmoid, cross-entropy
3. **Multi-class Classification** - Softmax, one-hot encoding
4. **Regularization** - Overfitting, L1/L2, bias-variance
5. **Decision Trees** - Non-linear models, feature importance
6. **Random Forests** - Ensembles, bagging
7. **Classification Metrics** - Precision, recall, F1, ROC
8. **Cross-Validation** - K-fold, stratification, hyperparameter tuning
9. **SVMs** - Margin maximization, kernels
10. **Feature Engineering** - Scaling, polynomial features
11. **End-to-End Pipeline** - Complete ML workflow
**11.5: Neural Networks from Scratch** ⭐ - Backpropagation, depth, bridge to deep learning
**11.75: RNNs from Scratch** ⭐ - BPTT, vanishing gradients, sequence modeling

## Phase 2: Transformers & Pretraining (Projects 12-15 + Bridges)
**Duration**: 5-7 weeks

Understanding transformer architecture and pretraining:

**12.1: Attention Mechanisms** ⭐ - Attention from scratch, multi-head, causal masking
**12.25: Embeddings** ⭐ - Word2Vec skip-gram, representation learning
12. **Transformer Architecture** - Build from scratch
13. **Tokenization** - BPE, special tokens, data loaders
14. **Pretraining** ⭐ - Train tiny transformer on Shakespeare
15. **Analysis** - Pretrained vs random comparison

## Phase 3: LLM Fine-tuning (Projects 16-17)
**Duration**: 4-6 weeks

Real-world LLM fine-tuning:

16. **Mistral Instruction Tuning** - LoRA fine-tuning with MLX
17. **Comparative Analysis** - Base vs tuned systematic evaluation

## Project Status

Track your progress:

- [ ] Project 1: Linear Regression
- [ ] Project 2: Logistic Regression
- [ ] Project 3: Multi-class Classification
- [ ] Project 4: Regularization
- [ ] Project 5: Decision Trees
- [ ] Project 6: Random Forests
- [ ] Project 7: Classification Metrics
- [ ] Project 8: Cross-Validation
- [ ] Project 9: SVMs
- [ ] Project 10: Feature Engineering
- [ ] Project 11: End-to-End Pipeline
- [ ] Project 11.5: Neural Networks from Scratch (Bridge)
- [ ] Project 11.75: RNNs from Scratch (Bridge)
- [ ] Project 12.1: Attention Mechanisms (Bridge)
- [ ] Project 12.25: Embeddings (Bridge)
- [ ] Project 12: Transformer Architecture
- [ ] Project 13: Tokenization
- [ ] Project 14: Pretraining (Core Project)
- [ ] Project 15: Analysis
- [ ] Project 16: Mistral Tuning
- [ ] Project 17: Comparative Analysis

## Navigation

Each project directory contains:
- Jupyter notebook with starter code and structure
- README (for complex projects)
- Data files (as needed)

## Learning Approach

For each project:
1. Read theoretical background
2. Implement from scratch
3. Visualize and experiment
4. Compare with library implementations
5. Document learnings

## Time Management

Suggested time per project:
- Projects 1-4: 3-5 days each
- Projects 5-11: 3-7 days each
- Project 11.5: 2-3 days (neural networks bridge)
- Project 11.75: 2-3 days (RNN bridge, includes understanding BPTT)
- Project 12.1: 2-3 days (attention bridge)
- Project 12.25: 2-3 days (embeddings bridge)
- Project 12: 1-2 weeks (full transformer assembly)
- Project 13: 3-5 days
- Project 14: 2-3 weeks (includes training time)
- Project 15: 3-5 days
- Projects 16-17: 2-3 weeks each

**Bridge Projects (+2 weeks)**: The four bridges (11.5, 11.75, 12.1, 12.25) add ~2 weeks total but save significant confusion later and make the transformer obvious rather than mysterious.

## Success Metrics

You're on track if:
- ✓ Can explain concepts without notes
- ✓ Code works from scratch (not copy-paste)
- ✓ Can predict parameter changes
- ✓ Can debug by understanding math
- ✓ Documenting insights, not just completing tasks
