# Phase 1: Classical ML Foundation

## Overview
Build foundational ML concepts from first principles before moving to deep learning.

## Projects (Weeks 1-12)

### Projects 1-4: Fundamentals from Scratch
- **Project 1**: Linear Regression - Gradient descent, loss functions
- **Project 2**: Logistic Regression - Classification, sigmoid, cross-entropy
- **Project 3**: Multi-class Classification - Softmax, one-hot encoding
- **Project 4**: Regularization - Overfitting, L1/L2, bias-variance

### Projects 5-7: Standard Algorithms
- **Project 5**: Decision Trees - Non-linear models, feature importance
- **Project 6**: Random Forests - Ensembles, bagging, variance reduction
- **Project 7**: Classification Metrics - Precision, recall, F1, ROC

### Projects 8-11: Advanced Concepts
- **Project 8**: Cross-Validation - K-fold, stratification, hyperparameter tuning
- **Project 9**: SVMs - Margin maximization, kernel methods
- **Project 10**: Feature Engineering - Scaling, polynomial features
- **Project 11**: End-to-End Pipeline - Complete ML workflow

### Project 11.5: Bridge to Deep Learning ⭐
- **Project 11.5**: Neural Networks from Scratch - Backpropagation, depth vs width, why deep learning works
  - Implements a 2-3 layer MLP from NumPy
  - Derives and codes backpropagation
  - Compares to logistic regression on non-linear data
  - Prepares intuition before building transformers
  - ✨ Enhanced: Full exercises on architecture variation, learning rate sensitivity

### Project 11.75: Bridge to Sequences ⭐
- **Project 11.75**: Recurrent Neural Networks from Scratch - BPTT, vanishing gradients
  - Implements vanilla RNN with full backpropagation through time
  - Character-level language modeling on Shakespeare
  - Analyzes gradient flow: eigenvalue analysis of Whh matrix
  - Demonstrates why transformers with attention solve recurrence problem
  - Intuition for sequence modeling before full transformer
  - ✨ Enhanced: Detailed exercises on LSTM implementation, gradient analysis, comparison to transformers

## Learning Outcomes
- Deep understanding of optimization and gradient descent
- Mastery of loss functions and regularization
- Proper evaluation and experimental design
- Foundation for understanding LLM training
- ✨ Hands-on experimentation through structured exercises

## Time Estimate
8-12 weeks of engaged learning (10-20 hours/week)

## Getting Started

1. **Read the intro** in each notebook to understand learning objectives
2. **Run the setup cell** - automatically configures paths and seeds
3. **Work through theory** and implementation cells
4. **Complete the exercises** - scaffold from simple to challenging
5. **Review the summary** - see connection to next project

All exercises have solutions; see suggested approaches in notebook summaries.
