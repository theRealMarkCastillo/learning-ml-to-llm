# Phase 1: Classical ML Foundation

## Overview
Build foundational ML concepts from first principles before moving to deep learning.

## Projects (Weeks 1-12)

### Projects 1-4: Fundamentals from Scratch
- **Project 1**: Linear Regression - Gradient descent, loss functions
- **Project 2**: Logistic Regression - Classification, sigmoid, cross-entropy
- **Project 3**: Multi-class Classification - Softmax, one-hot encoding
- **Project 4**: Regularization - Overfitting, L1/L2, bias-variance

### Projects 5-7: Standard Algorithms
- **Project 5**: Decision Trees - Non-linear models, feature importance
- **Project 6**: Random Forests - Ensembles, bagging, variance reduction
- **Project 7**: Classification Metrics - Precision, recall, F1, ROC

### Projects 8-11: Advanced Concepts
- **Project 8**: Cross-Validation - K-fold, stratification, hyperparameter tuning
- **Project 9**: SVMs - Margin maximization, kernel methods
- **Project 10**: Feature Engineering - Scaling, polynomial features
- **Project 11**: End-to-End Pipeline - Complete ML workflow

## Learning Outcomes
- Deep understanding of optimization and gradient descent
- Mastery of loss functions and regularization
- Proper evaluation and experimental design
- Foundation for understanding LLM training

## Time Estimate
8-12 weeks of engaged learning (10-20 hours/week)
