{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf13efde",
   "metadata": {},
   "source": [
    "# Project 1: Linear Regression from First Principles\n",
    "\n",
    "## Goal\n",
    "Understand gradient descent, loss functions, and parameter updates at the most basic level.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what a loss function is and why MSE\n",
    "- How gradient descent works mathematically\n",
    "- What is a learning rate and how it affects convergence\n",
    "- What does an epoch mean\n",
    "- How to know when to stop training\n",
    "\n",
    "## Date Started\n",
    "November 8, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445642f",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Theoretical Foundation\n",
    "\n",
    "### Linear Regression Model\n",
    "The linear regression model predicts output $y$ from input $x$ using:\n",
    "\n",
    "$$\\hat{y} = w \\cdot x + b$$\n",
    "\n",
    "Where:\n",
    "- $w$ = weight (slope)\n",
    "- $b$ = bias (intercept)\n",
    "- $\\hat{y}$ = predicted value\n",
    "\n",
    "### Loss Function: Mean Squared Error (MSE)\n",
    "We measure how well our model fits the data using MSE:\n",
    "\n",
    "$$L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Why MSE?** It penalizes larger errors more heavily and is differentiable.\n",
    "\n",
    "### Gradient Descent\n",
    "To minimize the loss, we update parameters in the direction that reduces loss:\n",
    "\n",
    "$$w := w - \\alpha \\frac{\\partial L}{\\partial w}$$\n",
    "$$b := b - \\alpha \\frac{\\partial L}{\\partial b}$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (step size)\n",
    "- $\\frac{\\partial L}{\\partial w}$ = gradient of loss with respect to weight\n",
    "\n",
    "### Gradients for Linear Regression\n",
    "$$\\frac{\\partial L}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i(y_i - \\hat{y}_i)$$\n",
    "$$\\frac{\\partial L}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f345b",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Setup and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a497d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/Users/mark/git/learning-ml-to-llm')\n",
    "\n",
    "from utils.data_generators import generate_linear_data\n",
    "from utils.visualization import plot_loss_curve, plot_regression_line\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fe84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "# True relationship: y = 4 + 3x + noise\n",
    "\n",
    "TRUE_SLOPE = 3.0\n",
    "TRUE_INTERCEPT = 4.0\n",
    "N_SAMPLES = 100\n",
    "NOISE_STD = 1.0\n",
    "\n",
    "X, y = generate_linear_data(\n",
    "    n_samples=N_SAMPLES,\n",
    "    slope=TRUE_SLOPE,\n",
    "    intercept=TRUE_INTERCEPT,\n",
    "    noise_std=NOISE_STD,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {N_SAMPLES} data points\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"\\nTrue parameters:\")\n",
    "print(f\"  Slope (w): {TRUE_SLOPE}\")\n",
    "print(f\"  Intercept (b): {TRUE_INTERCEPT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63843586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, alpha=0.6, s=50, color='blue', label='Data Points')\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Synthetic Linear Data with Noise', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData Statistics:\")\n",
    "print(f\"X - Mean: {X.mean():.2f}, Std: {X.std():.2f}, Range: [{X.min():.2f}, {X.max():.2f}]\")\n",
    "print(f\"y - Mean: {y.mean():.2f}, Std: {y.std():.2f}, Range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ea0af",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Implement Linear Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionFromScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from first principles using gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"\n",
    "        Initialize the model\n",
    "        \n",
    "        Args:\n",
    "            learning_rate: Step size for gradient descent\n",
    "            n_iterations: Number of training iterations\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.loss_history = []\n",
    "        self.weight_history = []\n",
    "        self.bias_history = []\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error loss\n",
    "        \n",
    "        Args:\n",
    "            y_true: True values\n",
    "            y_pred: Predicted values\n",
    "        \n",
    "        Returns:\n",
    "            MSE loss value\n",
    "        \"\"\"\n",
    "        n = len(y_true)\n",
    "        loss = (1 / n) * np.sum((y_true - y_pred) ** 2)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_gradients(self, X, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute gradients of loss with respect to weight and bias\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "            y_true: True values\n",
    "            y_pred: Predicted values\n",
    "        \n",
    "        Returns:\n",
    "            dw: Gradient with respect to weight\n",
    "            db: Gradient with respect to bias\n",
    "        \"\"\"\n",
    "        n = len(y_true)\n",
    "        \n",
    "        # Compute gradients\n",
    "        error = y_true - y_pred\n",
    "        dw = -(2 / n) * np.sum(X * error)\n",
    "        db = -(2 / n) * np.sum(error)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using current parameters\n",
    "        \n",
    "        Args:\n",
    "            X: Input features\n",
    "        \n",
    "        Returns:\n",
    "            Predictions\n",
    "        \"\"\"\n",
    "        return self.weight * X + self.bias\n",
    "    \n",
    "    def fit(self, X, y, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent\n",
    "        \n",
    "        Args:\n",
    "            X: Training features\n",
    "            y: Training targets\n",
    "            verbose: Whether to print progress\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Initialize parameters randomly\n",
    "        self.weight = np.random.randn()\n",
    "        self.bias = np.random.randn()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Initial parameters: w={self.weight:.4f}, b={self.bias:.4f}\")\n",
    "        \n",
    "        # Gradient descent loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Forward pass: compute predictions\n",
    "            y_pred = self.predict(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self._compute_loss(y, y_pred)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw, db = self._compute_gradients(X, y, y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weight -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Store parameter history\n",
    "            self.weight_history.append(self.weight)\n",
    "            self.bias_history.append(self.bias)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (iteration % 100 == 0 or iteration == self.n_iterations - 1):\n",
    "                print(f\"Iteration {iteration:4d}: Loss={loss:.4f}, w={self.weight:.4f}, b={self.bias:.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTraining complete!\")\n",
    "            print(f\"Final parameters: w={self.weight:.4f}, b={self.bias:.4f}\")\n",
    "\n",
    "print(\"LinearRegressionFromScratch class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759ee2e3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df433a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "LEARNING_RATE = 0.1\n",
    "N_ITERATIONS = 1000\n",
    "\n",
    "model = LinearRegressionFromScratch(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_iterations=N_ITERATIONS\n",
    ")\n",
    "\n",
    "print(f\"Training Linear Regression with:\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Iterations: {N_ITERATIONS}\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "model.fit(X, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018100b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare learned parameters with true parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PARAMETER COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSlope (weight):\")\n",
    "print(f\"  True value:    {TRUE_SLOPE:.4f}\")\n",
    "print(f\"  Learned value: {model.weight:.4f}\")\n",
    "print(f\"  Error:         {abs(TRUE_SLOPE - model.weight):.4f}\")\n",
    "\n",
    "print(f\"\\nIntercept (bias):\")\n",
    "print(f\"  True value:    {TRUE_INTERCEPT:.4f}\")\n",
    "print(f\"  Learned value: {model.bias:.4f}\")\n",
    "print(f\"  Error:         {abs(TRUE_INTERCEPT - model.bias):.4f}\")\n",
    "\n",
    "# Calculate final loss\n",
    "final_predictions = model.predict(X)\n",
    "final_loss = model._compute_loss(y, final_predictions)\n",
    "print(f\"\\nFinal Loss (MSE): {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0d263",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Visualize Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a50de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curve\n",
    "plot_loss_curve(\n",
    "    model.loss_history,\n",
    "    title=\"Training Loss Over Iterations\",\n",
    "    xlabel=\"Iteration\",\n",
    "    ylabel=\"Mean Squared Error\"\n",
    ")\n",
    "\n",
    "print(f\"Initial Loss: {model.loss_history[0]:.4f}\")\n",
    "print(f\"Final Loss: {model.loss_history[-1]:.4f}\")\n",
    "print(f\"Loss Reduction: {(1 - model.loss_history[-1]/model.loss_history[0]) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ff809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learned regression line\n",
    "predictions = model.predict(X)\n",
    "\n",
    "plot_regression_line(\n",
    "    X, y, predictions,\n",
    "    title=f\"Linear Regression Fit (w={model.weight:.2f}, b={model.bias:.2f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameter trajectory (how parameters changed during training)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Weight trajectory\n",
    "ax1.plot(model.weight_history, linewidth=2, color='blue')\n",
    "ax1.axhline(y=TRUE_SLOPE, color='red', linestyle='--', linewidth=2, label=f'True value ({TRUE_SLOPE})')\n",
    "ax1.set_xlabel('Iteration', fontsize=12)\n",
    "ax1.set_ylabel('Weight Value', fontsize=12)\n",
    "ax1.set_title('Weight Trajectory During Training', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Bias trajectory\n",
    "ax2.plot(model.bias_history, linewidth=2, color='green')\n",
    "ax2.axhline(y=TRUE_INTERCEPT, color='red', linestyle='--', linewidth=2, label=f'True value ({TRUE_INTERCEPT})')\n",
    "ax2.set_xlabel('Iteration', fontsize=12)\n",
    "ax2.set_ylabel('Bias Value', fontsize=12)\n",
    "ax2.set_title('Bias Trajectory During Training', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observe how the parameters converge to their true values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea48b52",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Experiment with Different Learning Rates\n",
    "\n",
    "**Key Question:** How does the learning rate affect convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "models = {}\n",
    "\n",
    "print(\"Training models with different learning rates...\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Learning Rate: {lr}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model_temp = LinearRegressionFromScratch(\n",
    "        learning_rate=lr,\n",
    "        n_iterations=1000\n",
    "    )\n",
    "    model_temp.fit(X, y, verbose=False)\n",
    "    \n",
    "    models[lr] = model_temp\n",
    "    \n",
    "    print(f\"  Final Loss: {model_temp.loss_history[-1]:.4f}\")\n",
    "    print(f\"  Final w: {model_temp.weight:.4f}\")\n",
    "    print(f\"  Final b: {model_temp.bias:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loss curves for different learning rates\n",
    "from utils.visualization import plot_learning_rate_comparison\n",
    "\n",
    "losses_dict = {lr: model.loss_history for lr, model in models.items()}\n",
    "\n",
    "plot_learning_rate_comparison(learning_rates, losses_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2234772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LEARNING RATE EFFECTS ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model_temp = models[lr]\n",
    "    initial_loss = model_temp.loss_history[0]\n",
    "    final_loss = model_temp.loss_history[-1]\n",
    "    \n",
    "    print(f\"\\nLearning Rate: {lr}\")\n",
    "    print(f\"  Initial Loss:    {initial_loss:.4f}\")\n",
    "    print(f\"  Final Loss:      {final_loss:.4f}\")\n",
    "    print(f\"  Loss Reduction:  {(1 - final_loss/initial_loss) * 100:.2f}%\")\n",
    "    print(f\"  Weight Error:    {abs(TRUE_SLOPE - model_temp.weight):.4f}\")\n",
    "    print(f\"  Bias Error:      {abs(TRUE_INTERCEPT - model_temp.bias):.4f}\")\n",
    "    \n",
    "    # Check convergence\n",
    "    if len(model_temp.loss_history) > 100:\n",
    "        last_100_change = abs(model_temp.loss_history[-1] - model_temp.loss_history[-100])\n",
    "        if last_100_change < 0.001:\n",
    "            print(f\"  Status: ✓ Converged\")\n",
    "        else:\n",
    "            print(f\"  Status: ⚠ Still changing (Δ={last_100_change:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb861e",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Compare with sklearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ce06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with sklearn's LinearRegression\n",
    "from sklearn.linear_model import LinearRegression as SklearnLR\n",
    "\n",
    "sklearn_model = SklearnLR()\n",
    "sklearn_model.fit(X, y)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: Our Implementation vs sklearn\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nSlope (weight):\")\n",
    "print(f\"  Our model:     {model.weight:.6f}\")\n",
    "print(f\"  sklearn:       {sklearn_model.coef_[0][0]:.6f}\")\n",
    "print(f\"  Difference:    {abs(model.weight - sklearn_model.coef_[0][0]):.6f}\")\n",
    "\n",
    "print(f\"\\nIntercept (bias):\")\n",
    "print(f\"  Our model:     {model.bias:.6f}\")\n",
    "print(f\"  sklearn:       {sklearn_model.intercept_[0]:.6f}\")\n",
    "print(f\"  Difference:    {abs(model.bias - sklearn_model.intercept_[0]):.6f}\")\n",
    "\n",
    "# Compare predictions\n",
    "our_predictions = model.predict(X)\n",
    "sklearn_predictions = sklearn_model.predict(X)\n",
    "\n",
    "prediction_diff = np.mean(np.abs(our_predictions - sklearn_predictions))\n",
    "print(f\"\\nMean Absolute Difference in Predictions: {prediction_diff:.6f}\")\n",
    "\n",
    "if prediction_diff < 0.01:\n",
    "    print(\"\\n✓ Our implementation matches sklearn very closely!\")\n",
    "else:\n",
    "    print(\"\\n⚠ Some difference exists (this is normal due to different optimization methods)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc135d",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Key Learnings and Reflections\n",
    "\n",
    "### What I Learned\n",
    "\n",
    "1. **Loss Function (MSE)**\n",
    "   - Measures prediction error\n",
    "   - Differentiable (needed for gradient descent)\n",
    "   - Penalizes large errors more\n",
    "\n",
    "2. **Gradient Descent**\n",
    "   - Iteratively updates parameters\n",
    "   - Moves in direction of steepest descent\n",
    "   - Requires tuning learning rate\n",
    "\n",
    "3. **Learning Rate Effects**\n",
    "   - Too small: slow convergence\n",
    "   - Too large: oscillation or divergence\n",
    "   - Sweet spot: steady decrease in loss\n",
    "\n",
    "4. **Convergence**\n",
    "   - Loss plateaus when parameters are near optimal\n",
    "   - Can monitor gradient magnitude\n",
    "   - Early stopping possible\n",
    "\n",
    "### Connections to Future Projects\n",
    "\n",
    "- **Same gradient descent** will be used in neural networks and transformers\n",
    "- **Same loss monitoring** applies to all ML training\n",
    "- **Learning rate tuning** is critical in deep learning\n",
    "- **Parameter updates** are the core of all optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7391a68",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: Next Steps\n",
    "\n",
    "### Experiments to Try\n",
    "- [ ] Add more features (multi-dimensional regression)\n",
    "- [ ] Implement mini-batch gradient descent\n",
    "- [ ] Add momentum to gradient descent\n",
    "- [ ] Try adaptive learning rates\n",
    "- [ ] Test on real datasets\n",
    "\n",
    "### Move to Project 2\n",
    "Next: **Binary Classification with Logistic Regression**\n",
    "- Different loss function (cross-entropy)\n",
    "- Sigmoid activation\n",
    "- Classification metrics"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
