# Phase 3: LLM Fine-tuning

## Overview
Fine-tune Mistral 7B and analyze how instruction tuning changes model behavior.

## Projects (Weeks 18-23)

### Project 16: Mistral Instruction Tuning (Week 18-21)
Fine-tune real Mistral 7B using MLX:
- LoRA parameter-efficient fine-tuning
- Instruction dataset preparation
- Training loop for production model
- Before/after evaluation
- **Memory**: ~20-30GB on your M4
- **Time**: Hours per training run

### Project 17: Comparative Analysis (Week 22-23)
Systematic comparison of base vs tuned:
- Instruction-following quality
- General capability preservation
- Attention pattern differences
- Failure mode analysis
- Research-grade documentation

## Learning Outcomes
- Hands-on LLM fine-tuning experience
- Understanding of LoRA and efficient training
- Rigorous model evaluation skills
- Connection to AI safety research

## Hardware Requirements
- M4 Mac with 64GB RAM: âœ“ Perfect
- Memory usage: ~20-30GB peak
- Training time: Hours per run
- MLX optimized for Apple silicon

## Connection to Research
This phase directly supports AI safety research:
- Understanding how fine-tuning changes behavior
- Analyzing model responses to edge cases
- Documenting epistemic properties
- Systematic evaluation methodology

## Time Estimate
4-6 weeks of intensive work
