{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be990dfa",
   "metadata": {},
   "source": [
    "# Project 17: Comparative Analysis — Base vs Instruction-Tuned Mistral\n",
    "\n",
    "## Goal\n",
    "Rigorously compare base Mistral with the instruction-tuned version from Project 16. Measure instruction-following improvement, capability preservation, and identify when fine-tuning helps vs. hurts.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load base + tuned models; generate on same prompts to compare qualitatively\n",
    "- Define evaluation metrics: length, instruction keyword coverage, specificity, accuracy (when applicable)\n",
    "- Analyze instruction-following improvements with concrete examples\n",
    "- Identify failure modes: catastrophic forgetting, overfitting to training domain\n",
    "- Measure general capability preservation (does tuning break general knowledge?)\n",
    "- Propose best practices for fine-tuning LLMs\n",
    "\n",
    "## Prerequisites\n",
    "- Project 14 (Pretraining): Understand loss metrics and model evaluation\n",
    "- Project 15 (Analysis): Know how to compare models systematically\n",
    "- Project 16 (Mistral Fine-Tuning): Have a tuned checkpoint to compare against\n",
    "\n",
    "## What You'll Build\n",
    "- Comparative evaluation framework: load base + tuned, generate on benchmark\n",
    "- Heuristic metrics: response length, keyword coverage, instruction adherence\n",
    "- Qualitative analysis: side-by-side examples of base vs. tuned outputs\n",
    "- Capability preservation checks: do general knowledge tasks work?\n",
    "- Failure analysis: when does tuning hurt performance?\n",
    "- Report: summary statistics, visualizations, recommendations\n",
    "\n",
    "## Estimated Time\n",
    "- Setup + loading: 15-30 min\n",
    "- Evaluation on benchmark: 30-60 min\n",
    "- Analysis + visualization: 30-45 min\n",
    "- Deep dives (optional): 1-2 hours\n",
    "\n",
    "## Usage Guide\n",
    "\n",
    "This notebook:\n",
    "1. Sets up model loading with graceful handling of missing adapters\n",
    "2. Loads base Mistral and tuned version (if available)\n",
    "3. Generates responses on a benchmark prompt set\n",
    "4. Computes heuristic metrics (length, keyword coverage, etc.)\n",
    "5. Creates comparison tables and visualizations\n",
    "6. Provides qualitative analysis framework\n",
    "7. Saves results to CSV and JSON\n",
    "\n",
    "Key functions:\n",
    "- `load_base_model()` / `load_tuned_model()` → safe model loading\n",
    "- `generate_response(model, prompt)` → generate and measure\n",
    "- `compute_metrics(response, prompt)` → length, keywords, etc.\n",
    "- `compare_models()` → run full evaluation on benchmark\n",
    "- `visualize_comparison()` → plot metrics side-by-side\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f0f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for comparative analysis!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Ready for comparative analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363773c7",
   "metadata": {},
   "source": [
    "## Plan\n",
    "We will:\n",
    "1. Define configuration (model names, adapter paths, prompt set, quick mode toggle).\n",
    "2. Provide helper loaders to gracefully handle missing tuned adapters.\n",
    "3. Load base model; attempt to load tuned model + merge LoRA if present.\n",
    "4. Generate responses for a benchmark prompt set.\n",
    "5. Compute simple heuristic metrics (length, overlap, instruction keyword coverage).\n",
    "6. Tabulate and save results (CSV + JSON metrics).\n",
    "7. Summarize differences and propose further evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c03e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready:\n",
      "  quick_mode: True\n",
      "  model_name: mlx-community/Qwen2.5-0.5B-Instruct\n",
      "  tuned_adapters exist: False\n",
      "  analysis_dir: /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts\n"
     ]
    }
   ],
   "source": [
    "# 1) Configuration\n",
    "from pathlib import Path\n",
    "import json, time, math\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Quick mode avoids large downloads by preferring a smaller model for smoke tests\n",
    "quick_mode = True  # set False to use the full Mistral model (may be multi-GB download)\n",
    "small_model_name = 'mlx-community/Qwen2.5-0.5B-Instruct'  # fallback small model\n",
    "full_model_name = 'mistralai/Mistral-7B-Instruct-v0.2'     # preferred full model\n",
    "model_name = small_model_name if quick_mode else full_model_name\n",
    "max_new_tokens = 128\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "\n",
    "# Locate repo root and artifacts\n",
    "repo_root = None\n",
    "cur = Path.cwd()\n",
    "for parent in [cur] + list(cur.parents):\n",
    "    if (parent / 'requirements.txt').exists() or (parent / '.git').exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "if repo_root is None:\n",
    "    repo_root = cur\n",
    "project16_artifacts = repo_root / 'projects' / 'phase3_llm_tuning' / 'project16_mistral_tuning' / 'artifacts'\n",
    "tuned_adapters_path = project16_artifacts / 'lora_adapters.safetensors'\n",
    "project17_dir = repo_root / 'projects' / 'phase3_llm_tuning' / 'project17_comparative_analysis'\n",
    "analysis_dir = project17_dir / 'analysis_artifacts'\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prompt set for head-to-head comparison\n",
    "prompts = [\n",
    "    \"Explain LoRA in one paragraph.\",\n",
    "    \"Write a Python function to compute Fibonacci numbers.\",\n",
    "    \"Give three bullet tips for learning ML.\",\n",
    "    \"Summarize the benefits of parameter-efficient fine-tuning.\",\n",
    "    \"What are common failure modes when instruction-tuning LLMs?\",\n",
    "]\n",
    "\n",
    "print('Config ready:')\n",
    "print('  quick_mode:', quick_mode)\n",
    "print('  model_name:', model_name)\n",
    "print('  tuned_adapters exist:', tuned_adapters_path.exists())\n",
    "print('  analysis_dir:', analysis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ef03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Helper functions for safe loading and generation\n",
    "from typing import Tuple, Dict, Any\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "def safe_load(model_id: str):\n",
    "    \"\"\"Load a model/tokenizer with graceful error handling.\"\"\"\n",
    "    try:\n",
    "        m, tok = load(model_id)\n",
    "        return m, tok\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load {model_id}:', repr(e))\n",
    "        return None, None\n",
    "\n",
    "def try_load_tuned(base_model_id: str, adapter_path: Path):\n",
    "    \"\"\"Attempt to load base model then apply LoRA adapters if available.\"\"\"\n",
    "    m, tok = safe_load(base_model_id)\n",
    "    if m is None:\n",
    "        return None, None\n",
    "    # Attempt adapter merge if file exists and peft utilities are present\n",
    "    if adapter_path.exists():\n",
    "        try:\n",
    "            from mlx_lm.peft import apply_lora, merge_lora_weights, load_lora_parameters, LoraConfig\n",
    "            # We don't know original config used; we load saved weights directly\n",
    "            load_lora_parameters(m, str(adapter_path))\n",
    "            tuned_model = merge_lora_weights(m)\n",
    "            print('Merged LoRA adapters into tuned model.')\n",
    "            return tuned_model, tok\n",
    "        except Exception as e:\n",
    "            print('Could not load/merge LoRA adapters:', repr(e))\n",
    "            print('Proceeding with base model as tuned fallback.')\n",
    "            return m, tok\n",
    "    else:\n",
    "        print('Adapter file not found; using base model as tuned placeholder.')\n",
    "        return m, tok\n",
    "\n",
    "def safe_generate(m, tok, prompt: str, **gen_kwargs):\n",
    "    if m is None or tok is None:\n",
    "        return '<model unavailable>'\n",
    "    try:\n",
    "        return generate(m, tok, prompt=prompt, **gen_kwargs)\n",
    "    except Exception as e:\n",
    "        return f'<generation error: {repr(e)}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31554686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready:\n",
      "  base: skipped\n",
      "  tuned: skipped | mode: quick_mode_skipped_load\n"
     ]
    }
   ],
   "source": [
    "# 3) Load base and tuned models (may download on first run)\n",
    "if quick_mode:\n",
    "    base_model, base_tok = None, None  # Skip heavy downloads in quick_mode\n",
    "    tuned_model, tuned_tok = None, None\n",
    "    tuned_kind = 'quick_mode_skipped_load'\n",
    "else:\n",
    "    base_model, base_tok = safe_load(model_name)\n",
    "    tuned_model, tuned_tok = try_load_tuned(full_model_name, tuned_adapters_path)\n",
    "    tuned_kind = 'adapters_merged' if tuned_model is not None else 'fallback_base'\n",
    "\n",
    "print('Models ready:')\n",
    "print('  base:', 'ok' if base_model is not None else 'skipped')\n",
    "print('  tuned:', 'ok' if tuned_model is not None else 'skipped', '| mode:', tuned_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7689d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs to /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts/base_vs_tuned_outputs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>base_output</th>\n",
       "      <th>tuned_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explain LoRA in one paragraph.</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Write a Python function to compute Fibonacci n...</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt          base_output  \\\n",
       "0   1                     Explain LoRA in one paragraph.  <model unavailable>   \n",
       "1   2  Write a Python function to compute Fibonacci n...  <model unavailable>   \n",
       "\n",
       "          tuned_output  \n",
       "0  <model unavailable>  \n",
       "1  <model unavailable>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Run generations and build comparison table\n",
    "rows = []\n",
    "for i, p in enumerate(prompts, start=1):\n",
    "    b_out = safe_generate(base_model, base_tok, prompt=p, max_tokens=max_new_tokens, verbose=False)\n",
    "    t_out = safe_generate(tuned_model, tuned_tok, prompt=p, max_tokens=max_new_tokens, verbose=False)\n",
    "    rows.append({'id': i, 'prompt': p, 'base_output': b_out, 'tuned_output': t_out})\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = analysis_dir / 'base_vs_tuned_outputs.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print('Saved outputs to', csv_path)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377530fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts/heuristic_metrics.csv\n",
      "Saved summary to /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts/summary_metrics.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>base_len</th>\n",
       "      <th>tuned_len</th>\n",
       "      <th>base_keyword_coverage</th>\n",
       "      <th>tuned_keyword_coverage</th>\n",
       "      <th>len_delta</th>\n",
       "      <th>coverage_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explain LoRA in one paragraph.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Write a Python function to compute Fibonacci n...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt  base_len  tuned_len  \\\n",
       "0   1                     Explain LoRA in one paragraph.         2          2   \n",
       "1   2  Write a Python function to compute Fibonacci n...         2          2   \n",
       "\n",
       "   base_keyword_coverage  tuned_keyword_coverage  len_delta  coverage_delta  \n",
       "0                    0.0                     0.0          0             0.0  \n",
       "1                    0.0                     0.0          0             0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Simple heuristic metrics\n",
    "import re\n",
    "def keyword_coverage(text: str, prompt: str):\n",
    "    # Percentage of unique prompt words (alphabetic) appearing in output\n",
    "    pw = {w.lower() for w in re.findall(r'[A-Za-z]+', prompt) if len(w) > 3}\n",
    "    if not pw:\n",
    "        return 0.0\n",
    "    ow = {w.lower() for w in re.findall(r'[A-Za-z]+', text)}\n",
    "    return len(pw & ow) / len(pw)\n",
    "\n",
    "metrics = []\n",
    "for r in rows:\n",
    "    base_len = len(r['base_output'].split()) if isinstance(r['base_output'], str) else 0\n",
    "    tuned_len = len(r['tuned_output'].split()) if isinstance(r['tuned_output'], str) else 0\n",
    "    base_cov = keyword_coverage(r['base_output'], r['prompt']) if isinstance(r['base_output'], str) else 0.0\n",
    "    tuned_cov = keyword_coverage(r['tuned_output'], r['prompt']) if isinstance(r['tuned_output'], str) else 0.0\n",
    "    metrics.append({\n",
    "        'id': r['id'],\n",
    "        'prompt': r['prompt'],\n",
    "        'base_len': base_len,\n",
    "        'tuned_len': tuned_len,\n",
    "        'base_keyword_coverage': base_cov,\n",
    "        'tuned_keyword_coverage': tuned_cov,\n",
    "        'len_delta': tuned_len - base_len,\n",
    "        'coverage_delta': tuned_cov - base_cov,\n",
    "    })\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_path = analysis_dir / 'heuristic_metrics.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "summary = {\n",
    "    'avg_len_delta': float(metrics_df['len_delta'].mean()),\n",
    "    'avg_cov_delta': float(metrics_df['coverage_delta'].mean()),\n",
    "    'prompts': len(metrics_df),\n",
    "    'quick_mode': quick_mode,\n",
    "    'tuned_kind': tuned_kind,\n",
    "}\n",
    "summary_path = analysis_dir / 'summary_metrics.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('Saved metrics to', metrics_path)\n",
    "print('Saved summary to', summary_path)\n",
    "metrics_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b24bc7d",
   "metadata": {},
   "source": [
    "# Exercises & Extensions\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "1. **Baseline Metrics**: Generate responses from base model on 5 prompts. Compute length, keyword coverage, specificity. Are there patterns? (Do some prompts get longer responses?)\n",
    "2. **Tuned vs. Base Side-by-Side**: Pick 3 prompts. Show base output next to tuned output. Qualitatively: is tuned more instruction-following? More specific?\n",
    "3. **Metric Correlation**: Compute correlation between response length and keyword coverage. Are they independent metrics or correlated?\n",
    "\n",
    "## Intermediate\n",
    "\n",
    "4. **Out-of-Domain Generalization**: Test on prompts NOT in fine-tuning dataset (e.g., if tuned on Q&A, test on creative writing). Does tuning hurt general capabilities? Plot performance on in-domain vs. out-of-domain.\n",
    "5. **Response Distribution**: Generate 10 responses per prompt using sampling (temperature > 0.7). Measure variance. Is tuned model more consistent? More diverse?\n",
    "6. **Failure Mode Analysis**: Find prompts where base > tuned (tuned is worse). What domain are these? Do they indicate catastrophic forgetting?\n",
    "\n",
    "## Advanced\n",
    "\n",
    "7. **Benchmark Evaluation**: Use a real benchmark (e.g., GSM8K for math, HumanEval for code). Score both models. Compute Δ performance. On what tasks does tuning help most?\n",
    "8. **Scaling Comparison**: Create multiple tuned models with different LoRA ranks (8, 16, 32, 64). Compare all on the benchmark. Plot performance vs. compute cost.\n",
    "9. **Human Preference Study**: Have 2-3 people rate base vs. tuned outputs on instruction-adherence (scale 1-5) on 20 prompts. Compute inter-rater agreement + average preference.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary & Bridge Forward\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- **Instruction-Following Improvement**: Fine-tuning typically makes models more helpful and specific to user instructions\n",
    "- **Trade-offs**: Tuning can hurt general capabilities if not careful (data imbalance, overfitting to narrow domain)\n",
    "- **Measurement Challenges**: Hard metrics (exact match, code execution) are better than heuristics (keyword coverage)\n",
    "- **Domain Specificity**: Models tuned on one domain often struggle on others\n",
    "- **Capability Preservation**: Need evaluation on both tuned domain AND held-out general tasks\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Fine-tuning evaluation determines **whether a model is ready for production**:\n",
    "\n",
    "1. **Quality Assurance**:\n",
    "   - Fine-tuning should improve target domain performance\n",
    "   - Must NOT degrade general knowledge\n",
    "   - Need benchmarks to catch regressions\n",
    "\n",
    "2. **Business Metrics**:\n",
    "   - Instruction-following → user satisfaction\n",
    "   - General preservation → reliability\n",
    "   - Inference speed → cost\n",
    "   - These need to align with product requirements\n",
    "\n",
    "3. **Science**:\n",
    "   - When does tuning help? (structured data, ample examples)\n",
    "   - When does it hurt? (small dataset, domain drift, overfitting)\n",
    "   - How much tuning is enough? (learning curves)\n",
    "\n",
    "## Bridge to Next Projects\n",
    "\n",
    "This is the **end of the learning path**, but here's how to continue:\n",
    "\n",
    "- **Production Deployment**:\n",
    "  - Serve tuned model via API (LM Studio, vLLM, SGLang)\n",
    "  - Monitor outputs for drift\n",
    "  - Collect user feedback and retune periodically\n",
    "\n",
    "- **Research Directions**:\n",
    "  - Scaling laws: tune increasingly large models\n",
    "  - Data efficiency: how little data needed for good tuning?\n",
    "  - Alignment: how to tune for safety + performance?\n",
    "  - Multi-task: can single model tune to multiple domains?\n",
    "\n",
    "- **Advanced Techniques**:\n",
    "  - DPO (Direct Preference Optimization): tune from human preferences\n",
    "  - RLHF (Reinforcement Learning from Human Feedback): scale preferences to tasks\n",
    "  - Mixture of Experts: specialize different components to different domains\n",
    "\n",
    "## Your Takeaway\n",
    "\n",
    "> **Fine-tuning is where research meets practice.** Pretraining learns general language; fine-tuning adapts to specific tasks. Good evaluation ensures fine-tuning improves targets without breaking generalization. This is how production LLM systems are built.\n",
    "\n",
    "---\n",
    "\n",
    "# Performance Notes\n",
    "\n",
    "- **Instruction-Following**: Typically 20-50% improvement on target domain with 1000+ instruction examples\n",
    "- **General Capability**: Small regressions (1-2% loss) are acceptable; >10% loss indicates overfitting\n",
    "- **Data Requirements**: 100-1000 examples for noticeable improvement; 10000+ for substantial change\n",
    "- **Tuning Convergence**: 1-5 epochs typical; more epochs = risk of overfitting\n",
    "- **Inference Speed**: Merged tuned model has same speed as base (LoRA just changes weights)\n",
    "- **Cost/Benefit**: Fine-tuning cost (hours on 1 GPU) << Pretraining cost (weeks on clusters); clear ROI\n",
    "\n",
    "---\n",
    "\n",
    "# Curriculum Summary: From Classical ML to LLMs\n",
    "\n",
    "**Phase 1: Classical ML Foundations** (Projects 1-11.75)\n",
    "- Linear regression → logistic regression → neural networks → RNNs\n",
    "- Learn optimization, backpropagation, and sequence modeling\n",
    "- Understand the vanishing gradient problem\n",
    "\n",
    "**Phase 2: Transformers & Pretraining** (Projects 12-15)\n",
    "- Attention mechanisms → embeddings → full transformer\n",
    "- Pretraining on character-level text\n",
    "- Analyze what models learn via pretraining\n",
    "\n",
    "**Phase 3: LLM Fine-Tuning** (Projects 16-17)\n",
    "- Fine-tune production models (Mistral 7B)\n",
    "- Compare base vs. tuned systematically\n",
    "- Understand trade-offs and deployment considerations\n",
    "\n",
    "**Key Insights**:\n",
    "1. Modern LLMs are scaled transformers + massive pretraining + careful fine-tuning\n",
    "2. Attention replaced recurrence; parallelization unlocked scaling\n",
    "3. Pretraining is expensive but essential; fine-tuning is cheap and practical\n",
    "4. Evaluation is critical: good metrics catch regressions before production\n",
    "\n",
    "**Next Steps**:\n",
    "- Deploy a fine-tuned model as an API\n",
    "- Evaluate on real-world benchmarks (MMLU, HumanEval, etc.)\n",
    "- Experiment with advanced techniques (DPO, RLHF, MoE)\n",
    "- Build applications on top of fine-tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904e84e",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "- This notebook compares base vs tuned outputs on a small prompt set and records heuristic metrics (length and prompt keyword coverage).\n",
    "- In quick_mode, tuned==base to validate the pipeline without heavy downloads. Disable quick_mode to use the full model and load adapters.\n",
    "- If adapters are missing, generate them by running Project 16 with `DRY_RUN=False` and saving LoRA adapters.\n",
    "\n",
    "Next improvements:\n",
    "- Add a richer evaluation set and human preference judgments.\n",
    "- Include task-specific checks (e.g., code execution tests for coding prompts).\n",
    "- Plot distributions and deltas; compute statistical significance over larger samples.\n",
    "- Optionally compute log-prob scores with the model for stronger comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
