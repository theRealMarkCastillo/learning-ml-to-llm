{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be990dfa",
   "metadata": {},
   "source": [
    "# Project 17: Comparative Analysis - Base vs Instruction-Tuned\n",
    "\n",
    "## Goal\n",
    "Systematically compare Mistral base vs your tuned version.\n",
    "\n",
    "## Learning Objectives\n",
    "- Instruction-following improvements\n",
    "- General capability preservation\n",
    "- Attention pattern changes\n",
    "- Failure mode analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88f0f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for comparative analysis!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Ready for comparative analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363773c7",
   "metadata": {},
   "source": [
    "## Plan\n",
    "We will:\n",
    "1. Define configuration (model names, adapter paths, prompt set, quick mode toggle).\n",
    "2. Provide helper loaders to gracefully handle missing tuned adapters.\n",
    "3. Load base model; attempt to load tuned model + merge LoRA if present.\n",
    "4. Generate responses for a benchmark prompt set.\n",
    "5. Compute simple heuristic metrics (length, overlap, instruction keyword coverage).\n",
    "6. Tabulate and save results (CSV + JSON metrics).\n",
    "7. Summarize differences and propose further evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c03e5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready:\n",
      "  quick_mode: True\n",
      "  model_name: mlx-community/Qwen2.5-0.5B-Instruct\n",
      "  tuned_adapters exist: False\n",
      "  analysis_dir: /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts\n"
     ]
    }
   ],
   "source": [
    "# 1) Configuration\n",
    "from pathlib import Path\n",
    "import json, time, math\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Quick mode avoids large downloads by preferring a smaller model for smoke tests\n",
    "quick_mode = True  # set False to use the full Mistral model (may be multi-GB download)\n",
    "small_model_name = 'mlx-community/Qwen2.5-0.5B-Instruct'  # fallback small model\n",
    "full_model_name = 'mistralai/Mistral-7B-Instruct-v0.2'     # preferred full model\n",
    "model_name = small_model_name if quick_mode else full_model_name\n",
    "max_new_tokens = 128\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "\n",
    "# Locate repo root and artifacts\n",
    "repo_root = None\n",
    "cur = Path.cwd()\n",
    "for parent in [cur] + list(cur.parents):\n",
    "    if (parent / 'requirements.txt').exists() or (parent / '.git').exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "if repo_root is None:\n",
    "    repo_root = cur\n",
    "project16_artifacts = repo_root / 'projects' / 'phase3_llm_tuning' / 'project16_mistral_tuning' / 'artifacts'\n",
    "tuned_adapters_path = project16_artifacts / 'lora_adapters.safetensors'\n",
    "project17_dir = repo_root / 'projects' / 'phase3_llm_tuning' / 'project17_comparative_analysis'\n",
    "analysis_dir = project17_dir / 'analysis_artifacts'\n",
    "analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prompt set for head-to-head comparison\n",
    "prompts = [\n",
    "    \"Explain LoRA in one paragraph.\",\n",
    "    \"Write a Python function to compute Fibonacci numbers.\",\n",
    "    \"Give three bullet tips for learning ML.\",\n",
    "    \"Summarize the benefits of parameter-efficient fine-tuning.\",\n",
    "    \"What are common failure modes when instruction-tuning LLMs?\",\n",
    "]\n",
    "\n",
    "print('Config ready:')\n",
    "print('  quick_mode:', quick_mode)\n",
    "print('  model_name:', model_name)\n",
    "print('  tuned_adapters exist:', tuned_adapters_path.exists())\n",
    "print('  analysis_dir:', analysis_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ef03b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Helper functions for safe loading and generation\n",
    "from typing import Tuple, Dict, Any\n",
    "import mlx.core as mx\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "def safe_load(model_id: str):\n",
    "    \"\"\"Load a model/tokenizer with graceful error handling.\"\"\"\n",
    "    try:\n",
    "        m, tok = load(model_id)\n",
    "        return m, tok\n",
    "    except Exception as e:\n",
    "        print(f'Failed to load {model_id}:', repr(e))\n",
    "        return None, None\n",
    "\n",
    "def try_load_tuned(base_model_id: str, adapter_path: Path):\n",
    "    \"\"\"Attempt to load base model then apply LoRA adapters if available.\"\"\"\n",
    "    m, tok = safe_load(base_model_id)\n",
    "    if m is None:\n",
    "        return None, None\n",
    "    # Attempt adapter merge if file exists and peft utilities are present\n",
    "    if adapter_path.exists():\n",
    "        try:\n",
    "            from mlx_lm.peft import apply_lora, merge_lora_weights, load_lora_parameters, LoraConfig\n",
    "            # We don't know original config used; we load saved weights directly\n",
    "            load_lora_parameters(m, str(adapter_path))\n",
    "            tuned_model = merge_lora_weights(m)\n",
    "            print('Merged LoRA adapters into tuned model.')\n",
    "            return tuned_model, tok\n",
    "        except Exception as e:\n",
    "            print('Could not load/merge LoRA adapters:', repr(e))\n",
    "            print('Proceeding with base model as tuned fallback.')\n",
    "            return m, tok\n",
    "    else:\n",
    "        print('Adapter file not found; using base model as tuned placeholder.')\n",
    "        return m, tok\n",
    "\n",
    "def safe_generate(m, tok, prompt: str, **gen_kwargs):\n",
    "    if m is None or tok is None:\n",
    "        return '<model unavailable>'\n",
    "    try:\n",
    "        return generate(m, tok, prompt=prompt, **gen_kwargs)\n",
    "    except Exception as e:\n",
    "        return f'<generation error: {repr(e)}>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31554686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready:\n",
      "  base: skipped\n",
      "  tuned: skipped | mode: quick_mode_skipped_load\n"
     ]
    }
   ],
   "source": [
    "# 3) Load base and tuned models (may download on first run)\n",
    "if quick_mode:\n",
    "    base_model, base_tok = None, None  # Skip heavy downloads in quick_mode\n",
    "    tuned_model, tuned_tok = None, None\n",
    "    tuned_kind = 'quick_mode_skipped_load'\n",
    "else:\n",
    "    base_model, base_tok = safe_load(model_name)\n",
    "    tuned_model, tuned_tok = try_load_tuned(full_model_name, tuned_adapters_path)\n",
    "    tuned_kind = 'adapters_merged' if tuned_model is not None else 'fallback_base'\n",
    "\n",
    "print('Models ready:')\n",
    "print('  base:', 'ok' if base_model is not None else 'skipped')\n",
    "print('  tuned:', 'ok' if tuned_model is not None else 'skipped', '| mode:', tuned_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7689d260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs to /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts/base_vs_tuned_outputs.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>base_output</th>\n",
       "      <th>tuned_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explain LoRA in one paragraph.</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Write a Python function to compute Fibonacci n...</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "      <td>&lt;model unavailable&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt          base_output  \\\n",
       "0   1                     Explain LoRA in one paragraph.  <model unavailable>   \n",
       "1   2  Write a Python function to compute Fibonacci n...  <model unavailable>   \n",
       "\n",
       "          tuned_output  \n",
       "0  <model unavailable>  \n",
       "1  <model unavailable>  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Run generations and build comparison table\n",
    "rows = []\n",
    "for i, p in enumerate(prompts, start=1):\n",
    "    b_out = safe_generate(base_model, base_tok, prompt=p, max_tokens=max_new_tokens, verbose=False)\n",
    "    t_out = safe_generate(tuned_model, tuned_tok, prompt=p, max_tokens=max_new_tokens, verbose=False)\n",
    "    rows.append({'id': i, 'prompt': p, 'base_output': b_out, 'tuned_output': t_out})\n",
    "df = pd.DataFrame(rows)\n",
    "csv_path = analysis_dir / 'base_vs_tuned_outputs.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print('Saved outputs to', csv_path)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377530fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts/heuristic_metrics.csv\n",
      "Saved summary to /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project17_comparative_analysis/analysis_artifacts/summary_metrics.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>base_len</th>\n",
       "      <th>tuned_len</th>\n",
       "      <th>base_keyword_coverage</th>\n",
       "      <th>tuned_keyword_coverage</th>\n",
       "      <th>len_delta</th>\n",
       "      <th>coverage_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explain LoRA in one paragraph.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Write a Python function to compute Fibonacci n...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                             prompt  base_len  tuned_len  \\\n",
       "0   1                     Explain LoRA in one paragraph.         2          2   \n",
       "1   2  Write a Python function to compute Fibonacci n...         2          2   \n",
       "\n",
       "   base_keyword_coverage  tuned_keyword_coverage  len_delta  coverage_delta  \n",
       "0                    0.0                     0.0          0             0.0  \n",
       "1                    0.0                     0.0          0             0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Simple heuristic metrics\n",
    "import re\n",
    "def keyword_coverage(text: str, prompt: str):\n",
    "    # Percentage of unique prompt words (alphabetic) appearing in output\n",
    "    pw = {w.lower() for w in re.findall(r'[A-Za-z]+', prompt) if len(w) > 3}\n",
    "    if not pw:\n",
    "        return 0.0\n",
    "    ow = {w.lower() for w in re.findall(r'[A-Za-z]+', text)}\n",
    "    return len(pw & ow) / len(pw)\n",
    "\n",
    "metrics = []\n",
    "for r in rows:\n",
    "    base_len = len(r['base_output'].split()) if isinstance(r['base_output'], str) else 0\n",
    "    tuned_len = len(r['tuned_output'].split()) if isinstance(r['tuned_output'], str) else 0\n",
    "    base_cov = keyword_coverage(r['base_output'], r['prompt']) if isinstance(r['base_output'], str) else 0.0\n",
    "    tuned_cov = keyword_coverage(r['tuned_output'], r['prompt']) if isinstance(r['tuned_output'], str) else 0.0\n",
    "    metrics.append({\n",
    "        'id': r['id'],\n",
    "        'prompt': r['prompt'],\n",
    "        'base_len': base_len,\n",
    "        'tuned_len': tuned_len,\n",
    "        'base_keyword_coverage': base_cov,\n",
    "        'tuned_keyword_coverage': tuned_cov,\n",
    "        'len_delta': tuned_len - base_len,\n",
    "        'coverage_delta': tuned_cov - base_cov,\n",
    "    })\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_path = analysis_dir / 'heuristic_metrics.csv'\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "summary = {\n",
    "    'avg_len_delta': float(metrics_df['len_delta'].mean()),\n",
    "    'avg_cov_delta': float(metrics_df['coverage_delta'].mean()),\n",
    "    'prompts': len(metrics_df),\n",
    "    'quick_mode': quick_mode,\n",
    "    'tuned_kind': tuned_kind,\n",
    "}\n",
    "summary_path = analysis_dir / 'summary_metrics.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('Saved metrics to', metrics_path)\n",
    "print('Saved summary to', summary_path)\n",
    "metrics_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904e84e",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "- This notebook compares base vs tuned outputs on a small prompt set and records heuristic metrics (length and prompt keyword coverage).\n",
    "- In quick_mode, tuned==base to validate the pipeline without heavy downloads. Disable quick_mode to use the full model and load adapters.\n",
    "- If adapters are missing, generate them by running Project 16 with `DRY_RUN=False` and saving LoRA adapters.\n",
    "\n",
    "Next improvements:\n",
    "- Add a richer evaluation set and human preference judgments.\n",
    "- Include task-specific checks (e.g., code execution tests for coding prompts).\n",
    "- Plot distributions and deltas; compute statistical significance over larger samples.\n",
    "- Optionally compute log-prob scores with the model for stronger comparisons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
