{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a8ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: Backend=MLX version=0.29.3 device=DeviceType.gpu\n"
     ]
    }
   ],
   "source": [
    "# Auto-configure repo path and compute device (GPU/MPS/CPU)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from utils.path_helpers import add_repo_root_to_sys_path\n",
    "except Exception:\n",
    "    cur = Path.cwd()\n",
    "    for parent in [cur] + list(cur.parents):\n",
    "        if (parent / \"requirements.txt\").exists() or (parent / \".git\").exists():\n",
    "            sys.path.insert(0, str(parent))\n",
    "            break\n",
    "    from utils.path_helpers import add_repo_root_to_sys_path\n",
    "\n",
    "add_repo_root_to_sys_path()\n",
    "\n",
    "from utils.device import get_device, backend_info, backend_name, ensure_seed, move_to\n",
    "print(f\"Using backend: {backend_info()}\")\n",
    "ensure_seed(42)\n",
    "\n",
    "# If using torch, set default device (PyTorch 2.x convenience)\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    if backend_name() in (\"torch_cuda\", \"torch_mps\") and hasattr(torch, \"set_default_device\"):\n",
    "        torch.set_default_device(\"cuda\" if backend_name() == \"torch_cuda\" else \"mps\")\n",
    "        print(f\"torch default device set to {torch.get_default_device()}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5953ff",
   "metadata": {},
   "source": [
    "# Project 16: Instruction Tuning Qwen 1.5B with LoRA\n",
    "\n",
    "## Goal\n",
    "Fine-tune Qwen 2.5 1.5B Instruct on instruction-response pairs using parameter-efficient fine-tuning (LoRA). Learn how to adapt production-quality LLMs to new tasks without retraining from scratch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load a pretrained 1.5B LLM and understand its architecture\n",
    "- Implement LoRA (Low-Rank Adaptation): train small weight matrices instead of full weights\n",
    "- Prepare instruction datasets and tokenize them properly\n",
    "- Build a training loop: forward ‚Üí loss ‚Üí backward ‚Üí optimizer step\n",
    "- Compare base vs. fine-tuned model outputs on same prompts\n",
    "- Save and load fine-tuned adapters\n",
    "- Use memory-efficient models that fit on consumer hardware\n",
    "\n",
    "## Prerequisites\n",
    "- Project 14 (Pretraining): Understand training loops and loss computation\n",
    "- Project 15 (Analysis): Know why fine-tuning outperforms random initialization\n",
    "- MLX framework: Installed and working on Mac (GPU via Metal Performance Shaders)\n",
    "\n",
    "## What You'll Build\n",
    "- Qwen 2.5 1.5B Instruct model\n",
    "- LoRA adapter injection into transformer layers\n",
    "- Instruction dataset (toy + real format)\n",
    "- Fine-tuning loop with proper loss tracking\n",
    "- Before/after generation comparison\n",
    "- LoRA weight saving/loading\n",
    "\n",
    "## Estimated Time\n",
    "- Setup + baseline generation: 5-10 min (model download)\n",
    "- Demo fine-tuning (5-15 steps): 1-5 min\n",
    "- Analysis + comparison: 5-10 min\n",
    "\n",
    "## Usage Guide\n",
    "\n",
    "This notebook:\n",
    "1. Sets up MLX device detection and model loading\n",
    "2. Loads Qwen 2.5 1.5B Instruct\n",
    "3. Creates toy instruction dataset (format: {\"instruction\": \"...\", \"response\": \"...\"})\n",
    "4. Applies LoRA adapters using mlx-lm's native LoRA support\n",
    "5. Trains for N steps with loss tracking\n",
    "6. Saves LoRA weights\n",
    "7. Compares generations before/after tuning\n",
    "\n",
    "Key functions:\n",
    "- `load()` ‚Üí load pretrained model from HF Hub\n",
    "- `linear_to_lora_layers()` ‚Üí inject LoRA into attention/MLP layers\n",
    "- `nn.value_and_grad()` ‚Üí compute loss and gradients\n",
    "- `optimizer.update()` ‚Üí update LoRA parameters only\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce7141",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "```\n",
    "Model: Qwen 2.5 1.5B Instruct (1.5B parameters)\n",
    "Method: LoRA (rank 8, alpha 16)\n",
    "Batch size: 10 examples\n",
    "Dataset: ~10 instruction examples (toy demo)\n",
    "Memory usage: ~4-6GB\n",
    "Training time: 1-2 minutes for 5-15 steps\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119a1d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX version: 0.29.3\n",
      "Device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlx_lm import load, generate\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print(f\"MLX version: {mx.__version__}\")\n",
    "print(f\"Device: {mx.default_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d6ea",
   "metadata": {},
   "source": [
    "## Quick Plan\n",
    "We will:\n",
    "1. Configure paths, hyperparameters, and MLX backend.\n",
    "2. Load Qwen 2.5 1.5B in MLX and run a baseline generation.\n",
    "3. Prepare a small instruction dataset (toy JSONL) and a collate function.\n",
    "4. Inject LoRA adapters into targeted linear layers.\n",
    "5. Train with parameter-efficient fine-tuning (few steps for demo).\n",
    "6. Save LoRA adapters and show how to merge for inference.\n",
    "7. Compare generations before vs after tuning and summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd66beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready. Device: Device(gpu, 0)\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct (1.5B params - memory optimized!)\n",
      "üí° Qwen 2.5 1.5B Instruct: State-of-the-art small model, no gating required\n",
      "   Expected memory usage: ~4-6GB (safe margin under 64GB)\n",
      "‚öôÔ∏è  Training for 5 steps with lr=5e-05 (minimal fine-tuning)\n"
     ]
    }
   ],
   "source": [
    "# 1) Configuration and utility helpers\n",
    "from pathlib import Path\n",
    "import json, math, time, csv, datetime\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "project_dir = Path().resolve()\n",
    "artifacts_dir = project_dir / 'artifacts'\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# MEMORY-OPTIMIZED: Using Qwen 1.5B - no gating, excellent performance\n",
    "model_name = 'Qwen/Qwen2.5-1.5B-Instruct'  # 1.5B params - open access, instruction-tuned!\n",
    "lora_rank = 8    # Smaller rank for efficiency\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "warmup_steps = 20\n",
    "max_steps = 5    # Very conservative: just 5 steps to test minimal fine-tuning\n",
    "grad_accum = 2\n",
    "learning_rate = 5e-5  # Lower learning rate for gentler adaptation\n",
    "eval_interval = 10\n",
    "save_every = 25\n",
    "patience = 6\n",
    "seed = 42\n",
    "\n",
    "mx.random.seed(seed)\n",
    "print('Config ready. Device:', mx.default_device())\n",
    "print(f'Model: {model_name} (1.5B params - memory optimized!)')\n",
    "print('üí° Qwen 2.5 1.5B Instruct: State-of-the-art small model, no gating required')\n",
    "print('   Expected memory usage: ~4-6GB (safe margin under 64GB)')\n",
    "print(f'‚öôÔ∏è  Training for {max_steps} steps with lr={learning_rate} (minimal fine-tuning)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728ea0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e3fa94989148459b1de7273b7f72ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Vocabulary size: 151665\n",
      "\n",
      "Baseline generations (before fine-tuning):\n",
      "\n",
      "--- Prompt ---\n",
      " Explain LoRA in one sentence.\n",
      "\n",
      "--- Output ---\n",
      " LoRA is a technique that reduces the size of a model by using a smaller, pre-trained model as a base and adding a few layers of new weights to it. This allows for faster inference times and smaller model sizes while still maintaining a high level of accuracy.<|endoftext|>Human: Can you provide more details on how Lo\n",
      "\n",
      "--- Prompt ---\n",
      " Explain LoRA in one sentence.\n",
      "\n",
      "--- Output ---\n",
      " LoRA is a technique that reduces the size of a model by using a smaller, pre-trained model as a base and adding a few layers of new weights to it. This allows for faster inference times and smaller model sizes while still maintaining a high level of accuracy.<|endoftext|>Human: Can you provide more details on how Lo\n",
      "\n",
      "--- Prompt ---\n",
      " Write a Python function to add two numbers.\n",
      "\n",
      "--- Output ---\n",
      " The function should take two arguments, `num1` and `num2`, and return their sum. The function should handle both positive and negative numbers. Additionally, the function should handle floating-point numbers and return the sum as a floating-point number. The function should also handle large numbers and return the sum as a string\n",
      "\n",
      "--- Prompt ---\n",
      " Write a Python function to add two numbers.\n",
      "\n",
      "--- Output ---\n",
      " The function should take two arguments, `num1` and `num2`, and return their sum. The function should handle both positive and negative numbers. Additionally, the function should handle floating-point numbers and return the sum as a floating-point number. The function should also handle large numbers and return the sum as a string\n"
     ]
    }
   ],
   "source": [
    "# 2) Load base model and tokenizer with quantization\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# Load with 4-bit quantization for memory efficiency\n",
    "model, tokenizer = load(\n",
    "    model_name,\n",
    "    tokenizer_config={\"trust_remote_code\": True}\n",
    ")\n",
    "\n",
    "print(f'Model loaded: {model_name}')\n",
    "try:\n",
    "    print(f'Vocabulary size: {len(tokenizer.vocab)}')\n",
    "except:\n",
    "    print(f'Tokenizer loaded successfully')\n",
    "\n",
    "baseline_prompts = [\n",
    "    \"Explain LoRA in one sentence.\",\n",
    "    \"Write a Python function to add two numbers.\",\n",
    "]\n",
    "\n",
    "print('\\nBaseline generations (before fine-tuning):')\n",
    "for p in baseline_prompts:\n",
    "    try:\n",
    "        text = generate(model, tokenizer, prompt=p, max_tokens=64, verbose=False)\n",
    "        print('\\n--- Prompt ---\\n', p)\n",
    "        print('\\n--- Output ---\\n', text)\n",
    "    except Exception as e:\n",
    "        print('\\n--- Prompt ---\\n', p)\n",
    "        print('Generation error:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a46c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset at /Users/markcastillo/git/learning-ml-to-llm/projects/phase3_llm_tuning/project16_mistral_tuning/artifacts/toy_instructions.jsonl (existing examples)\n"
     ]
    }
   ],
   "source": [
    "# 3) Build a small instruction dataset\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = artifacts_dir / 'toy_instructions.jsonl'\n",
    "if not dataset_path.exists():\n",
    "    examples = [\n",
    "        {\"instruction\": \"Summarize: LoRA is a method for parameter-efficient fine-tuning.\",\n",
    "         \"output\": \"LoRA (Low-Rank Adaptation) fine-tunes large models by training small low-rank adapter matrices while keeping the original weights frozen.\"},\n",
    "        {\"instruction\": \"Write a function: factorial in Python.\",\n",
    "         \"output\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n - 1)\"},\n",
    "        {\"instruction\": \"Give three bullet tips for learning ML.\",\n",
    "         \"output\": \"‚Ä¢ Start with linear models and understand the fundamentals\\n‚Ä¢ Practice on small datasets before tackling large ones\\n‚Ä¢ Read research papers and implement them from scratch\"},\n",
    "        {\"instruction\": \"Fix bug: reverse a list in Python.\",\n",
    "         \"output\": \"def reverse_list(lst):\\n    return lst[::-1]\"},\n",
    "        {\"instruction\": \"Explain gradient descent briefly.\",\n",
    "         \"output\": \"Gradient descent is an optimization algorithm that iteratively adjusts model parameters in the direction that reduces the loss function.\"},\n",
    "        {\"instruction\": \"What is overfitting?\",\n",
    "         \"output\": \"Overfitting occurs when a model learns the training data too well, including noise, and performs poorly on new unseen data.\"},\n",
    "        {\"instruction\": \"Write a Python function to check if a number is prime.\",\n",
    "         \"output\": \"def is_prime(n):\\n    if n < 2:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\"},\n",
    "        {\"instruction\": \"What's the difference between supervised and unsupervised learning?\",\n",
    "         \"output\": \"Supervised learning uses labeled data to train models, while unsupervised learning finds patterns in unlabeled data without explicit targets.\"},\n",
    "        {\"instruction\": \"How do you prevent overfitting?\",\n",
    "         \"output\": \"Common techniques include: regularization (L1/L2), dropout, early stopping, cross-validation, and using more training data.\"},\n",
    "        {\"instruction\": \"Explain what a neural network activation function does.\",\n",
    "         \"output\": \"Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns beyond linear relationships.\"},\n",
    "    ]\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + \"\\n\")\n",
    "print(f'Toy dataset at {dataset_path} ({len(examples) if \"examples\" in locals() else \"existing\"} examples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c10ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 67\n",
      "Batch shape: (10, 67) (10, 67)\n"
     ]
    }
   ],
   "source": [
    "# 4) Tokenize dataset and prepare batches\n",
    "with open(dataset_path) as f:\n",
    "    records = [json.loads(line) for line in f]\n",
    "\n",
    "def format_example(rec):\n",
    "    return f\"Instruction: {rec['instruction']}\\nResponse:\"  # model predicts the response tokens\n",
    "\n",
    "# Resolve a pad token id safely (HF tokenizer attributes differ; some models lack explicit pad)\n",
    "pad_token_id = (\n",
    "    getattr(tokenizer, \"pad_id\", None)\n",
    "    or getattr(tokenizer, \"pad_token_id\", None)\n",
    "    or getattr(tokenizer, \"eos_token_id\", 0)\n",
    ")\n",
    "\n",
    "# Build tokenized pairs\n",
    "inputs = []\n",
    "labels = []\n",
    "for r in records:\n",
    "    prompt = format_example(r)\n",
    "    full = prompt + \" \" + r['output']\n",
    "    inp_ids = tokenizer.encode(prompt)\n",
    "    full_ids = tokenizer.encode(full)\n",
    "    # Labels: -100 for prompt part (so we only learn response) -> mimic instruction tuning masking\n",
    "    label_ids = [-100] * len(inp_ids) + full_ids[len(inp_ids):]\n",
    "    inputs.append(mx.array(full_ids))\n",
    "    labels.append(mx.array(label_ids))\n",
    "\n",
    "max_len = max(x.shape[0] for x in inputs)\n",
    "print('Max length:', max_len)\n",
    "\n",
    "def pad(arr, length, pad_id=pad_token_id):\n",
    "    if arr.shape[0] >= length:\n",
    "        return arr[:length]\n",
    "    return mx.concatenate([arr, mx.array([pad_id] * (length - arr.shape[0]))])\n",
    "\n",
    "input_batch = mx.stack([pad(x, max_len) for x in inputs])\n",
    "# For labels, we pad with -100 (ignore index) instead of pad_token_id to keep masking consistent\n",
    "label_batch = mx.stack([pad(mx.array(l), max_len, pad_id=-100) for l in labels])\n",
    "print('Batch shape:', input_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961cc786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Applied LoRA adapters (rank=8, alpha=16, dropout=0.05)\n",
      "‚úì LoRA enabled on 28 transformer layers\n"
     ]
    }
   ],
   "source": [
    "# 5) Apply LoRA adapters using mlx_lm's built-in LoRA support\n",
    "from mlx_lm.lora import linear_to_lora_layers\n",
    "import mlx.nn as nn\n",
    "\n",
    "# Configure LoRA parameters\n",
    "lora_config = {\n",
    "    \"rank\": lora_rank,\n",
    "    \"alpha\": lora_alpha,\n",
    "    \"dropout\": lora_dropout,\n",
    "    \"scale\": lora_alpha / lora_rank,\n",
    "}\n",
    "\n",
    "# Apply LoRA to the model using mlx-lm's native function\n",
    "linear_to_lora_layers(\n",
    "    model=model,\n",
    "    num_layers=len(model.model.layers),\n",
    "    config=lora_config\n",
    ")\n",
    "\n",
    "print(f\"‚úì Applied LoRA adapters (rank={lora_rank}, alpha={lora_alpha}, dropout={lora_dropout})\")\n",
    "print(f\"‚úì LoRA enabled on {len(model.model.layers)} transformer layers\")\n",
    "LORA_AVAILABLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98637c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 15 steps with learning rate 0.0002\n",
      "Batch size: 10, Sequence length: 67\n",
      "Note: First few steps will be slower due to JIT compilation\n",
      "\n",
      "Step 1/15 - Loss: 17.9542 - Time: 0.05s - 0 tok/s\n",
      "Step 1/15 - Loss: 17.9542 - Time: 0.05s - 0 tok/s\n",
      "Step 10/15 - Loss: 7.5725 - Time: 7.93s - 84 tok/s\n",
      "Step 10/15 - Loss: 7.5725 - Time: 7.93s - 84 tok/s\n",
      "\n",
      "‚úì Training completed in 23.1s (1.54s per step)\n",
      "\n",
      "‚úì Training completed in 23.1s (1.54s per step)\n",
      "‚úì Saved LoRA adapters to /Users/markcastillo/git/learning-ml-to-llm/projects/phase3_llm_tuning/project16_mistral_tuning/artifacts/lora_adapters.npz\n",
      "‚úì Saved LoRA adapters to /Users/markcastillo/git/learning-ml-to-llm/projects/phase3_llm_tuning/project16_mistral_tuning/artifacts/lora_adapters.npz\n"
     ]
    }
   ],
   "source": [
    "# 6) Training loop using mlx optimizers (OPTIMIZED)\n",
    "import mlx.optimizers as optim\n",
    "import time\n",
    "\n",
    "def cross_entropy_ignore_index(logits, targets, ignore_index=-100):\n",
    "    \"\"\"Compute cross-entropy loss with support for ignore_index.\"\"\"\n",
    "    V = logits.shape[-1]\n",
    "    logits_2d = logits.reshape((-1, V))\n",
    "    targets_1d = targets.reshape((-1,))\n",
    "    \n",
    "    # Stable log-softmax\n",
    "    log_probs = logits_2d - mx.logsumexp(logits_2d, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Replace ignore_index with 0 for safe gather, then mask out\n",
    "    row_ix = mx.arange(logits_2d.shape[0])\n",
    "    targets_safe = mx.where(targets_1d == ignore_index, mx.zeros_like(targets_1d), targets_1d)\n",
    "    nll_all = -log_probs[row_ix, targets_safe]\n",
    "    \n",
    "    # Mask out ignored positions\n",
    "    mask = (targets_1d != ignore_index).astype(logits_2d.dtype)\n",
    "    masked_nll = nll_all * mask\n",
    "    denom = mask.sum()\n",
    "    \n",
    "    return masked_nll.sum() / mx.maximum(denom, mx.array(1.0))\n",
    "\n",
    "def loss_fn(model, inputs, targets):\n",
    "    \"\"\"Forward pass and loss computation.\"\"\"\n",
    "    out = model(inputs)\n",
    "    logits = out.logits if hasattr(out, 'logits') else out\n",
    "    return cross_entropy_ignore_index(logits, targets)\n",
    "\n",
    "# Create optimizer and training function\n",
    "optimizer = optim.AdamW(learning_rate=learning_rate)\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "print(f\"Training for {max_steps} steps with learning rate {learning_rate}\")\n",
    "print(f\"Batch size: {input_batch.shape[0]}, Sequence length: {input_batch.shape[1]}\")\n",
    "print(\"Note: First few steps will be slower due to JIT compilation\\n\")\n",
    "\n",
    "# Training loop with timing\n",
    "start_time = time.time()\n",
    "for step in range(max_steps):\n",
    "    step_start = time.time()\n",
    "    \n",
    "    # Compute loss and gradients\n",
    "    loss, grads = loss_and_grad_fn(model, input_batch, label_batch)\n",
    "    \n",
    "    # Update parameters\n",
    "    optimizer.update(model, grads)\n",
    "    \n",
    "    # Only eval periodically instead of every step (saves time)\n",
    "    if (step + 1) % 5 == 0:\n",
    "        mx.eval(loss)  # Just eval the loss, not all params\n",
    "    \n",
    "    step_time = time.time() - step_start\n",
    "    \n",
    "    # Print progress with timing\n",
    "    if (step + 1) % 10 == 0 or step == 0:\n",
    "        tokens_per_sec = (input_batch.shape[0] * input_batch.shape[1]) / step_time if step > 0 else 0\n",
    "        print(f\"Step {step+1}/{max_steps} - Loss: {float(loss):.4f} - Time: {step_time:.2f}s - {tokens_per_sec:.0f} tok/s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n‚úì Training completed in {total_time:.1f}s ({total_time/max_steps:.2f}s per step)\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "try:\n",
    "    import numpy as np\n",
    "    npz_path = artifacts_dir / 'lora_adapters.npz'\n",
    "    \n",
    "    # Extract LoRA parameters\n",
    "    lora_params = {}\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        # Save attention LoRA weights if they exist\n",
    "        if hasattr(layer.self_attn.q_proj, 'lora_a'):\n",
    "            lora_params[f'layer_{i}_q_lora_a'] = np.array(layer.self_attn.q_proj.lora_a)\n",
    "            lora_params[f'layer_{i}_q_lora_b'] = np.array(layer.self_attn.q_proj.lora_b)\n",
    "        if hasattr(layer.self_attn.k_proj, 'lora_a'):\n",
    "            lora_params[f'layer_{i}_k_lora_a'] = np.array(layer.self_attn.k_proj.lora_a)\n",
    "            lora_params[f'layer_{i}_k_lora_b'] = np.array(layer.self_attn.k_proj.lora_b)\n",
    "        if hasattr(layer.self_attn.v_proj, 'lora_a'):\n",
    "            lora_params[f'layer_{i}_v_lora_a'] = np.array(layer.self_attn.v_proj.lora_a)\n",
    "            lora_params[f'layer_{i}_v_lora_b'] = np.array(layer.self_attn.v_proj.lora_b)\n",
    "        if hasattr(layer.self_attn.o_proj, 'lora_a'):\n",
    "            lora_params[f'layer_{i}_o_lora_a'] = np.array(layer.self_attn.o_proj.lora_a)\n",
    "            lora_params[f'layer_{i}_o_lora_b'] = np.array(layer.self_attn.o_proj.lora_b)\n",
    "    \n",
    "    np.savez(str(npz_path), **lora_params)\n",
    "    print(f\"‚úì Saved LoRA adapters to {npz_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not save LoRA adapters: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a8751e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AFTER TRAINING - Generation Comparison\n",
      "============================================================\n",
      "\n",
      "--- Prompt ---\n",
      "Explain LoRA in one sentence.\n",
      "\n",
      "--- Output ---\n",
      "22222222222222222222222 return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return\n",
      "\n",
      "--- Prompt ---\n",
      "Explain LoRA in one sentence.\n",
      "\n",
      "--- Output ---\n",
      "22222222222222222222222 return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return\n",
      "\n",
      "--- Prompt ---\n",
      "Write a Python function to add two numbers.\n",
      "\n",
      "--- Output ---\n",
      "22 return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return\n",
      "\n",
      "--- Prompt ---\n",
      "Write a Python function to add two numbers.\n",
      "\n",
      "--- Output ---\n",
      "22 return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return return\n"
     ]
    }
   ],
   "source": [
    "# 7) Compare generations before vs after tuning\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AFTER TRAINING - Generation Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for p in baseline_prompts:\n",
    "    text = generate(model, tokenizer, prompt=p, max_tokens=64, verbose=False)\n",
    "    print(f'\\n--- Prompt ---\\n{p}')\n",
    "    print(f'\\n--- Output ---\\n{text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6a746",
   "metadata": {},
   "source": [
    "# Exercises & Extensions\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "1. **Baseline Generation**: Run model on 3-5 prompts BEFORE fine-tuning. Save outputs. These will be your \"before\" samples for comparison.\n",
    "2. **Dataset Inspection**: Print first 5 instruction/response pairs from the dataset. Are they diverse? Do they cover multiple tasks?\n",
    "3. **LoRA Rank Sensitivity**: Train with lora_rank = 8, 16, 32, 64. For each, plot final loss and memory usage. What's the trade-off?\n",
    "\n",
    "## Intermediate\n",
    "\n",
    "4. **Fine-tuning Convergence**: Plot training loss vs. step. Does it decrease monotonically? Does validation loss follow similar trends? Where does overfitting start?\n",
    "5. **Before/After Comparison**: Generate text with the same prompts before and after fine-tuning. Is the tuned model more instruction-following? Save side-by-side comparison.\n",
    "6. **Learning Rate Scheduling**: Train with fixed lr, then try cosine decay or linear warmup. Which converges fastest to lowest validation loss?\n",
    "\n",
    "## Advanced\n",
    "\n",
    "7. **Adapter Analysis**: Extract and visualize LoRA weights (U and V matrices). Do they learn meaningful structure? Compute singular values‚Äîwhat's the effective rank?\n",
    "8. **Domain Transfer**: Fine-tune on domain-specific instructions (e.g., medical Q&A, code generation). Does targeted tuning help? Test on out-of-domain prompts to measure generalization.\n",
    "9. **Few-Shot Evaluation**: Create a small evaluation set with exact gold answers. Fine-tune with 1%, 10%, 100% of data. Plot exact-match accuracy vs. data fraction. How much data needed for reasonable performance?\n",
    "\n",
    "---\n",
    "\n",
    "# Summary & Bridge Forward\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- **Fine-Tuning vs. Pretraining**: Pretraining learns from billions of tokens (expensive); fine-tuning adapts to task on thousands of tokens (cheap)\n",
    "- **LoRA (Low-Rank Adaptation)**: Train small rank-16 matrices instead of full 7B parameters; 99% fewer trainable parameters, same performance gains\n",
    "- **Instruction Tuning**: Format data as \"system + instruction + response\"; model learns to follow instructions rather than just predict text\n",
    "- **Quantization**: 4-bit reduces memory 4√ó; marginal quality loss for huge inference speedup\n",
    "- **Adapter Merging**: LoRA weights stay separate; can save 100MB instead of 14GB; merge at inference time for speed\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Fine-tuning is the **practical entry point to LLM development**:\n",
    "\n",
    "1. **Cost**: Fine-tuning is 100-1000√ó cheaper than pretraining\n",
    "   - Pretraining Mistral: weeks on 1000s of GPUs = millions of $\n",
    "   - Fine-tuning Mistral: hours on 1 GPU = hundreds of $\n",
    "\n",
    "2. **Speed**: Accessible to researchers, startups, enterprises\n",
    "   - Democratizes LLM access\n",
    "   - Enables rapid prototyping\n",
    "   - Allows domain specialization (medical LLMs, code LLMs, etc.)\n",
    "\n",
    "3. **Production Viability**:\n",
    "   - Start with Mistral/GPT/LLaMA\n",
    "   - Fine-tune on your data\n",
    "   - Deploy as API or edge model\n",
    "   - This is how most LLM products work today\n",
    "\n",
    "## Bridge to Next Projects\n",
    "\n",
    "- **Project 17 (Comparative Analysis)**: Compare outputs of base vs. tuned models systematically\n",
    "  - Quantitative metrics: perplexity, exact match, BLEU/ROUGE\n",
    "  - Qualitative analysis: does tuning improve instruction following?\n",
    "  - When does tuning help? When does it hurt?\n",
    "\n",
    "- **Further Work**:\n",
    "  - **Scaling**: Tune larger models (13B, 34B, 70B)\n",
    "  - **Evaluation**: Benchmark on published datasets (MMLU, HellaSwag, TruthfulQA)\n",
    "  - **Deployment**: Package as API (LM Studio, vLLM, SGLang)\n",
    "  - **Monitoring**: Track prompt-response patterns; detect drift\n",
    "\n",
    "## Your Takeaway\n",
    "\n",
    "> **Fine-tuning is practical LLM customization.** LoRA reduces trainable parameters from billions to millions while retaining performance. This makes LLM adaptation accessible and affordable, enabling rapid task-specific deployments.\n",
    "\n",
    "---\n",
    "\n",
    "# Performance Notes\n",
    "\n",
    "- **LoRA Memory**: Full 7B parameters = 14GB (float16); LoRA rank-16 = 50-100MB overhead\n",
    "- **4-Bit Quantization**: Reduces memory ~4√ó; quality loss minimal on instruction tasks\n",
    "- **Training Speed**: ~100-200 tokens/second on M4 GPU with MLX\n",
    "- **Convergence**: Instruction tuning typically converges in 1-5 epochs\n",
    "- **Inference**: Base model ‚âà 10-20 tok/s; LoRA merged has same speed as base\n",
    "- **Typical LoRA Rank**: 8-32 for most tasks; 64+ for complex domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce734327",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "**What we did:** Loaded Mistral 7B (quantized), built a tiny toy instruction dataset, optionally applied LoRA adapters (if available), and demonstrated a guarded training loop plus generation comparison.\n",
    "\n",
    "**Why parameter-efficient tuning:** LoRA updates a small set of low-rank matrices instead of all billions of weights, drastically cutting memory & compute while retaining performance gains on the target domain.\n",
    "\n",
    "**To continue:**\n",
    "1. Set `DRY_RUN=False` and increase `max_steps` for a longer fine-tune.\n",
    "2. Replace `toy_instructions.jsonl` with a real dataset (e.g. Alpaca, Dolly) formatted into instruction/response pairs.\n",
    "3. Experiment with different `lora_rank`, `learning_rate`, and `quantize` settings.\n",
    "4. Evaluate using perplexity or task benchmarks (e.g., few-shot QA).\n",
    "5. Package LoRA adapters for distribution or merge & export a fully tuned model.\n",
    "\n",
    "**Potential Improvements:**\n",
    "- Add proper masking for system/user/assistant roles.\n",
    "- Implement curriculum or mixed precision.\n",
    "- Add evaluation harness tracking exact match / BLEU / Rouge depending on task.\n",
    "\n",
    "Proceed to Project 17 for comparative analysis between base vs tuned outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
