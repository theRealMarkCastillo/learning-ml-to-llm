{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a8ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: Backend=MLX version=0.29.3 device=DeviceType.gpu\n"
     ]
    }
   ],
   "source": [
    "# Auto-configure repo path and compute device (GPU/MPS/CPU)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from utils.path_helpers import add_repo_root_to_sys_path\n",
    "except Exception:\n",
    "    cur = Path.cwd()\n",
    "    for parent in [cur] + list(cur.parents):\n",
    "        if (parent / \"requirements.txt\").exists() or (parent / \".git\").exists():\n",
    "            sys.path.insert(0, str(parent))\n",
    "            break\n",
    "    from utils.path_helpers import add_repo_root_to_sys_path\n",
    "\n",
    "add_repo_root_to_sys_path()\n",
    "\n",
    "from utils.device import get_device, backend_info, backend_name, ensure_seed, move_to\n",
    "print(f\"Using backend: {backend_info()}\")\n",
    "ensure_seed(42)\n",
    "\n",
    "# If using torch, set default device (PyTorch 2.x convenience)\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    if backend_name() in (\"torch_cuda\", \"torch_mps\") and hasattr(torch, \"set_default_device\"):\n",
    "        torch.set_default_device(\"cuda\" if backend_name() == \"torch_cuda\" else \"mps\")\n",
    "        print(f\"torch default device set to {torch.get_default_device()}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5953ff",
   "metadata": {},
   "source": [
    "# Project 16: Instruction Tuning Mistral 7B\n",
    "\n",
    "## Goal\n",
    "Fine-tune a production large language model (Mistral 7B) on instruction-response pairs using parameter-efficient fine-tuning (LoRA). Learn how to adapt pretrained models to new tasks without retraining from scratch.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load a pretrained LLM (Mistral 7B) and understand its architecture\n",
    "- Implement LoRA (Low-Rank Adaptation): train small weight matrices instead of full weights\n",
    "- Prepare instruction datasets and tokenize them properly\n",
    "- Build a training loop: forward → loss → backward → optimizer step\n",
    "- Compare base vs. fine-tuned model outputs on same prompts\n",
    "- Quantize models to fit on consumer hardware (4-bit quantization)\n",
    "- Save and load fine-tuned adapters\n",
    "\n",
    "## Prerequisites\n",
    "- Project 14 (Pretraining): Understand training loops and loss computation\n",
    "- Project 15 (Analysis): Know why fine-tuning outperforms random initialization\n",
    "- MLX framework: Installed and working on Mac (GPU via Metal Performance Shaders)\n",
    "\n",
    "## What You'll Build\n",
    "- Quantized Mistral 7B loaded via MLX\n",
    "- LoRA adapter injection into transformer layers\n",
    "- Instruction dataset (toy + real format)\n",
    "- Fine-tuning loop with validation + checkpointing\n",
    "- Before/after generation comparison\n",
    "- LoRA weight saving/loading\n",
    "- Optional: merged model for inference\n",
    "\n",
    "## Estimated Time\n",
    "- Setup + baseline generation: 30 min\n",
    "- Demo fine-tuning (300 steps): 1-3 hours\n",
    "- Full fine-tuning (3000+ steps): 10-24 hours\n",
    "- Analysis + comparison: 30-60 min\n",
    "\n",
    "## Usage Guide\n",
    "\n",
    "This notebook:\n",
    "1. Sets up MLX device detection and model loading\n",
    "2. Loads Mistral 7B with 4-bit quantization\n",
    "3. Creates toy instruction dataset (format: {\"instruction\": \"...\", \"response\": \"...\"})\n",
    "4. Applies LoRA adapters to linear layers\n",
    "5. Trains for N steps with validation loss tracking\n",
    "6. Saves LoRA weights and merges for inference\n",
    "7. Compares generations before/after tuning\n",
    "\n",
    "Key functions:\n",
    "- `load()` → load pretrained model from HF Hub\n",
    "- `apply_lora()` → inject rank-16 adaptations\n",
    "- `get_batch()` → tokenize and batch instruction pairs\n",
    "- `train_step()` → forward + loss + backward for LoRA weights only\n",
    "- `merge_lora_weights()` → combine LoRA adapters with base weights\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce7141",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "```\n",
    "Model: Mistral 7B\n",
    "Method: LoRA (rank 32)\n",
    "Batch size: 32-64\n",
    "Dataset: ~10k instruction examples\n",
    "Memory usage: ~20-30GB\n",
    "Training time: Hours per run\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119a1d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLX version: 0.29.3\n",
      "Device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlx_lm import load, generate\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "print(f\"MLX version: {mx.__version__}\")\n",
    "print(f\"Device: {mx.default_device()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c66d6ea",
   "metadata": {},
   "source": [
    "## Quick Plan\n",
    "We will:\n",
    "1. Configure paths, hyperparameters, and MLX backend.\n",
    "2. Load Mistral 7B in MLX and run a baseline generation.\n",
    "3. Prepare a small instruction dataset (toy JSONL) and a collate function.\n",
    "4. Inject LoRA adapters into targeted linear layers.\n",
    "5. Train with parameter-efficient fine-tuning (few steps for demo).\n",
    "6. Save LoRA adapters and show how to merge for inference.\n",
    "7. Compare generations before vs after tuning and summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd66beb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready. Device: Device(gpu, 0)\n"
     ]
    }
   ],
   "source": [
    "# 1) Configuration and utility helpers\n",
    "from pathlib import Path\n",
    "import json, math, time, csv, datetime\n",
    "import numpy as np\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "project_dir = Path().resolve()\n",
    "artifacts_dir = project_dir / 'artifacts'\n",
    "artifacts_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'  # can switch to base if desired\n",
    "use_4bit = True  # quantization for memory\n",
    "lora_rank = 16   # keep small for Mac memory\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "warmup_steps = 50\n",
    "max_steps = 300  # demo length; increase for real tuning\n",
    "grad_accum = 4\n",
    "learning_rate = 5e-5\n",
    "eval_interval = 50\n",
    "save_every = 100\n",
    "patience = 6\n",
    "seed = 42\n",
    "\n",
    "mx.random.seed(seed)\n",
    "print('Config ready. Device:', mx.default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "728ea0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlx_lm.load() does not support 'quantize' in this version; loading without quantization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7005c15e1e341959af75b41dfc27335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccd156a276a48a3b01f3ec31db17e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b85669befd4a3aaddb79ba2f7f8bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a3583b25734e6681ec9ce9050fa47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model, tokenizer = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantize\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mq4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Fallback for mlx_lm versions without 'quantize' kwarg support\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: load() got an unexpected keyword argument 'quantize'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Fallback for mlx_lm versions without 'quantize' kwarg support\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mmlx_lm.load() does not support \u001b[39m\u001b[33m'\u001b[39m\u001b[33mquantize\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in this version; loading without quantization.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     model, tokenizer = \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m system_prompt = \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m baseline_prompts = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExplain LoRA in one paragraph.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWrite a Python function to compute Fibonacci numbers.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/mlx_lm/utils.py:264\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path_or_hf_repo, tokenizer_config, model_config, adapter_path, lazy, return_config, revision)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    229\u001b[39m     path_or_hf_repo: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    230\u001b[39m     tokenizer_config={},\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m     Tuple[nn.Module, TokenizerWrapper, Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[32m    239\u001b[39m ]:\n\u001b[32m    240\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[33;03m    Load the model and tokenizer from a given path or a huggingface repository.\u001b[39;00m\n\u001b[32m    242\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    262\u001b[39m \u001b[33;03m        ValueError: If model class or args class are not found.\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     model_path = \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_hf_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m     model, config = load_model(model_path, lazy, model_config=model_config)\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/mlx_lm/utils.py:101\u001b[39m, in \u001b[36m_download\u001b[39m\u001b[34m(path_or_hf_repo, revision)\u001b[39m\n\u001b[32m     97\u001b[39m model_path = Path(path_or_hf_repo)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path.exists():\n\u001b[32m    100\u001b[39m     model_path = Path(\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_hf_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel*.safetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizer.model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.tiktoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtiktoken.model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.jsonl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m*.jinja\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    330\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/learning-ml-to-llm/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2) Load base model and tokenizer; baseline generation\n",
    "try:\n",
    "    model, tokenizer = load(model_name, quantize='q4' if use_4bit else None)\n",
    "except TypeError:\n",
    "    # Fallback for mlx_lm versions without 'quantize' kwarg support\n",
    "    print(\"mlx_lm.load() does not support 'quantize' in this version; loading without quantization.\")\n",
    "    model, tokenizer = load(model_name)\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "baseline_prompts = [\n",
    "    \"Explain LoRA in one paragraph.\",\n",
    "    \"Write a Python function to compute Fibonacci numbers.\",\n",
    "]\n",
    "\n",
    "print('Baseline generations:')\n",
    "for p in baseline_prompts:\n",
    "    try:\n",
    "        text = generate(model, tokenizer, prompt=p, max_tokens=128, verbose=False)\n",
    "        print('\\n--- Prompt ---\\n', p)\n",
    "        print('\\n--- Output ---\\n', text)\n",
    "    except Exception as e:\n",
    "        print('\\n--- Prompt ---\\n', p)\n",
    "        print('Generation error:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a46c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy dataset at /Users/mark/git/learning-ml-to-llm/projects/phase3_llm_tuning/project16_mistral_tuning/artifacts/toy_instructions.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 3) Build a tiny instruction dataset (toy JSONL)\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = artifacts_dir / 'toy_instructions.jsonl'\n",
    "if not dataset_path.exists():\n",
    "    examples = [\n",
    "        {\"instruction\": \"Summarize: LoRA is a method for parameter-efficient fine-tuning.\",\n",
    "         \"output\": \"LoRA fine-tunes large models by training small low-rank adapters while freezing original weights.\"},\n",
    "        {\"instruction\": \"Write a function: factorial in Python.\",\n",
    "         \"output\": \"def factorial(n):\\n    return 1 if n<=1 else n*factorial(n-1)\"},\n",
    "        {\"instruction\": \"Give three bullet tips for learning ML.\",\n",
    "         \"output\": \"- Start with linear models\\n- Practice on small datasets\\n- Read research and implement\"},\n",
    "        {\"instruction\": \"Fix bug: reverse a list in Python.\",\n",
    "         \"output\": \"def reverse_list(xs):\\n    return xs[::-1]\"},\n",
    "    ]\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        for ex in examples:\n",
    "            f.write(json.dumps(ex) + \"\\n\")\n",
    "print('Toy dataset at', dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10ede0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m prompt = format_example(r)\n\u001b[32m     13\u001b[39m full = prompt + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + r[\u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m inp_ids = \u001b[43mtokenizer\u001b[49m.encode(prompt)\n\u001b[32m     15\u001b[39m full_ids = tokenizer.encode(full)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Labels: -100 for prompt part (so we only learn response) -> mimic instruction tuning masking\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# 4) Tokenize dataset and prepare batches\n",
    "with open(dataset_path) as f:\n",
    "    records = [json.loads(line) for line in f]\n",
    "\n",
    "def format_example(rec):\n",
    "    return f\"Instruction: {rec['instruction']}\\nResponse:\"  # model predicts the response tokens\n",
    "\n",
    "# Build tokenized pairs\n",
    "inputs = []\n",
    "labels = []\n",
    "for r in records:\n",
    "    prompt = format_example(r)\n",
    "    full = prompt + \" \" + r['output']\n",
    "    inp_ids = tokenizer.encode(prompt)\n",
    "    full_ids = tokenizer.encode(full)\n",
    "    # Labels: -100 for prompt part (so we only learn response) -> mimic instruction tuning masking\n",
    "    label_ids = [-100]*len(inp_ids) + full_ids[len(inp_ids):]\n",
    "    inputs.append(mx.array(full_ids))\n",
    "    labels.append(mx.array(label_ids))\n",
    "\n",
    "max_len = max(x.shape[0] for x in inputs)\n",
    "print('Max length:', max_len)\n",
    "\n",
    "def pad(arr, length, pad_id=tokenizer.pad_id):\n",
    "    if arr.shape[0] >= length:\n",
    "        return arr[:length]\n",
    "    return mx.concatenate([arr, mx.array([pad_id]*(length - arr.shape[0]))])\n",
    "\n",
    "input_batch = mx.stack([pad(x, max_len) for x in inputs])\n",
    "label_batch = mx.stack([pad(mx.array(l), max_len, pad_id=-100) for l in labels])\n",
    "print('Batch shape:', input_batch.shape, label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961cc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Apply LoRA adapters (if available in mlx_lm)\n",
    "LORA_AVAILABLE = False\n",
    "try:\n",
    "    from mlx_lm.peft import LoraConfig, apply_lora, mark_only_lora_as_trainable, save_lora_parameters, merge_lora_weights\n",
    "    LORA_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    print('LoRA utilities not found in mlx_lm.peft, proceeding without training. Error:', repr(e))\n",
    "\n",
    "if LORA_AVAILABLE:\n",
    "    lcfg = LoraConfig(\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    )\n",
    "    apply_lora(model, lcfg)\n",
    "    mark_only_lora_as_trainable(model)\n",
    "    print('Applied LoRA to target modules and marked as trainable.')\n",
    "else:\n",
    "    print('Skipping LoRA application. You can upgrade mlx_lm to a version with PEFT support.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98637c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Training loop (guarded; short demo)\n",
    "DRY_RUN = True  # set to False to attempt a few training steps\n",
    "\n",
    "if not LORA_AVAILABLE:\n",
    "    print('LoRA not available; training is skipped.')\n",
    "elif DRY_RUN:\n",
    "    print('DRY_RUN=True: skipping heavy training. Set DRY_RUN=False to try a few steps (may be slow).')\n",
    "else:\n",
    "    import mlx.optimizers as optim\n",
    "\n",
    "    def cross_entropy_ignore_index(logits, targets, ignore_index=-100):\n",
    "        # logits: [B,T,V], targets: [B,T]\n",
    "        V = logits.shape[-1]\n",
    "        logits_2d = logits.reshape((-1, V))\n",
    "        targets_1d = targets.reshape((-1,))\n",
    "        mask = (targets_1d != ignore_index)\n",
    "        logits_masked = logits_2d[mask]\n",
    "        targets_masked = targets_1d[mask]\n",
    "        log_probs = logits_masked - mx.logsumexp(logits_masked, axis=-1, keepdims=True)\n",
    "        nll = -log_probs[mx.arange(log_probs.shape[0]), targets_masked]\n",
    "        return nll.mean() if nll.shape[0] > 0 else mx.array(0.0)\n",
    "\n",
    "    opt = optim.AdamW(learning_rate)\n",
    "\n",
    "    def loss_fn(_model):\n",
    "        out = _model(input_batch)\n",
    "        return cross_entropy_ignore_index(out.logits, label_batch)\n",
    "\n",
    "    val_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "    for step in range(min(20, max_steps)):\n",
    "        loss, grads = val_and_grad(model)\n",
    "        opt.update(model, grads)\n",
    "        if (step+1) % 5 == 0:\n",
    "            print(f'step {step+1} loss {float(loss):.4f}')\n",
    "\n",
    "    # Save LoRA params if utilities present\n",
    "    try:\n",
    "        save_lora_parameters(model, str(artifacts_dir / 'lora_adapters.safetensors'))\n",
    "        print('Saved LoRA adapters to artifacts directory.')\n",
    "    except Exception as e:\n",
    "        print('Could not save LoRA adapters:', repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Merge adapters for inference (optional)\n",
    "if LORA_AVAILABLE:\n",
    "    try:\n",
    "        merged_model = merge_lora_weights(model)\n",
    "        print('Merged LoRA weights into a copy of the model for inference.')\n",
    "    except Exception as e:\n",
    "        merged_model = model\n",
    "        print('Could not merge LoRA (using current model):', repr(e))\n",
    "else:\n",
    "    merged_model = model\n",
    "\n",
    "# Compare generations after tuning (or baseline if skipped)\n",
    "for p in baseline_prompts:\n",
    "    text = generate(merged_model, tokenizer, prompt=p, max_tokens=128, verbose=False)\n",
    "    print('\\n=== AFTER / CURRENT MODEL ===\\nPrompt:', p, '\\nOutput:', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6a746",
   "metadata": {},
   "source": [
    "# Exercises & Extensions\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "1. **Baseline Generation**: Run model on 3-5 prompts BEFORE fine-tuning. Save outputs. These will be your \"before\" samples for comparison.\n",
    "2. **Dataset Inspection**: Print first 5 instruction/response pairs from the dataset. Are they diverse? Do they cover multiple tasks?\n",
    "3. **LoRA Rank Sensitivity**: Train with lora_rank = 8, 16, 32, 64. For each, plot final loss and memory usage. What's the trade-off?\n",
    "\n",
    "## Intermediate\n",
    "\n",
    "4. **Fine-tuning Convergence**: Plot training loss vs. step. Does it decrease monotonically? Does validation loss follow similar trends? Where does overfitting start?\n",
    "5. **Before/After Comparison**: Generate text with the same prompts before and after fine-tuning. Is the tuned model more instruction-following? Save side-by-side comparison.\n",
    "6. **Learning Rate Scheduling**: Train with fixed lr, then try cosine decay or linear warmup. Which converges fastest to lowest validation loss?\n",
    "\n",
    "## Advanced\n",
    "\n",
    "7. **Adapter Analysis**: Extract and visualize LoRA weights (U and V matrices). Do they learn meaningful structure? Compute singular values—what's the effective rank?\n",
    "8. **Domain Transfer**: Fine-tune on domain-specific instructions (e.g., medical Q&A, code generation). Does targeted tuning help? Test on out-of-domain prompts to measure generalization.\n",
    "9. **Few-Shot Evaluation**: Create a small evaluation set with exact gold answers. Fine-tune with 1%, 10%, 100% of data. Plot exact-match accuracy vs. data fraction. How much data needed for reasonable performance?\n",
    "\n",
    "---\n",
    "\n",
    "# Summary & Bridge Forward\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- **Fine-Tuning vs. Pretraining**: Pretraining learns from billions of tokens (expensive); fine-tuning adapts to task on thousands of tokens (cheap)\n",
    "- **LoRA (Low-Rank Adaptation)**: Train small rank-16 matrices instead of full 7B parameters; 99% fewer trainable parameters, same performance gains\n",
    "- **Instruction Tuning**: Format data as \"system + instruction + response\"; model learns to follow instructions rather than just predict text\n",
    "- **Quantization**: 4-bit reduces memory 4×; marginal quality loss for huge inference speedup\n",
    "- **Adapter Merging**: LoRA weights stay separate; can save 100MB instead of 14GB; merge at inference time for speed\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Fine-tuning is the **practical entry point to LLM development**:\n",
    "\n",
    "1. **Cost**: Fine-tuning is 100-1000× cheaper than pretraining\n",
    "   - Pretraining Mistral: weeks on 1000s of GPUs = millions of $\n",
    "   - Fine-tuning Mistral: hours on 1 GPU = hundreds of $\n",
    "\n",
    "2. **Speed**: Accessible to researchers, startups, enterprises\n",
    "   - Democratizes LLM access\n",
    "   - Enables rapid prototyping\n",
    "   - Allows domain specialization (medical LLMs, code LLMs, etc.)\n",
    "\n",
    "3. **Production Viability**:\n",
    "   - Start with Mistral/GPT/LLaMA\n",
    "   - Fine-tune on your data\n",
    "   - Deploy as API or edge model\n",
    "   - This is how most LLM products work today\n",
    "\n",
    "## Bridge to Next Projects\n",
    "\n",
    "- **Project 17 (Comparative Analysis)**: Compare outputs of base vs. tuned models systematically\n",
    "  - Quantitative metrics: perplexity, exact match, BLEU/ROUGE\n",
    "  - Qualitative analysis: does tuning improve instruction following?\n",
    "  - When does tuning help? When does it hurt?\n",
    "\n",
    "- **Further Work**:\n",
    "  - **Scaling**: Tune larger models (13B, 34B, 70B)\n",
    "  - **Evaluation**: Benchmark on published datasets (MMLU, HellaSwag, TruthfulQA)\n",
    "  - **Deployment**: Package as API (LM Studio, vLLM, SGLang)\n",
    "  - **Monitoring**: Track prompt-response patterns; detect drift\n",
    "\n",
    "## Your Takeaway\n",
    "\n",
    "> **Fine-tuning is practical LLM customization.** LoRA reduces trainable parameters from billions to millions while retaining performance. This makes LLM adaptation accessible and affordable, enabling rapid task-specific deployments.\n",
    "\n",
    "---\n",
    "\n",
    "# Performance Notes\n",
    "\n",
    "- **LoRA Memory**: Full 7B parameters = 14GB (float16); LoRA rank-16 = 50-100MB overhead\n",
    "- **4-Bit Quantization**: Reduces memory ~4×; quality loss minimal on instruction tasks\n",
    "- **Training Speed**: ~100-200 tokens/second on M4 GPU with MLX\n",
    "- **Convergence**: Instruction tuning typically converges in 1-5 epochs\n",
    "- **Inference**: Base model ≈ 10-20 tok/s; LoRA merged has same speed as base\n",
    "- **Typical LoRA Rank**: 8-32 for most tasks; 64+ for complex domains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce734327",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "**What we did:** Loaded Mistral 7B (quantized), built a tiny toy instruction dataset, optionally applied LoRA adapters (if available), and demonstrated a guarded training loop plus generation comparison.\n",
    "\n",
    "**Why parameter-efficient tuning:** LoRA updates a small set of low-rank matrices instead of all billions of weights, drastically cutting memory & compute while retaining performance gains on the target domain.\n",
    "\n",
    "**To continue:**\n",
    "1. Set `DRY_RUN=False` and increase `max_steps` for a longer fine-tune.\n",
    "2. Replace `toy_instructions.jsonl` with a real dataset (e.g. Alpaca, Dolly) formatted into instruction/response pairs.\n",
    "3. Experiment with different `lora_rank`, `learning_rate`, and `quantize` settings.\n",
    "4. Evaluate using perplexity or task benchmarks (e.g., few-shot QA).\n",
    "5. Package LoRA adapters for distribution or merge & export a fully tuned model.\n",
    "\n",
    "**Potential Improvements:**\n",
    "- Add proper masking for system/user/assistant roles.\n",
    "- Implement curriculum or mixed precision.\n",
    "- Add evaluation harness tracking exact match / BLEU / Rouge depending on task.\n",
    "\n",
    "Proceed to Project 17 for comparative analysis between base vs tuned outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
