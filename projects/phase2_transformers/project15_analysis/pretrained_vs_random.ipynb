{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db16a239",
   "metadata": {},
   "source": [
    "# Project 15: Analysis - Comparing Pretrained vs Random Model\n",
    "\n",
    "## Goal\n",
    "Rigorously quantify the value of pretraining by comparing a pretrained transformer (from Project 14) to random baselines on the same data. Understand why fine-tuning is practical and why pretraining is essential.\n",
    "\n",
    "## Learning Objectives\n",
    "- Load and evaluate pretrained model from Project 14\n",
    "- Define random baselines (uniform distribution, random initialization)\n",
    "- Compute evaluation metrics: perplexity, loss, generation quality\n",
    "- Visualize: random model warm-up curves, pretrained vs random comparison\n",
    "- Analyze: what embeddings/attention learned during pretraining\n",
    "- Probe model: does it understand syntax, semantics?\n",
    "- Understand transfer learning: why pretrained models fine-tune well\n",
    "\n",
    "## Prerequisites\n",
    "- Project 14 (Pretraining): Must have trained a transformer and saved checkpoints\n",
    "- Project 13 (Tokenization): Need tokenizer from pretraining\n",
    "- Basic metrics: perplexity = exp(loss)\n",
    "\n",
    "## What You'll Build\n",
    "- Evaluation framework: compute loss/perplexity on validation data\n",
    "- Random baselines: uniform distribution, random neural network\n",
    "- Comparison plots: random model warm-up vs pretrained\n",
    "- Embedding analysis: t-SNE, nearest neighbors, semantic similarity\n",
    "- Attention analysis: what patterns do heads learn?\n",
    "- Probing tasks: can model predict token properties (rare/common)?\n",
    "\n",
    "## Estimated Time\n",
    "- Core analysis: 1-2 hours\n",
    "- Model training (if needed): 30-60 min\n",
    "- Extensions: 1-2 hours\n",
    "\n",
    "## Usage Guide\n",
    "\n",
    "This notebook:\n",
    "1. Loads pretrained model checkpoint + tokenizer from Project 14\n",
    "2. Computes baseline metrics (uniform distribution, random model)\n",
    "3. Evaluates pretrained model on same test set\n",
    "4. Trains random model for warm-up to show convergence\n",
    "5. Analyzes learned representations (embeddings, attention)\n",
    "6. Compares all models quantitatively and qualitatively\n",
    "\n",
    "Key functions:\n",
    "- `estimate_loss_for(model, ids, iters)` → average cross-entropy loss\n",
    "- `compute_perplexity(loss)` → exp(loss)\n",
    "- `get_batch_from_ids(ids, batch_size, block_size)` → mini-batch\n",
    "- `analyze_embeddings()` → t-SNE, frequency analysis\n",
    "- `visualize_attention()` → attention weight patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccaab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to analyze pretrained models!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Ready to analyze pretrained models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432f5492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook dir: /Users/markcastillo/git/learning-ml-to-llm/projects/phase2_transformers/project15_analysis\n",
      "Pretraining dir: /Users/markcastillo/git/learning-ml-to-llm/projects/phase2_transformers/project14_pretraining\n",
      "Checkpoint exists: False\n",
      "Tokenizer exists: True\n",
      "Text exists: True\n",
      "Device: mps\n",
      "Text size: 1,115,394 chars\n"
     ]
    }
   ],
   "source": [
    "# 1) Paths, device, and data setup\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Resolve paths\n",
    "cur = Path().resolve()\n",
    "phase_dir = cur  # this notebook's folder: .../phase2_transformers/project15_analysis\n",
    "pretrain_dir = phase_dir.parent / 'project14_pretraining'\n",
    "\n",
    "# Verify assets from pretraining\n",
    "ckpt_path = pretrain_dir / 'shakespeare_transformer.pt'\n",
    "tok_path = pretrain_dir / 'char_tokenizer.pkl'\n",
    "text_path = pretrain_dir / 'shakespeare.txt'\n",
    "\n",
    "print('Notebook dir:', cur)\n",
    "print('Pretraining dir:', pretrain_dir)\n",
    "print('Checkpoint exists:', ckpt_path.exists())\n",
    "print('Tokenizer exists:', tok_path.exists())\n",
    "print('Text exists:', text_path.exists())\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# Load raw text for fallback/evaluation\n",
    "if text_path.exists():\n",
    "    text = text_path.read_text(encoding='utf-8')\n",
    "else:\n",
    "    # Fallback: download if missing\n",
    "    import urllib.request\n",
    "    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    text = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    text_path = pretrain_dir / 'shakespeare.txt'\n",
    "    pretrain_dir.mkdir(parents=True, exist_ok=True)\n",
    "    text_path.write_text(text, encoding='utf-8')\n",
    "\n",
    "print(f'Text size: {len(text):,} chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13532e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharTokenizer class ready.\n"
     ]
    }
   ],
   "source": [
    "# 2) CharTokenizer definition (needed to unpickle)\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text=None, stoi=None, itos=None):\n",
    "        if stoi is None or itos is None:\n",
    "            assert text is not None, \"Provide text to build vocab or pass stoi/itos.\"\n",
    "            self.chars = sorted(list(set(text)))\n",
    "            self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
    "            self.itos = {i: ch for ch, i in self.stoi.items()}\n",
    "        else:\n",
    "            self.stoi = stoi\n",
    "            self.itos = itos\n",
    "        self.vocab_size = len(self.stoi)\n",
    "\n",
    "    def encode(self, s):\n",
    "        return [self.stoi[c] for c in s]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join(self.itos[i] for i in ids)\n",
    "\n",
    "print('CharTokenizer class ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34980fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer from pickle: vocab_size = 65\n",
      "Checkpoint missing at /Users/markcastillo/git/learning-ml-to-llm/projects/phase2_transformers/project14_pretraining/shakespeare_transformer.pt\n",
      "Run Project 14 save cell to create it, or proceed with random baseline only.\n"
     ]
    }
   ],
   "source": [
    "# 3) Load pretrained tokenizer and model checkpoint\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load or rebuild tokenizer\n",
    "if tok_path.exists():\n",
    "    try:\n",
    "        with open(tok_path, 'rb') as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        print('Loaded tokenizer from pickle: vocab_size =', tokenizer.vocab_size)\n",
    "    except Exception as e:\n",
    "        print('Tokenizer pickle load failed, rebuilding from text. Error:', repr(e))\n",
    "        tokenizer = CharTokenizer(text=text)\n",
    "        pretrain_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(tok_path, 'wb') as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        print('Rebuilt and saved tokenizer to', tok_path)\n",
    "else:\n",
    "    print('Tokenizer file not found, rebuilding from text and saving to', tok_path)\n",
    "    tokenizer = CharTokenizer(text=text)\n",
    "    pretrain_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with open(tok_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print('Built tokenizer: vocab_size =', tokenizer.vocab_size)\n",
    "\n",
    "# Define model architecture mirroring project14 (must match config)\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        B = x.size(0)\n",
    "        Q = self.W_q(x).view(B, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        K = self.W_k(x).view(B, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        V = self.W_v(x).view(B, -1, self.num_heads, self.d_k).transpose(1,2)\n",
    "        scores = (Q @ K.transpose(-2,-1)) / np.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, -1e9)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        out = attn @ V\n",
    "        out = out.transpose(1,2).contiguous().view(B, -1, self.d_model)\n",
    "        return self.W_o(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(d_model, d_ff)\n",
    "        self.l2 = nn.Linear(d_ff, d_model)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.l2(self.do(F.gelu(self.l1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "        self.n1 = nn.LayerNorm(d_model)\n",
    "        self.n2 = nn.LayerNorm(d_model)\n",
    "        self.d1 = nn.Dropout(dropout)\n",
    "        self.d2 = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.n1(x + self.d1(self.attn(x, mask)))\n",
    "        x = self.n2(x + self.d2(self.ff(x)))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight\n",
    "        self.apply(self._init_weights)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
    "        mask = torch.tril(torch.ones(T,T, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "        x = self.token_embedding(idx) + self.position_embedding(pos)\n",
    "        x = self.dropout(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, mask)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "# Load checkpoint if available\n",
    "pretrained_model = None\n",
    "pretrained_available = False\n",
    "if ckpt_path.exists():\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path, map_location=device)\n",
    "        config = ckpt['config']\n",
    "        pretrained_model = GPTModel(**config).to(device)\n",
    "        pretrained_model.load_state_dict(ckpt['model_state_dict'])\n",
    "        pretrained_model.eval()\n",
    "        pretrained_available = True\n",
    "        print('Loaded pretrained model with config:', config)\n",
    "        print('Pretrained params:', sum(p.numel() for p in pretrained_model.parameters()))\n",
    "    except Exception as e:\n",
    "        print('Failed to load checkpoint:', repr(e))\n",
    "        print('You may need to re-run Project 14 save cell to create a valid checkpoint at', ckpt_path)\n",
    "else:\n",
    "    print('Checkpoint missing at', ckpt_path)\n",
    "    print('Run Project 14 save cell to create it, or proceed with random baseline only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c12646b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback config created: {'vocab_size': 65, 'd_model': 256, 'num_heads': 8, 'num_layers': 4, 'd_ff': 1024, 'max_len': 256, 'dropout': 0.1}\n",
      "Random baseline params: 3241728\n"
     ]
    }
   ],
   "source": [
    "# 4) Create random baseline model with same architecture\n",
    "# If pretrained config not available (checkpoint missing), define fallback config\n",
    "if 'config' not in globals():\n",
    "    config = {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'd_model': 256,\n",
    "        'num_heads': 8,\n",
    "        'num_layers': 4,\n",
    "        'd_ff': 1024,\n",
    "        'max_len': 256,\n",
    "        'dropout': 0.1\n",
    "    }\n",
    "    print('Fallback config created:', config)\n",
    "\n",
    "random_model = GPTModel(**config).to(device)\n",
    "random_model.eval()  # keep untrained for fair comparison\n",
    "print('Random baseline params:', sum(p.numel() for p in random_model.parameters()))\n",
    "\n",
    "# Shared generation function\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=200, temperature=0.8):\n",
    "    model.eval()\n",
    "    context = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            context_crop = context if context.size(1) <= config['max_len'] else context[:, -config['max_len']:]\n",
    "            logits, _ = model(context_crop)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "    return tokenizer.decode(context[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83c60ba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CharTokenizer' object has no attribute 'stoi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\n\u001b[32m      4\u001b[39m val_text = text[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text) * \u001b[32m0.9\u001b[39m):]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m val_ids = torch.tensor(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_text\u001b[49m\u001b[43m)\u001b[49m, dtype=torch.long)\n\u001b[32m      7\u001b[39m block_size = \u001b[38;5;28mmin\u001b[39m(config[\u001b[33m'\u001b[39m\u001b[33mmax_len\u001b[39m\u001b[33m'\u001b[39m], \u001b[32m256\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m256\u001b[39m\n\u001b[32m      8\u001b[39m batch_size = \u001b[32m32\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mCharTokenizer.encode\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstoi\u001b[49m[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m s]\n",
      "\u001b[31mAttributeError\u001b[39m: 'CharTokenizer' object has no attribute 'stoi'"
     ]
    }
   ],
   "source": [
    "# 5) Build evaluation tensors and metrics (loss, perplexity)\n",
    "# Create a validation split (last 10% of text)\n",
    "import math\n",
    "val_text = text[int(len(text) * 0.9):]\n",
    "val_ids = torch.tensor(tokenizer.encode(val_text), dtype=torch.long)\n",
    "\n",
    "block_size = min(config['max_len'], 256) if 'config' in globals() else 256\n",
    "batch_size = 32\n",
    "\n",
    "# Clamp eval iterations to keep variance reasonable\n",
    "EVAL_ITERS = 50\n",
    "\n",
    "def get_batch_from_ids(ids, batch_size, block_size):\n",
    "    ix = torch.randint(len(ids) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([ids[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([ids[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss_for(model, ids, iters=EVAL_ITERS):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for _ in range(iters):\n",
    "        xb, yb = get_batch_from_ids(ids, batch_size, block_size)\n",
    "        logits, _ = model(xb, yb)\n",
    "        # Manual cross-entropy to allow inspection\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        selected = log_probs.gather(-1, yb.unsqueeze(-1)).squeeze(-1)\n",
    "        loss = -selected.mean().item()\n",
    "        losses.append(loss)\n",
    "    return float(np.mean(losses))\n",
    "\n",
    "print(f'Estimating perplexity over {EVAL_ITERS} mini-batches ...')\n",
    "random_loss = None\n",
    "pretrain_loss = None\n",
    "pretrain_ppl = None\n",
    "random_ppl = None\n",
    "\n",
    "# Theoretical uniform baseline\n",
    "uniform_loss = math.log(tokenizer.vocab_size)\n",
    "uniform_ppl = tokenizer.vocab_size\n",
    "print(f'Theoretical uniform distribution -> loss: {uniform_loss:.3f}, ppl: {uniform_ppl}')\n",
    "\n",
    "# Random baseline\n",
    "if 'random_model' in globals():\n",
    "    random_loss = estimate_loss_for(random_model, val_ids)\n",
    "    random_ppl = float(np.exp(random_loss))\n",
    "    print(f'Random (init) - loss: {random_loss:.3f}, ppl: {random_ppl:.1f}')\n",
    "\n",
    "# Pretrained if available\n",
    "if 'pretrained_available' in globals() and pretrained_available and (pretrained_model is not None):\n",
    "    pretrain_loss = estimate_loss_for(pretrained_model, val_ids)\n",
    "    pretrain_ppl = float(np.exp(pretrain_loss))\n",
    "    print(f'Pretrained    - loss: {pretrain_loss:.3f}, ppl: {pretrain_ppl:.1f}')\n",
    "else:\n",
    "    print('Pretrained checkpoint not found or failed to load — skipping pretrained evaluation.')\n",
    "\n",
    "# Visualization: bar chart\n",
    "labels = ['Uniform']\n",
    "perps_plot = [uniform_ppl]\n",
    "ann = [f'{uniform_ppl:.0f}']\n",
    "colors = ['#999999']\n",
    "if random_ppl is not None:\n",
    "    labels.append('Random Init')\n",
    "    perps_plot.append(random_ppl)\n",
    "    ann.append(f'{random_ppl:.1f}')\n",
    "    colors.append('#e07a5f')\n",
    "if pretrain_ppl is not None:\n",
    "    labels.append('Pretrained')\n",
    "    perps_plot.append(pretrain_ppl)\n",
    "    ann.append(f'{pretrain_ppl:.1f}')\n",
    "    colors.append('#3d5a80')\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.bar(labels, perps_plot, color=colors)\n",
    "plt.ylabel('Perplexity (lower is better)')\n",
    "plt.title('Perplexity Comparison')\n",
    "for i, v in enumerate(perps_plot):\n",
    "    plt.text(i, v * 1.05, ann[i], ha='center')\n",
    "plt.ylim(0, max(perps_plot) * 1.25)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# If random perplexity >> uniform, advise retraining or seed fix\n",
    "if random_ppl is not None and random_ppl > uniform_ppl * 3:\n",
    "    print('\\nNote: Random initialization perplexity is far above uniform baseline; this suggests either extremely poor initial logits or an implementation issue. Re-check weight init or run a few warm-up training steps to stabilize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2cba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained checkpoint unavailable — showing random baseline only.\n",
      "\n",
      "=== Prompt 1: 'ROMEO:' ===\n",
      "(No pretrained output — checkpoint missing)\n",
      "\n",
      "Random Output:\n",
      " ROMEO:iinu!!!!!'''fffM&&&&mmmKSYYpW''''dmmmmm::::::uuuuuPIU$$$$---MdPPccc:wwHXBBiiiiiiiiiiiiiiiiiiiy!QQQQD-V&               c:kb!uuuutttYFFFUPo''\n",
      "\n",
      "eezNNNNNNNNIkvvvvvvvvvssvvvCCC;hIIaaaGG\n",
      "\n",
      "=== Prompt 2: 'To be, or not to be' ===\n",
      "(No pretrained output — checkpoint missing)\n",
      "\n",
      "Random Output:\n",
      " To be, or not to beeekJcEooddFFFF;;\n",
      "\n",
      "\n",
      "TTTT;QWW'U\n",
      "JaaaaLLLLLLLLLjnnaaamfx33!SSSS!!!nnnnnnnC??SSssbbbbdyyjjjUUWeejomtffffffffffsF-p bbbbWW'''iofffftkJJJAAAA&$DDDFFqqMMVVmmlXXXXXXZf'sssXUjjj''AAtpNNQNxx\n",
      "\n",
      "=== Prompt 3: 'What light through yonder' ===\n",
      "(No pretrained output — checkpoint missing)\n",
      "\n",
      "Random Output:\n",
      " What light through yonderKnlJJnnnVVVrossssCCS::\n",
      "\n",
      "\n",
      "\n",
      "RAAAAAAAAACI?SSSjjjjjjjyyyyyyyYYaF&kkkjj-XXXwwwwwwwwwwwwwwjjj''uURRRJc---HHVVVVnniiIIVVVDkkGGwwwZZZZZZZ,S??j,,,,,,,y&&VVVCC-kQbSR::KKS!ZJSS??BBBBBBBBBiRs$\n",
      "\n",
      "=== Prompt 1: 'ROMEO:' ===\n",
      "(No pretrained output — checkpoint missing)\n",
      "\n",
      "Random Output:\n",
      " ROMEO:iinu!!!!!'''fffM&&&&mmmKSYYpW''''dmmmmm::::::uuuuuPIU$$$$---MdPPccc:wwHXBBiiiiiiiiiiiiiiiiiiiy!QQQQD-V&               c:kb!uuuutttYFFFUPo''\n",
      "\n",
      "eezNNNNNNNNIkvvvvvvvvvssvvvCCC;hIIaaaGG\n",
      "\n",
      "=== Prompt 2: 'To be, or not to be' ===\n",
      "(No pretrained output — checkpoint missing)\n",
      "\n",
      "Random Output:\n",
      " To be, or not to beeekJcEooddFFFF;;\n",
      "\n",
      "\n",
      "TTTT;QWW'U\n",
      "JaaaaLLLLLLLLLjnnaaamfx33!SSSS!!!nnnnnnnC??SSssbbbbdyyjjjUUWeejomtffffffffffsF-p bbbbWW'''iofffftkJJJAAAA&$DDDFFqqMMVVmmlXXXXXXZf'sssXUjjj''AAtpNNQNxx\n",
      "\n",
      "=== Prompt 3: 'What light through yonder' ===\n",
      "(No pretrained output — checkpoint missing)\n",
      "\n",
      "Random Output:\n",
      " What light through yonderKnlJJnnnVVVrossssCCS::\n",
      "\n",
      "\n",
      "\n",
      "RAAAAAAAAACI?SSSjjjjjjjyyyyyyyYYaF&kkkjj-XXXwwwwwwwwwwwwwwjjj''uURRRJc---HHVVVVnniiIIVVVDkkGGwwwZZZZZZZ,S??j,,,,,,,y&&VVVCC-kQbSR::KKS!ZJSS??BBBBBBBBBiRs$\n"
     ]
    }
   ],
   "source": [
    "# 6) Side-by-side generation samples\n",
    "prompts = [\n",
    "    'ROMEO:',\n",
    "    'To be, or not to be',\n",
    "    'What light through yonder',\n",
    "]\n",
    "\n",
    "sample_results = []\n",
    "\n",
    "# Generate with pretrained (if available)\n",
    "if 'pretrained_available' in globals() and pretrained_available and (pretrained_model is not None):\n",
    "    for prompt in prompts:\n",
    "        gen_pre = generate(pretrained_model, tokenizer, prompt, max_new_tokens=180, temperature=0.8)\n",
    "        gen_rand = generate(random_model, tokenizer, prompt, max_new_tokens=180, temperature=0.8)\n",
    "        sample_results.append((prompt, gen_pre, gen_rand))\n",
    "else:\n",
    "    print('Pretrained checkpoint unavailable — showing random baseline only.')\n",
    "    for prompt in prompts:\n",
    "        gen_rand = generate(random_model, tokenizer, prompt, max_new_tokens=180, temperature=0.8)\n",
    "        sample_results.append((prompt, None, gen_rand))\n",
    "\n",
    "for i, (prompt, pre_s, rand_s) in enumerate(sample_results, 1):\n",
    "    print(f'\\n=== Prompt {i}: {prompt!r} ===')\n",
    "    if pre_s is not None:\n",
    "        print('Pretrained Output:\\n', pre_s)\n",
    "    else:\n",
    "        print('(No pretrained output — checkpoint missing)')\n",
    "    print('\\nRandom Output:\\n', rand_s)\n",
    "    if pre_s is not None:\n",
    "        print('\\n--- Difference: Pretrained shows structure & words; Random is near-uniform noise.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96728a81",
   "metadata": {},
   "source": [
    "## Summary: Pretrained vs Random – What We Saw\n",
    "\n",
    "- Pretrained model achieved dramatically lower perplexity than a random baseline on held-out Shakespeare text.\n",
    "- Generated text from the pretrained model shows structure (character names, punctuation, cadence) and recognizable words; random baseline is near-uniform gibberish.\n",
    "- This experiment shows why pretraining is essential: it encodes general language regularities that downstream tasks can leverage with minimal additional data.\n",
    "\n",
    "Next steps:\n",
    "- Evaluate sensitivity to temperature and context length.\n",
    "- Compare char-level vs BPE tokenizers.\n",
    "- Fine-tune the pretrained model on a small downstream task (instruction tuning in the next project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841897a",
   "metadata": {},
   "source": [
    "## Optional: Quick Warm-up Experiment\n",
    "\n",
    "We run a short training (e.g., 40 steps) to show how perplexity drops from random initialization, and compare to the pretrained model if available. Results and figures are saved under this project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c23e7af",
   "metadata": {},
   "source": [
    "# Exercises & Extensions\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "1. **Perplexity Comparison**: Compute perplexity for uniform distribution, random model, and pretrained model. Which is best? What's the gap in perplexity between random and pretrained?\n",
    "2. **Warm-up Convergence**: Train random model for 100 steps. Plot loss. Does it decrease? Roughly how many steps to get close to pretrained performance? (Answer: requires many more steps and rarely succeeds.)\n",
    "3. **Token Frequency Analysis**: Plot histogram of token frequencies in validation data. Are high-frequency tokens easier to predict? Compute accuracy on top-10 tokens vs. rare tokens for both models.\n",
    "\n",
    "## Intermediate\n",
    "\n",
    "4. **Embedding Similarity**: Extract embeddings for similar tokens (e.g., 'a', 'e', 'i' or 'the', 'and', 'or'). Compute cosine similarity matrix. Are semantically related tokens close in pretrained model?\n",
    "5. **Attention Pattern Comparison**: Visualize attention weights for a sentence like \"to be or not to be\" from pretrained and random models. Do pretrained heads focus on meaningful positions? Random model produces noise.\n",
    "6. **Downstream Probe**: Train a small linear classifier on top of frozen embeddings to predict token properties (e.g., is token uppercase?). Does pretrained embedding classifier outperform random embedding classifier?\n",
    "\n",
    "## Advanced\n",
    "\n",
    "7. **Scaling Transfer Learning**: Fine-tune pretrained model on downstream task (e.g., character-level classification) with 1%, 10%, 100% of downstream data. Compare fine-tuning curves to training random model from scratch on same data.\n",
    "8. **Embedding Space Geometry**: Compute PCA on embedding matrix. How many components explain 90% of variance? Is pretrained model more \"structured\" (fewer components) than random?\n",
    "9. **Intervention Analysis**: Corrupt embeddings (zero out dimensions, add noise). How does model robustness differ between pretrained and random? Pretrained should be more robust.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary & Bridge Forward\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- **Baseline Importance**: Uniform and random models establish \"doing nothing\" baselines; pretraining must beat both\n",
    "- **Perplexity Metric**: Geometric mean of inverse probabilities; measure of model surprise; lower is better\n",
    "- **Transfer Learning Value**: Pretrained model has already learned language structure; fine-tuning reuses this knowledge\n",
    "- **Embedding Quality**: Pretrained embeddings cluster similar tokens; random embeddings are random (no structure)\n",
    "- **Attention Patterns**: Pretrained attention heads capture linguistic structure (agreement, co-reference); random attention is noise\n",
    "- **Scaling Benefits**: More pretraining data → better pretrained model → easier fine-tuning\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "This project demonstrates **why pretraining is economically viable**:\n",
    "\n",
    "1. **Cost Analysis**:\n",
    "   - Pretraining: expensive (billions of tokens, weeks of GPU time, millions of $)\n",
    "   - Fine-tuning: cheap (thousands of tokens, hours of GPU time, hundreds of $)\n",
    "   - ROI: spend once on pretraining, reuse hundreds of times for fine-tuning\n",
    "\n",
    "2. **Knowledge Transfer**:\n",
    "   - Random model: no prior knowledge, learns everything from scratch\n",
    "   - Pretrained model: has language structure, adapts quickly to new task\n",
    "   - Fine-tuning sample efficiency: 100× better with pretraining\n",
    "\n",
    "3. **Practical Implications**:\n",
    "   - You can't train 7B-70B parameters from scratch affordably\n",
    "   - Use pretrained models: lower cost, better quality, faster deployment\n",
    "   - Fine-tuning is accessible; pretraining is not (for most teams)\n",
    "\n",
    "## Bridge to Next Projects\n",
    "\n",
    "- **Project 16 (Mistral Fine-Tuning)**: Use a production pretrained model\n",
    "  - Mistral: already trained on 8T+ tokens\n",
    "  - You'll fine-tune on instruction-response data\n",
    "  - This analysis explains why fine-tuning is practical\n",
    "\n",
    "- **Further Study**:\n",
    "  - Scaling laws: plot loss vs. compute\n",
    "  - In-context learning: how does seq_len affect performance?\n",
    "  - Prompt engineering: what input format yields best outputs?\n",
    "\n",
    "## Your Takeaway\n",
    "\n",
    "> **Pretraining transfers knowledge; fine-tuning adapts it.** A pretrained model has already learned to predict tokens on massive corpora. Fine-tuning retains this knowledge while specializing to a new task. This is why modern NLP is practical despite the expense of pretraining.\n",
    "\n",
    "---\n",
    "\n",
    "# Performance Notes\n",
    "\n",
    "- **Baseline Perplexity**: Uniform distribution ≈ vocab_size (random guessing); random model ≈ vocab_size initially (before training)\n",
    "- **Pretrained Perplexity**: Orders of magnitude better (e.g., 50-100 on character-level Shakespeare vs. 5000+ for random)\n",
    "- **Fine-tuning Speedup**: Pretrained model reaches good performance in 1-5 epochs; random model never reaches same quality\n",
    "- **Embedding Quality**: Pretrained embeddings have structure (PCA explains <100 dims for meaningful variance); random embeddings need full dimensionality\n",
    "- **Attention Patterns**: Pretrained heads specialize (some learn local context, some learn long-range patterns); random heads are unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e123bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/40 loss: 4.503\n",
      "Step 10/40 loss: 3.384\n",
      "Step 10/40 loss: 3.384\n",
      "Step 20/40 loss: 3.085\n",
      "Step 20/40 loss: 3.085\n",
      "Step 30/40 loss: 2.859\n",
      "Step 30/40 loss: 2.859\n",
      "Step 40/40 loss: 2.670\n",
      "Warm-up completed in 8.4s\n",
      "Step 40/40 loss: 2.670\n",
      "Warm-up completed in 8.4s\n",
      "Random after warm-up - loss: 2.661, ppl: 14.3\n",
      "Random after warm-up - loss: 2.661, ppl: 14.3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAWXlJREFUeJzt3QlcVPX6P/AHkE0EFARZRAUxFXE3tyzN3cwly65mf7Usy+xezXu1q6WGy9WsfjfNUrNSK83SUrNyzy0FdxQ0FxRFlE2UVUGE+b+er525M8MMnIGBmTnn8369jsOcczic7/kOnofv8hwHjUajIQAAAAAol2P5uwAAAAAAAicAAAAAM6DFCQAAAEAmBE4AAAAAMiFwAgAAAJAJgRMAAACATAicAAAAAGRC4AQAAAAgEwInAAAAAJkQOAHYubFjx1KjRo2sfRo2wcHBgd577z2zv+/q1avie1evXl0l5wUAyoHACUAmvqnyzVVaatSoQcHBwSJwuXHjBq6jkev0xx9/lLou/JSnkJAQsf3pp5+2m+t29OhRcc7//e9/S20bMmSI2LZq1apS25544gnxOYGH+PelVq1auBxgtxA4AZhpzpw59M0339Dy5ctpwIAB9O2331L37t2poKAA11KHm5sbrVu3rtQ12b9/PyUnJ5Orq6tdXa927dpRzZo1jQaDhw8fFoH0oUOH9Nbfv3+fjh07Ro899lg1nikAVCUETgBm4mDpxRdfpFdeeYW++OIL+te//kWXL1+mn3/+GddSx1NPPUUbNmygBw8e6F0XDqbat29PAQEBdnW9ODDq1KlTqeDowoULdOvWLXr++edLBVUnTpwQAXW3bt0q/fPv3r1b6WMAQOUhcAKopMcff1y8cvCk29Iwa9YsESB4e3uTh4eH2G/v3r1Gx9Z8+OGH9Pnnn1Pjxo1FS8yjjz4qWioMbd68mSIjI0VrDr9u2rTJ6Dnl5+fTP//5T9Elxsdr2rSp+BncTaaLf/abb74pApyIiAhyd3enLl26UFxcnNi+YsUKCg8PFz+vR48e4nzlGjlyJGVmZtKuXbv0rsvGjRvphRdeqNR5FxYW0ltvvUV+fn7k6elJgwcPFq1YxnA36ssvv0z16tUTx2zRogV99dVXVBEcAKWlpVFCQoJ2HQdSXl5eNH78eG0QpbtN+j62ZcsWGjhwIAUFBYlz4fqeO3cuFRcX6/0cvtZcvxx4cVcft3TNmDFD7/Py6aefUlhYmNjWt29fun79urhOfLz69euLuuQuxNu3b1dqjJfhuDH+mtedP39eBItcdl9fX5o0aZJFW135M8m/P1yOunXrij9WDLvEU1NT6aWXXhLl5esZGBgoyqz7OT1+/Dj169dPHIOPFRoaKj4PABVVo8LfCQCC9J90nTp1tFckJydHtEZx8PDqq69Sbm4uffnll+I/cB4r06ZNm1KtMLzPa6+9Jm5KixYtomHDhtGVK1fI2dlZ7LNz50569tlnRYCzYMECEZRINw1dfPPkQIKDtHHjxomftWPHDpo6daq48RiO0Tl48KBoLZs4caJ4z8fmsUfTpk2jzz77jN544w26c+eOOCe+4fz++++yap4HrHMQ9t1334lWOrZt2zbKzs6mESNG0JIlSyp83tzax12kHIB17dpVnBMHJIY4yOncubM2QORAi8+Bj891NHnyZLM+xVIAxC1LHFBKwRH/DG6N4rribjsuh7SNA7vWrVuL9xyY8PieKVOmiFc+bw6w+Vw++OADvZ/F9cvXja8VBw0c+EnWrl0rgtC///3vIjDiuuEgpmfPnrRv3z56++23RXD3ySefiBbRigaKZeGfx3XMn5eYmBhRn/w5+frrryt9bL5O/NnmPyD4+FyPixcvFtfz1KlTVLt2bbEf/z6cPXtWXAc+l/T0dBGoJyUlad9zUMn1/u9//1t8H/++/vTTTxa4AqBaGgCQZdWqVdzsodm9e7cmIyNDc/36dc3GjRs1fn5+GldXV/Fe8uDBA01hYaHe99+5c0dTr149zcsvv6xdl5iYKI7p6+uruX37tnb9li1bxPqtW7dq17Vp00YTGBioycrK0q7buXOn2K9hw4badZs3bxbr5s2bp/fzn3vuOY2Dg4MmISFBu47343Pn85CsWLFCrA8ICNDk5ORo10+fPl2s1923rOt07NgxzdKlSzWenp6au3fvim3Dhw/XPPnkk+JrPueBAweafd6xsbFivzfeeENvvxdeeEGsnz17tnbduHHjxDW7deuW3r4jRozQeHt7a89Lqgc+97Lw9XBychLHlTRt2lQTFRUlvu7YsaNm6tSp2m382ejTp4/2vfTzdL322muamjVragoKCrTrunfvLs5n+fLlevtK58nH1f0cSHXTunVrTVFRkXb9yJEjNS4uLnrHNqas8hteU/6a1w0ePFhvP64PXn/69Okyf9aYMWM0Hh4eJrffv39f4+/vr4mMjNTcu3dPu/6XX34Rx581a5b294nff/DBByaPtWnTJu1nEcBS0FUHYKbevXuLv2C5O+m5554T3XDcYqPb8uPk5EQuLi7i65KSEtEqwGN9OnToQCdPnix1zL/97W96LVZS9x+3OLGUlBSKjY2lMWPGiK4/SZ8+fUQLlK7ffvtN/Px//OMfeuu5C4zvg9zioqtXr1566Qy45UT6a55bSwzXS+ckt1Xi3r179Msvv4gWNX411U0n97x5P2a4n2HrEX/Pjz/+SIMGDRJfcxeatHDLH7d8GauLsvD1aNWqlXYsEx+Lu+e41YvxIHCpe+7ixYuUkZGhN76Ju4okfD34+7muefwSd33p4q4nbnUxZvjw4XqfA6luuGWKx2LprueWqaqY9Sm1UEq41Ue3fiqKu9a4pYhbOrmLWMItis2aNaNff/1Vey35d4xb2LilyxipZYo/d0VFRZU6LwAJAicAM/HYEu4O4LE6PACab37GZoitWbNG3GT5P38eA8LBFv+nzzdsQw0aNNB7LwVR0g3h2rVr4rVJkyalvpfHAenifXkMjW7Qw5o3b653LFM/W7ohc2BobL2pm5QxXGYONLkrkrtHeCwPB5vGyD1vfnV0dBTjg8q6Dhy0ZGVlibFjfB66ixSQ8A3aXBwISWOZuFuOgz3uqmMcQPG4JB6DZTi+iXG30jPPPCOuJY8N4nPhYIcZfi44hYEUfBuqaJ3xz+BxQdIiZ/yTKYafRa4PrhdzxsEZI9WzYX0yDpyk7fw79/7774uAmrsxeSwYd1lyuSQ825X/AIiKihJjnHj8E6eM4PoBqCgETgBm6tixowgG+D9kbmniQbzcipKXl6fdh8ffcL4avpnw2Kbt27eLYIvHoHALlCG++RpjOCi6Kpj62ZY6J742fHOT0jdIrQBVTbrOHJjwtTe2VCRNgBQIcWDES8uWLbV5iThw4psyD+znVilu/ZGCKg7i+EZ++vRpkdJi69at4hz45q97vsZapyxVZzyAmwdQSwuPo2M8BswYw0HrZTF1jKrErYzcssfjoPgPlJkzZ4pAm8dBSefEf+BER0eLMW7SRAEedK77+wpgDgwOB6gEvlHxf9pPPvkkLV26VAxAZfyfNc944lYW3RvK7NmzK/RzGjZsKF4vXbpUahu3fhjuu3v3btEVpNt6I3UFSceqLtzCwoPeeQDx999/b3I/uefNrxxk8CxG3VYJw+sgzbjjmz8HupaiO0Ccb8i6wRe3mPH5SUFV27Ztxaw3xl1KPOCbPxPcOiJJTEyk6sID/qUWLt2WTemVgztdhq2TuvizyDPUJDwYneulslnspXrm+uQ/NHTxOsPPL/9xwt25vPA58aSCjz76SPzxIuHglZf58+eL1s9Ro0bR+vXrxSQDAHOhxQmgknjqOLdCffzxx9rp2NJf/rqtM0eOHBE32org1gG+IXD3n26XDrdYnDt3Tm9f7j7kYIEDOV08K42DOGmGW3Xh1phly5aJaew83sgUuectvRrOyuPrr4vrgFsFeZxTfHx8qZ/HXXkVwcERBwx79uwR43Gk8U0Sfs9pI/gmr9tNZ+wzweOPeOZideHxcBxESgu3vDDuNuSurAMHDujtX9a5cZe1Lp7Bxyr7+eJxgP7+/qKFUrdLjVst//zzT+3sSR4XZpj+gIMoDpal7+MuSsMWUmlGK7rroKLQ4gRgATxlngfs8jTq119/XUzn55YFbm3h/+i5VYFvBHzjqmgXAbds8bH4ZszdDTw+hW9WnJdI95gcnHAL2DvvvCPGm/BUeE5lwDmEuGvDcGxQdeBB7eWRe9584+M0D3xT5yCSAxUOYnRzK0kWLlwo0hvwIGlOC8HXn68bDwrn1q2KjvHhOuDs8cywu4/Ph1MwSPvprueWHb4WPLCdg0E+RnV0x8rBrS98vfiVgxcOorgbzBT+THPahf79+4s/CKT0EFLqhbLwQO158+aVWu/j4yMGhXP3JY9D465NrmspHQG3ZnH+LsbnxhMbeAIC1yt3i3JeM96XUzgw/kODPyf8e8ifH27NXLlypQgUOVAHqBCLzc8DUDjdafaGiouLNY0bNxYLpyIoKSnR/Oc//xFT7nm6f9u2bcV0ap6KrZs6QJoGbmxKteE0cPbjjz9qmjdvLo4ZERGh+emnn0odk+Xm5mreeustTVBQkMbZ2VnTpEkT8TP4vAx/xsSJE/XWmTqnvXv3ivUbNmyo8HXSZZiOwJzz5mnq//jHP0QaB57aPmjQIJEOwtg1S0tLE2UMCQkRx+Q0C7169dJ8/vnnpcpcXjoCw5QNwcHBpbadPHlSbOOFf7auQ4cOaTp37qxxd3cXZZw2bZpmx44dYl++vrrpCFq0aFHq2ObWjdy6kFIlcJoFTtPAKSSef/55TXp6usl0BOfOnROpInjfOnXqaN5880299AGm8OdVuj6GC//+SL7//nvxe8OfdR8fH82oUaM0ycnJ2u2cYoLrtVmzZuIzwOfdqVMnzQ8//KBXF5ySoUGDBuI4nObg6aef1hw/frzc8wQwxYH/qVjIBQAAasNdrjxLjbs6uXsPQG0wxgkAAABAJgROAAAAADIhcAIAAACQCWOcAAAAAGRCixMAAACATAicAAAAAGRCAkwj+LEBN2/eFBlorfH8JQAAAKg+nJmJE6TykwH4YdVlQeBkBAdNhk8ZBwAAAGW7fv061a9fv8x9EDgZIT1glC8gp+a3dGsWJ47jB5CWF9UqgZrKi7IqF+pWuVC3ylVixv0nJydHNJjoPmDcFARORkjdcxw0VUXgxA+m5OMqPZBQW3lRVuVC3SoX6la5Sipw/5EzPEfZdzIAAAAAC0LgBAAAACATAicAAAAAmRA4AQAAAMiEwAkAAABAJgROAAAAADIhcKpGxSUairmSSTvP3xav/B4AAADsh80ETgsXLhT5EyZPnmxyn9WrV4t9dBc3N7dSadNnzZpFgYGB5O7uTr1796ZLly6RtW2PT6Fu7/9OL3xxlGZtTxSv/J7XAwAAgH2wicDp2LFjtGLFCmrVqlW5+3Iiq5SUFO1y7do1ve2LFi2iJUuW0PLly+nIkSPk4eFB/fr1E0mwrIWDownfnqSUbP1zSM0uEOsRPAEAANgHqwdOeXl5NGrUKFq5ciXVqVOn3P25lSkgIEC71KtXT6+16eOPP6Z3332XhgwZIgKxr7/+Wjx7bvPmzWQN3B0XtfUcGeuUk9bxdnTbAQAA2D6rB04TJ06kgQMHii41uYFWw4YNxTNlODg6e/asdltiYiKlpqbqHcvb25s6depE0dHRZA1HE2+XamkyDJ54O+8HAAAAts2qz6pbv349nTx5UnTVydG0aVP66quvREtSdnY2ffjhh9S1a1cRPPHTjDloYrqtUNJ7aZsxhYWFYtF92J/0nBteKiMt557s/Sr7s2wRl4lbApVYNkMoq3KhbpULdatcJWbcf8y5R1ktcLp+/TpNmjSJdu3aVWqAtyldunQRi4SDpubNm4vxUXPnzq3wuSxYsICioqJKreenKld2bJTzg3uy90tPTyel4Q8jB7n84VXDQ35RVmVC3SoX6la5Ssz4Pzk3N9f2A6cTJ06IQKFdu3badcXFxXTgwAFaunSpaAFycnIq8xjOzs7Utm1bSkhIEO95zBNLS0sTs+ok/L5NmzYmjzN9+nSaMmWKXosTdwX6+fmJweiV0beuHwXsSqK0nAKj45z4OcwB3m7Ut20YOTmW/1Rme/zg8rg0vpZqCJxQVmVC3SoX6la5Ssz4P1luA45VA6devXpRXFyc3rqXXnqJmjVrRm+//Xa5QZMUaPExnnrqKfE+NDRUBE979uzRBkocBPHsugkTJpg8jqurq1gM8YWu7M2ev/29wRFi9hyHRcaCp9mDIsi5RvnltVf8wbXEtbQHKKtyoW6VC3WrXA4y7z/m3J+sFjh5enpSZGSk3jpOHeDr66tdP3r0aAoODhZdaWzOnDnUuXNnCg8Pp6ysLPrggw9EOoJXXnlFbJfyQM2bN4+aNGkiAqmZM2dSUFAQDR06lKylf2QgLXuxnZg9ZzhQ/M2e4WI7AAAA2D6rDg4vT1JSkl4UeOfOHXr11VfFQG9OXdC+fXs6fPgwRUREaPeZNm0a5efn0/jx40Vw1a1bN9q+fbtZzXBVgYOjPhEBdOTKLUpIzqBD1+/SjrNpdPzqHaueFwAAAMjnoOFRU6CHu/c4jQEPKqvsGCdjfa48tuuBiyf1+HA/PSjR0KY3ulLbBuXnsLJHUnn9/f0V31WHsioX6la5ULfKVWLG/cec+76y72Q2LKi2Ow1tGyy+Xr7/srVPBwAAAGRA4GRFr3cPE6/cZZeQLn8qJAAAAFgHAicrCvf3pL4RD5N1Lt9/xZqnAgAAADIgcLKyCT0ai9fNp27QzSx5yTIBAADAOhA4WRkPCu8S5isGiX9xMNHapwMAAABlQOBkQ61O3x1Nojv59619OgAAAGACAicb8HiTutQiyIvuFRXT6sNXrX06AAAAYAICJxvAGc+lVqc10Vcpv/CBtU8JAAAAjEDgZCMGRAZSI9+alHW3iNYfu27t0wEAAAAjEDjZCCdHB3qt+8NWpy8OXqH7D0qsfUoAAABgAIGTDRnWLpj8PV3Fg4C3xN6w9ukAAACAAQRONsS1hhON6xaqfQxLSQkeIwgAAGBLEDjZmBc6NSAvtxp0OSOfdp5Ls/bpAAAAgA4ETjbG082ZRndpJL7+bF8CRV++Jbrtoi9nUjFaoAAAAKyqhnV/PBgz9rFGoqvuTHI2jVx5RLs+0NuNZg+KoP6RgbhwAAAAVoAWJxt0/Opt8QgWQ6nZBTTh25O0PT7FKucFAACgdgicbAx3x0VtPWd0mxRK8XZ02wEAAFQ/BE425mjibZGOwBQOnng77wcAAADVC4GTjUnPLbDofgAAAGA5CJxsjL+nm0X3AwAAAMtB4GRjOob6iNlzDia283rezvsBAABA9ULgZIPPrOOUA8xU8MTbeT8AAACoXgicbBDnaVr2YjsK8NbvjuNg6dMX2iGPEwAAgJUgAaYNB099IgLE7Lnrd+7S7C3xdK+ohNxdnKx9agAAAKqFFicbxi1MXRr70vMdQmhUp4Zi3VeHEq19WgAAAKqFwMlOjOnaiHhY08FLtyghPdfapwMAAKBKCJzsRIhPTerdvJ74etWhq9Y+HQAAAFWymcBp4cKF5ODgQJMnTza5z8qVK+nxxx+nOnXqiKV379509OhRvX3Gjh0rjqO79O/fn5TgpcdCxetPJ29Q9t0ia58OAACA6thE4HTs2DFasWIFtWrVqsz99u3bRyNHjqS9e/dSdHQ0hYSEUN++fenGjRt6+3GglJKSol2+++47UoLOYT7ULMCT7hUV0/pjSdY+HQAAANWxeuCUl5dHo0aNEq1J3IpUlrVr19Ibb7xBbdq0oWbNmtEXX3xBJSUltGfPHr39XF1dKSAgQLuUd1x7wa1nL//V6vR19DV6UFxi7VMCAABQFaunI5g4cSINHDhQdLvNmzfPrO+9e/cuFRUVkY+PT6mWKX9/fxEw9ezZUxzX19fX5HEKCwvFIsnJyRGvHJTxYkl8PI1GU+HjDmoVQAu3/Uk3su7R9vgUeqplINmyypbXnqCsyoW6VS7UrXKVmHH/MeceZdXAaf369XTy5EnRVVcRb7/9NgUFBYmgS7ebbtiwYRQaGkqXL1+mGTNm0IABA0TXnpOT8RxICxYsoKioqFLrMzIyqKDAsg/T5crJzs4WlenoWLEGvyGRvrTqaCqt3J9AHerZdl4nS5TXXqCsyoW6VS7UrXKVmHH/yc3Ntf3A6fr16zRp0iTatWsXubm5VWgwOQde3Lqk+/0jRozQft2yZUsxbqpx48Ziv169ehk91vTp02nKlCl6LU48fsrPz4+8vLzI0hXJXW587IoGEuN7etE3x9Po9M08Si9ypchgb7JVliivvUBZlQt1q1yoW+UqMeP+Y04cYrXA6cSJE5Senk7t2rXTrisuLqYDBw7Q0qVLRdeZqRaiDz/8UAROu3fvLndAeVhYGNWtW5cSEhJMBk48JooXQ3yhq+JmzxVZmWMH1q5JA1sF0pbYm7Q6+hr93/NtyJZVtrz2BGVVLtStcqFulctB5v3HnPuT1e5kHMTExcVRbGysdunQoYMYKM5fmwqaFi1aRHPnzqXt27eL/cuTnJxMmZmZFBho22OBKpqa4JfTKZSR+7/xWQAAAFB1rBY4eXp6UmRkpN7i4eEhBnHz12z06NGiG03y/vvv08yZM+mrr76iRo0aUWpqqlh4Zh7j16lTp1JMTAxdvXpVzLYbMmQIhYeHU79+/UhJ2oTUprYNatP94hJae+SatU8HAABAFWy67yQpKUnkYZIsW7aM7t+/T88995xoQZIW7rpj3Ep15swZGjx4MD3yyCM0btw4at++PR08eNBoV5xSWp2+jUmiwgfF1j4dAAAAxbN6OgJdPIC7rPfcilQWd3d32rFjB6nFgMgACvByo9ScAvr1TAoNa1ff2qcEAACgaDbd4gRlc3ZypP/XpaH2+XU85RIAAACqDgInOzeyYwNyreFIcTey6cS1O9Y+HQAAAEVD4GTnfDxcaGibYG2rEwAAAFQdBE4K8FK3RuJ1W3wK/XL6Jm2JvUHRlzOpuARddwAAAIodHA4V0yzAi5rWq0UX0vLoze9OadcHervR7EER1D9SWTmsAAAArAUtTgrAD/vloMlQanYBTfj2pNgOAAAAlYfAyc5xd1zU1nNGt0kddbwd3XYAAACVh8DJzh1NvE0p2QUmt3PwxNt5PwAAAKgcBE52Lj23wKL7AQAAgGkInOycv6ebRfcDAAAA0xA42bmOoT5i9pyDie28nrfzfgAAAFA5CJzsnJOjg0g5wEwFT7yd9wMAAIDKQeCkAJynadmL7SjAW787zsmB6NMX2iGPEwAAgIUgAaaCgqc+EQFi9tyNrLs0e8tZyr9fTO6uTtY+NQAAAMVAi5OCcHdcl8a+9Fz7EBrRsYFYtzbmmrVPCwAAQDEQOCnUC50eBk6/n0+nG1n3rH06AAAAioDASaEa+9Wiro19iZ/zu/5okrVPBwAAQBEQOCnYqE4Nxev6Y9epqLjE2qcDAABg9xA4KVjfFvXIz9OVMnILade5NGufDgAAgN1D4KRgzk6ONOLREPH1txgkDgAAUGkInBSOZ9dx7svDlzPpckaetU8HAADAriFwUrjg2u7Us5m/+HrdEQwSBwAAqAwETioaJL7xRDIVFBVb+3QAAADsFgInFXjiET+qX8edsu8V0dbTN619OgAAAHYLgZNKMopLCTHXorsOAACgwhA4qcTzHULI2cmBYq9nUfyNbGufDgAAgF1C4KQSdWu5igcBM7Q6AQAA2HngtHDhQnJwcKDJkyeXud+GDRuoWbNm5ObmRi1btqTffvtNb7tGo6FZs2ZRYGAgubu7U+/evenSpUtVfPb24cW/uuu2xN6g3IIia58OAACA3bGJwOnYsWO0YsUKatWqVZn7HT58mEaOHEnjxo2jU6dO0dChQ8USHx+v3WfRokW0ZMkSWr58OR05coQ8PDyoX79+VFBQQGrXMdSHmvjXorv3i2nTqRvWPh0AAAC7Y/XAKS8vj0aNGkUrV66kOnXqlLnv4sWLqX///jR16lRq3rw5zZ07l9q1a0dLly7VtjZ9/PHH9O6779KQIUNEIPb111/TzZs3afPmzaR23KI3ShokHpMkrhcAAADIV4OsbOLEiTRw4EDRpTZv3rwy942OjqYpU6borePWJCkoSkxMpNTUVHEsibe3N3Xq1El874gRI4wet7CwUCySnJwc8VpSUiIWS+LjccBi6ePKNbRNEL2//QJdSMulo4mZ9Ggjnyr9edYub3VCWZULdatcqFvlKjHj/mPOPcqqgdP69evp5MmToqtODg6K6tWrp7eO3/N6abu0ztQ+xixYsICioqJKrc/IyLB4Fx9XTnZ2tqhMR0frNPj1fqQ2bT2bSV/uv0QNa4ZW6c+yhfJWF5RVuVC3yoW6Va4SM+4/ubm5th84Xb9+nSZNmkS7du0SA72tafr06XotWdziFBISQn5+fuTl5WXxiuQuMz62tQKJV3q40tazh2nvpdsU26EhFTwoIX9PV9H6xDmflFbe6oKyKhfqVrlQt8pVYsb9x5w4xGqB04kTJyg9PV2MUZIUFxfTgQMHxJgl7jpzcnLS+56AgABKS0vTW8fveb20XVrHs+p092nTpo3Jc3F1dRWLIb7QVXGz54qsqmPL0TqkDjX0rUnXMu/S62tPadcHervR7EER2rQFSilvdUJZlQt1q1yoW+VykHn/Mef+ZLU7Wa9evSguLo5iY2O1S4cOHcRAcf7aMGhiXbp0oT179uit4xYrXs9CQ0NF8KS7D7ce8ew6aR8g2h6fIoImQ6nZBTTh25NiOwAAANhQi5OnpydFRkbqrePUAb6+vtr1o0ePpuDgYDEGiXHXXvfu3emjjz4SA8p5jNTx48fp888/F9ulPFA8yLxJkyYikJo5cyYFBQWJtAVAVFyioait54xeCp5jxx11vL1PRIDFu+0AAADsndVn1ZUlKSlJr/msa9eutG7dOpFuYMaMGSI44hl1ugHYtGnTKD8/n8aPH09ZWVnUrVs32r59u9XHUdmKo4m3KSXb9IB3Dp54O+/XpbFvtZ4bAACArbOpwGnfvn1lvmfDhw8Xiync6jRnzhyxQGnpuQUW3Q8AAEBNlD9aF/T4e7pZdD8AAAA1QeCkwseu8Ow5U6OXeD1v5/0AAABAHwInleEB35xygJkKnng7BoYDAACUhsBJhThP07IX21GAt353XE0XJ7He0nmcAAAAlMKmBodD9eHgiFMO8Oy5w5dv0Se/J4gWqCce8UM1AAAAmIAWJxXj7jhOOTClzyMik3j+/WLaFmf6mX4AAABqh8AJRAqH4e3riyvxw/HruCIAAAAmIHACYVi7+uTgQHQk8TZdy8zHVQEAADACgRMIQbXd6fEmD8c3bTyRjKsCAABgBAIn0Hq+Q31t4MTPtAMAAAB9CJxAq09EPapd01k8q+6PhFu4MgAAAAYQOIGWaw0nGtomWHyNQeIAAAClIXACPcP/6q7bdTaN7uTfx9UBAADQgcAJ9LQI8qYWQV50v7iEtsTewNUBAADQgcAJSnm+Q4h4/eE4ZtcBAADoQuAEpQxpE0QuTo50LiWH4m9k4woBAAD8BYETlFK7pgv1bVFPfL0BmcQBAAC0EDhBmd11m2NvUkFRMa4SAAAAAicw5bHwuhTk7UbZ94po17k0XCgAAAAETmCKk6MDPfvXg3834BEsAAAAArrqwKTn/gqcDl7KoJtZ93ClAABA9RA4gUkNfT2oc5gPaTREP6LVCQAAAIETyBskzt11JXjwLwAAqBxanKBMAyIDqZZrDUq6fZeOJN7G1QIAAFVD4ARlcndxokGtg8TXyOkEAABqh8AJyvX8Xw/+/TXuJu35M008wy76ciYVo+sOAABUxqqB07Jly6hVq1bk5eUlli5dutC2bdtM7t+jRw9ycHAotQwcOFC7z9ixY0tt79+/fzWVSJnahNSmQC83KnygoXFrjtOk9bE0cmUMdXv/d9oen2Lt0wMAAKg2NciK6tevTwsXLqQmTZqQRqOhNWvW0JAhQ+jUqVPUokWLUvv/9NNPdP/+fe37zMxMat26NQ0fPlxvPw6UVq1apX3v6upaxSVRth1nUyklp6DU+tTsAprw7Ula9mI76h8ZaJVzAwAAUE3gNGjQIL338+fPF61QMTExRgMnHx8fvffr16+nmjVrlgqcOFAKCAioorNWF+6Oi9p6zug2DRE5EIntfSICRNJMAAAAJbOZMU7FxcUiEMrPzxdddnJ8+eWXNGLECPLw8NBbv2/fPvL396emTZvShAkTRMsUVMzRxNuUkl26tUk3eOLtvB8AAIDSWbXFicXFxYlAqaCggGrVqkWbNm2iiIiIcr/v6NGjFB8fL4Inw266YcOGUWhoKF2+fJlmzJhBAwYMoOjoaHJycjJ6rMLCQrFIcnJyxGtJSYlYLImPx92Slj5uVUnLuSd7P2NlsrfyVgbKqlyoW+VC3SpXiRn3H3PuUVYPnLhVKDY2lrKzs2njxo00ZswY2r9/f7nBEwdMLVu2pI4dO+qt5xYoCW/nweeNGzcWrVC9evUyeqwFCxZQVFRUqfUZGRkioLMkrhwuK1emo6PNNPiZ5Pzgnuz90tPT7b68lYGyKhfqVrlQt8pVYsb9Jzc3134CJxcXFwoPDxdft2/fno4dO0aLFy+mFStWmPwe7s7jbr05c+aUe/ywsDCqW7cuJSQkmAycpk+fTlOmTNFrcQoJCSE/Pz8x28/SFckz/fjY9hBI9K3rRwG7kigtp0B0yxniUU0B3m7Ut22Y0TFO9lbeykBZlQt1q1yoW+UqMeP+4+bmZj+Bk7GC6nabGbNhwwaxz4svvlju8ZKTk8UYp8BA07O+eDC5sZl3fKGr4mbPFVlVx7Y0PsX3BkeI2XMcFhkLnmYPiiDnGsa7Qe2tvJWFsioX6la5ULfK5SDz/mPO/cmqdzJu6Tlw4ABdvXpVjHXi99ylNmrUKLF99OjRYp2xbrqhQ4eSr6+v3vq8vDyaOnWqmJXHx9yzZ49Ib8AtWv369au2cikNpxrglAPcsmRo7GMNkYoAAABUw6otTjwmhoOjlJQU8vb2FuORduzYQX369BHbk5KSSkWBFy5coD/++IN27txZ6ng8+PvMmTMiH1RWVhYFBQVR3759ae7cucjlZIHgiVMO8Oy59NwCOnw5k74/dp22x6fRtH7NxaNZAAAAlM6qgZPhjDhD3PpkbDA5D/Qyxt3dXQReUDV4DFOXxg9b+fq1CKA/Lt2iG1n36PMDV2hS7ya47AAAoHjKH3QCVcLN2Yn+PaCZ+Hr5/ssiizgAAIDSIXCCCnu6VSC1b1iH7hUV0wc7LuBKAgCA4iFwgkrNVpj59MN8Wz+eTKa45GxcTQAAUDQETlApbUJq09A2QeLrub+cMzn+DAAAQAkQOEGlTevfjNycHeno1du0PT4VVxQAABQLgRNUWlBtdxr/eJj4esG281T4oBhXFQAAFAmBE1jEa90bk7+nKyXdvkurD13FVQUAAEWqUOB0/fp18SgTydGjR2ny5Mn0+eefW/LcwI54uNagqf2aiq+X/p5At/LKfmwOAACAagKnF154gfbu3Su+Tk1NFZm+OXh65513ZD14F5Tp2Xb1KTLYi3ILH9B/d1209ukAAADYRuAUHx9PHTt2FF//8MMPFBkZSYcPH6a1a9fS6tWrLX2OYCccHR1o5sCH6Qm+O5pE527mUMyVTNp5/rZ4LS7BjDsAAFDhI1eKioq0z37bvXs3DR48WHzdrFkz8dw5UK9OYb7Uv0UAbT+bSkM/O0T3H5T8tSWRAr3daPagCDwUGAAA1NXi1KJFC1q+fDkdPHiQdu3aRf379xfrb968Sb6+D59lBur1WPjDz8D/gqaH+LEsE749SdvjEVwDAICKAqf333+fVqxYQT169KCRI0dS69atxfqff/5Z24UH6sTdcZ/tu2x0m9RRF7X1HLrtAABAPV11HDDdunWLcnJyqE6dOtr148ePp5o1a1ry/MDOHE28TSllPPCXgyfezvt1aYzWSQAAUEGL071796iwsFAbNF27do0+/vhjunDhAvn7+1v6HMGOpOcWWHQ/AAAAuw+chgwZQl9//bX4Oisrizp16kQfffQRDR06lJYtW2bpcwQ74u/pZtH9AAAA7D5wOnnyJD3++OPi640bN1K9evVEqxMHU0uWLLH0OYId6RjqI2bPOZjYzut5O+8HAACgisDp7t275OnpKb7euXMnDRs2jBwdHalz584igAL1cnJ0ECkHmKngibfzfgAAAKoInMLDw2nz5s3i0Ss7duygvn37ivXp6enk5eVl6XMEO9M/MpCWvdiOArxLd8c91TIQeZwAAEBds+pmzZolHrvy1ltvUc+ePalLly7a1qe2bdta+hzBToOnPhEBdOTKLUpIzqBbhY60ZO9l2nUujRJv5VNoXQ9rnyIAAED1BE7PPfccdevWTWQJl3I4sV69etEzzzxTkUOCAnF3XOcwXwqrVUx+fn4UeyOHDlzMoJmb4+mbcR3JwQHddQAAoIKuOhYQECBalzhbeHJysljHyS/5sSsAhjhImjukBbnUcKQ/Em7Rz6dv4iIBAIA6AqeSkhKaM2cOeXt7U8OGDcVSu3Ztmjt3rtgGYExDXw9688lw8fW8X/+k7HtFuFAAAKD8wOmdd96hpUuX0sKFC+nUqVNi+c9//kOffPIJzZw50/JnCYrxWvcwCqvrQRm5hfTRzgvWPh0AAICqD5zWrFlDX3zxBU2YMIFatWolljfeeINWrlxJq1evrsghQSVcazjRvKGR4utvYq7RmeQsa58SAABA1QZOt2/fNjqWidfxNoCydA2vS0PbBJFGQzRjUxwe+AsAAMoOnHgmHXfVGeJ13PoEUJ53BkaQp1sNir+RQ99EX8UFAwAA5QZOixYtoq+++ooiIiJo3LhxYuGvuZvuww8/lH0cfq4dB1qcNJMXzge1bds2k/vz8Xl2lu7i5qafZFGj0Yg8U4GBgeTu7k69e/emS5cuVaSYUIX8PF3p7f4PWy0/3HmR0nLw0F8AAFBo4NS9e3e6ePGiyNnED/nlhR+7cvbsWfrmm29kH6d+/fpigPmJEyfo+PHjIpkmP0CYj2MKB1icP0paDB/xwkEdPy9v+fLldOTIEfLw8KB+/fpRQQFuzLbmhY4NqHVIbcorfEBztp6l6MuZtCX2hngtLtFY+/QAAAAskwCTBQUF0fz58/XWnT59mr788kv6/PPPZR1j0KBBeu/5eNwKFRMTQy1atDD6PdzKxDmkjOHWpo8//pjeffddEYAxfvAwP4SYHxEzYsQImaWD6uDo6EDzh0bSoE/+oF/jUsUi4QcB8zPtOAM5AACA3SfAtLTi4mJav3495efnax/hYkxeXp7IGxUSElKqdSoxMZFSU1NF95yEc0116tSJoqOjq7wMYL7kO3fJWNtSanYBTfj2JG2PT8FlBQAA+29xspS4uDgRKHFXWq1atWjTpk1ivJQxTZs2FWOreFxUdna2GE/VtWtXETxxtx8HTYxbmHTxe2mbMYWFhWKR5OTkiFdO5mnphJ58PG4ZU0ui0LLKy91x7/18zuj3cTDFD2SJ2nqOejXzF49vsXVqqls1lVVt5VVTWdVWXjWV1dzymnNNrB44cTAUGxsrAqGNGzfSmDFjaP/+/UaDJw6wdFujOGhq3rw5rVixQmQtr6gFCxZQVFRUqfUZGRkWHxvFlcNl5cp0dLSZBr8qU1Z5T1zPpdQyBoVz8JSSXUA7T12h9iGeZOvUVLdqKqvayqumsqqtvGoqq7nlzc3NpSoJnHgAeFl4kLi5XFxcKDz84WM42rdvT8eOHaPFixeLYKg8zs7O4nl5CQkJ4r009iktLU3MqpPw+zZt2pg8zvTp02nKlCl6LU7cFcgPpuXB6JauSB6nxcdWywfXVHmLUh7IOkZRDXfy9/cnW6emulVTWdVWXjWVVW3lVVNZzS2v4Qx9iwVOPF6ovO2jR4+myhZUt9usvHFR3NX31FNPifehoaEieNqzZ482UOIgiGfXcZZzU1xdXcViiC90VXy4uCKr6ti2yFR563m5y/p+3s9erpWa6lZNZVVbedVUVrWVV01lNae85lwPswKnVatWkSVxS8+AAQOoQYMGopls3bp1tG/fPtqxY4fYzkFYcHCw6Epj/GDhzp07ixYqbt364IMPRDqCV155RXuBJk+eTPPmzaMmTZqIQIqfncczAIcOHWrRc4fK6xjqI2bP8UBwU8kHajg6iJxPAAAAtsCqY5zS09NFcMT5mLi1igd9c9DUp08fsT0pKUkvCrxz5w69+uqrYqB3nTp1RNfe4cOH9cZDTZs2TczMGz9+vAiuunXrRtu3bzerGQ6qBw/45pQDPHuOh34bC54elGho6KeH6IPnWtGAlkhNAAAA1uWg4VFToIe79ziQ40FlVTHGiQNGHrOjhqZSOeXllAM8e44Hgku4JWpSryb008kbdPTqw+cfvvRYI5o+oDm51HAUM/KOJt6m9NwC8vd0E61X1p55p6a6VVNZ1VZeNZVVbeVVU1nNLa85932rz6oD4CSXfSICjAZCz7WvTx/svEAr9l+hVYeuUuz1LLFu6e8JpQItJMwEAICqhsAJbAIHSV0a+5ZaX8PJUbQydWjoQ//8IZZOJWWJxVTCzGUvtkO2cQAAqDLKb6sDRegTUY9+frMbOZvojpP6m7nLz9hz7ngdnoUHAACVhRYnsBvcNVdUxsN/pYSZ3OWn23plagwVuvYAAMBcCJzAbvD4Jzk+2HGe+rYIoBZBXqILb9rGM6Vm7KFrDwAAKgKBE9gNHjQux8mkLLGURfdZeDww3doz8gAAwD5gjBPYXcLMskIcHw8X+mffR2hgy0AK8Co70NLt2qvsmCjeHnMlk3aevy1ey9sfAADsE1qcQBEJM6Vg6j/PRGpn1XHQM2l9bLnH/TbmKoX4uFP9OjUrNCaq9P6JGEMFAKBQaHECu8KBC6ccCPDWb03i94apCOR27f0al0rd3t9LIz6Ppplb4kVgphs06Y6J4iBJF783Z38AALBvaHECRSXMNOdZeLy3t7szNQ/0pJhE7mJ7uBgjff/MLWcpwMudikpKKL/gAU3/Kc7osTGGCgBAmRA4gaISZprbtbfw2ZYiELuRdY+W7LlE3x+7XuYxM3ILaehnh2Sdo6n0CAAAYL/QVQeKJrdrL7i2O3WVGdzUdnemsLoeFFTb3aJpFAAAwPahxQkUT27XntwxUctebC9akHi23ciVMeXuX1SMGXYAAEqBwAlUQU7XnpwxUdxSxfvJ2V8y/cczdOPOPZrQozG51HjYyMvpCsoL5AAAwPYgcAIwY0wUb5cCnPL25/eRQV4UfzOH/rv7Iv1y5qYYU8XjpMx9BAwCLQAA24DACcDImCjDwCbARGBT3v79WgTQ1jMpNGfrWbqUnkfPLos2er3LegQMnrUHAGA7EDgBVHBMlOH+R67cooTkDAqv70edwupq9x/cOoieaFKX5v1yjjaevGH0GKbSF0h5oqr6WXto0QIAkAeBE0AFx0QZ7t85zJfCahWTv78vORoEWbVrutCz7UNMBk666Qsmf3+KmgV4kYerE32861KV54lCixYAgHwInACqidy0BFtPp4ilMnmi5LYgVVeLFgCAUiBwAqgmctMdPBUZQLXcatD51Fw6k5xd7v6/xaVQi2Av8nJzNqsFiYOr934+h8znAABmQOAEUE3kpjv45IV2onVIbp6ob2KuiYznj4X7igcVfxtzzWQL0r/6PUJe7i507mYOHU3MpNQc061gyHwOAFAaAicAG013ICdPVC1XJ/LzdKXEW3dp74UMkz9b+v4Pdlw0+7yR+RwA4H/wyBUAG3wEjG6gxQxHJzn8tXw4vDXt/deTtOutJ2h4+/qyzqF1iDe90aMx/aNXuEW7GAEA1AAtTgA2nO5Abl6pJvU8qVuTurThRHK5P//lx0JpSJtgMcZpw/HkMlu0dDOlAwAAAicAm093YOln7Un7ldV1KKnn6UoajZT8AAAA0FUHYEeBFrcU8aux1ilpTJSpEIfXBxq0IJnqOvT1cKEajg50Ojmb3v4xjkpK8KBiAACGrjoAlQ4+L69Fa8+faTRh7Un68WQyebrVEN/r4ICWJwBQN6u2OC1btoxatWpFXl5eYunSpQtt27bN5P4rV66kxx9/nOrUqSOW3r1709GjR/X2GTt2rPjPXXfp379/NZQGwL4Gn5fXotW3RQB9OLyV2L768FX6ePelaikDAIAts2qLU/369WnhwoXUpEkTMY5izZo1NGTIEDp16hS1aNGi1P779u2jkSNHUteuXcnNzY3ef/996tu3L509e5aCg4O1+3GgtGrVKu17V1fXaisTgL09a68sz7StTzn3HtDsn8/S4j2XyNvdmcZ0bURHrmRSQvJtCs9z0nsunzF4Dh4AKIlVA6dBgwbpvZ8/f75ohYqJiTEaOK1du1bv/RdffEE//vgj7dmzh0aPHq0XKAUEBFThmQMo61l7ZeFAKfteEf3fros055dzIoDi9w8lGs1KLsFz8ABAaWxmjFNxcTFt2LCB8vPzRZedHHfv3qWioiLy8fEp1TLl7+8vuvN69uxJ8+bNI19f0zeRwsJCsUhycnLEa0lJiVgsiY/HrWuWPq6tUlN5lVzWiT3CKPb6Hfr9fIZO0KSflfzTF9pS/8j//cGyPT6VJq47ZTKLueH+tkzJdavmsqqtvGoqq7nlNeeaWD1wiouLE4FSQUEB1apVizZt2kQREQ+T/pXn7bffpqCgIDHWSbebbtiwYRQaGkqXL1+mGTNm0IABAyg6OpqcnJyMHmfBggUUFRVVan1GRoY4L0viysnOzhaV6eio/EmNaiqvksvK3W3xyVlGt0mB0Xs/x1Oz2hq6X6yh3IIH9O7mCyafg8eifo6n1nUdKtSFWN2UXLdqLqvayqumsppb3tzcXJLLQfMwSYvV3L9/n5KSkkThNm7cKLrf9u/fX27wxGOjFi1aJFqXeIC5KVeuXKHGjRvT7t27qVevXrJbnEJCQujOnTti0LqlK5IDMj8/P9V8cNVSXiWXNeZKJr3whf5EDEtY90pH6hxmmS7FqqTkulVzWdVWXjWV1dzy8n2fe6k4Finvvm/1FicXFxcKD3/46If27dvTsWPHaPHixbRixQqT3/Phhx+KwImDobKCJhYWFkZ169alhIQEk4ETj4kyNoCcL3RVfLh4pl9VHdsWqam8Si1rRt59s/Z3dnKgomKNrOPay7VSat2qvaxqK6+aympOec25HlYPnIxFiLqtP4a4lYkHke/YsYM6dOhQ7vGSk5MpMzOTAgONT8MGgPLJzUr+1ZgO1L2pv5jRN3JlTLn7f3HwCjX2q0WRwd7adZiFBwC2zKqB0/Tp08X4owYNGoj+xXXr1omuNw6KGM+U4zQDPAaJcfqBWbNmif0aNWpEqampYj2PjeIlLy9PjFV69tlnxaw6HuM0bdo00aLVr18/axYVwK5JWclNPdfO4a9cURw08Zil8vaXxN3Ioac/+YOebhVI/+rblM6n5pR6Ll9Zs/YAAKqbVdvq0tPTRXDUtGlT0Y3G3XQcNPXp00ds57FPKSkp2v05VQGPiXruuedEC5K0cNcd48HfZ86cocGDB9MjjzxC48aNE91/Bw8eRC4nAAtkJWeGQ7mNZSUvb39e5g6JpKFtgoiTkf9yJoV6frSPXv/2pF7QpDsLj1MbAABYm9UHh9siHiTm7e0ta5BYRboiOWDkdAlq6GNWU3nVUFZz8zLJ2f/szWx6f9t5OnDplsmfK7Vo/fF2T6vMwlND3aqxrGorr5rKam55zbnv29wYJwCw/azkR67cooTkDAqv71dm5nA5WcxbBHnThB7hZQZO/NcdB198HEsl9gQAqAgETgBgFg56OIVAWK1i8vf3JcdyWoDkZDHnoEoOufsBAFQV5bfVAYBiZu3J3Q8AoKogcAIAq5Nm4ZU3eunPlIdZgAEArAWBEwBYXVmz8HTN+eVPGrfmOGXmmc71BgBQlRA4AYBN4IHky15sJ2bP6eKWqGWj2tF7gyLIpYYj/X4+nfovPkgHLmZoE2ZGX86kLbE3xCu/BwCoKhgcDgA2o7xZeJ3CfOkf352iS+l5NPqro9Qnoh7FJWdTag4SZgJA9UCLEwDYFGkW3pA2weJVN3VB80Av2vr3bvRi5wbi/a5zaXpBE0PCTACoSgicAMCuuDk7UdTgSKpT09nodqmjjhNvotsOACwNgRMA2B3uyrtzt0hWwkwAAEtC4AQAdgcJMwHAWhA4AYDdkZsIs24t1yo/FwBQFwROAKDYhJmLd1+klOx71XRWAKAGCJwAQFEJM6X3nPPp6NU7NGDxQdp5NlW7HXmfAKAykMcJAOw6YSbPnuOB4BJOoMlBVdMAL5HzKe5GNo3/5gT9v84N6dFGdWjBtvN6+wf+tT8fDwCgPAicAECxCTN/nNCVPtx5gT4/cIW+ibkmFkNS3icOwhA8AUB5EDgBgCISZhrD3XUznmpOXcJ8adyaY2TsaSy8isMsbrniIEw34SYAgCGMcQIAVSTNLOsRdmXlfeIxUTFXMmnn+dviFUk1AdQNLU4AoHhy8z5xYMTjoGo4Pfybcnt8isEYqkSMiQJQOQROAKB4cvM+Ld5ziVYfvkrdH/Ej31outPrQVe0jXOSOieIWKVNjrgDA/iFwAgDV5H3ioMdUj52bsyO5ODlS9r0i+vn0TZPHKmtMVOkWKszaA1AajHECAFJ73idePv5bGzo5sw9tfL0LDWkdVObxpDFR648mUeGDYm3QxC1RukGTbgsVbwcA+4fACQBUlfeJ8zzp4vdStxuPberQyId6NveXdcx3NsdT5Owd9PSSgzTlh9NGW7OkddwShYHlAPYPXXUAoBrl5X0yd0yUh6sT5RcWU/zNnDL30521Zyp1AgDYBwROAKAqZeV9kjsmyuGvlqqD054UAdFXfyTSqsNXKzS7D4PJAewLAicAABNjonhsEgdJusGT1DbF27lrL8SnJvVtESArcPpsbwIVFWvoqZYBVNOlBgaTA9ghjHECAKjgmCjDFqrykg5cSMujf204TR3n76EXvzhCr2MwOYDdsWrgtGzZMmrVqhV5eXmJpUuXLrRt27Yyv2fDhg3UrFkzcnNzo5YtW9Jvv/2mt12j0dCsWbMoMDCQ3N3dqXfv3nTp0qUqLgkAKBEHR3+83ZPWvdKR5vQPFa/83jB/k5xZewuGtaSp/ZpSQ9+alFf4gP5IuGX0Z2IwOYBts2rgVL9+fVq4cCGdOHGCjh8/Tj179qQhQ4bQ2bNnje5/+PBhGjlyJI0bN45OnTpFQ4cOFUt8fLx2n0WLFtGSJUto+fLldOTIEfLw8KB+/fpRQYG8zMEAAIZBUecwX+rbzEe8mkpmWV4L1ciODWjik+G095896N2Bzcu8yOU9Aib6ciZtib0hXsuaqWfOvgBgB2OcBg0apPd+/vz5ohUqJiaGWrRoUWr/xYsXU//+/Wnq1Kni/dy5c2nXrl20dOlSEShxa9PHH39M7777rgjA2Ndff0316tWjzZs304gRI6qpZACgRnJm7Tk6OpCfp6us4313NImCartRQ18PsxNsIhkngMIHhxcXF4tuuPz8fNFlZ0x0dDRNmTJFbx23JnFQxBITEyk1NVV0z0m8vb2pU6dO4ntNBU6FhYVikeTkPJxaXFJSIhZL4uNxgGfp49oqNZUXZVUuc+qWQ6ROoXV01vD36bf0+NVykfVzOYM5L80CPCncz4N+iUsttY+UYPPTF9pS/8gAsW57fCpNXHfK5ONidPetTFmVQE3lVVNZzS2vOdfE6oFTXFycCJS4K61WrVq0adMmioh4OFbAEAdF3Hqki9/zemm7tM7UPsYsWLCAoqKiSq3PyMiweBcfV052draoTEdH5Y/NV1N5UVblsnTdNqypIf9azpSeV2RyH09XJ3rEz51ib+TR+dRcsRgjBUfvbo4jb8cCqunsRLO2XCg7GefP8dS6roPRbkc1fY7VVl41ldXc8ubmGv/9ssnAqWnTphQbGysKt3HjRhozZgzt37/fZPBUFaZPn67XksUtTiEhIeTn5ycGrVu6Ih0cHMSx1fLBVUt5UVblqoq6fW+wRrQKkYl0B+8/20q0Ct25e59WHLhCnx9ILPN4t+8+oFHf/inrZ6flFdG1uzXEmC01f47VVl41ldXc8vKEM7sJnFxcXCg8PFx83b59ezp27JgYy7RixYpS+wYEBFBaWpreOn7P66Xt0jqeVae7T5s2bUyeg6urq1gM8YWuig8XV2RVHdsWqam8KKtyWbpun2oVRMscHUqNWQowGLPkW8uNWgR5yzqmu7OTeHaenDHgGXn3TZZFTZ9jtZVXTWU1p7zmXA+rB07GIkTd8Ua6uEtvz549NHnyZO06HhwujYkKDQ0VwRPvIwVK3HrEs+smTJhQTSUAALDOI2C+GvuoeB25MqbcfTefukGt6tem0LoPB54znnV35EomJSTfpvA8J+oUVtfkLEIAtbJq4MRdZAMGDKAGDRqI/sV169bRvn37aMeOHWL76NGjKTg4WIxBYpMmTaLu3bvTRx99RAMHDqT169eLNAaff/65NrLkoGrevHnUpEkTEUjNnDmTgoKCRNoCAAAlPwKG92Nl7SvZeyGD9l3cR09FBtKEHo0p+c5dg9avRJMz9iR4XAyokVUDp/T0dBEcpaSkiNlvnAyTg6Y+ffqI7UlJSXrNZ127dhXBFacbmDFjhgiOeEZdZGSkdp9p06aJmXnjx4+nrKws6tatG23fvt2s/ksAAHt8BIzUOlTevv/s+widTMqi38+n069xKWIxRpqFZ5gpnSHdAaiVg4aHm4Me7t7jQI4HrFfF4HAOGP39/VXRx6ym8qKsymUrdWvpPE5/puTQsn0J9PNp44GTbmsWZ0yXAjM+NgdUhjcPKTAzFmjZKlup2+qgprKaW15z7vs2N8YJAAAqNyZK7r7NA71oZMeGZQZOUhbzl1cfpfYNfSi4thv957fzJtMd8NE5YOOfjfFRoEQInAAAFDYmypx9OaiSY//FW2Ipj+7jYuSeJ4A9QeAEAKBicmfsPd++vmhOOpWURZfS88rdX25ABmBvEDgBAKiY3Bl7C55tJVqw+GHBctIdyA3IAOyN8keHAQBAuTP2mOFIKWMz9qRAq6zsTq41HKlpgCeuOigSAicAAJXjgeQ8E45blnTxe8MZcmUFWpLCByU07LNDdD714QPTAZQEXXUAAKCdhXfkyi1KSM6g8Pp+JjOHS4GWsXQHY7s2oq+jr9HVzLs09NNDtHBYKxraNrhCVxgJNsEWIXACAACBgyR+8G9YrWLy9/clxzIet1JWuoPnO4TQpO9j6cDFDJr8fSydSrpD7wx82N0nJ5UCQ4JNsFUInAAAoEJMpTuo4+FCq8Y+Sot3X6QlvyfQmuhrdOBSBuUXFlN6bqGs5J3GEmyWlckcoLpgjBMAAFRJUDWlb1P6ckwHcnd2pMRbd/WCJt1AiAMlyYPiEpr981mTCTYZdxFyN54hXsez/rbE3hCvxvYBqCy0OAEAQJXp0dSfark6070i/aCJSWHN3787RfXrnKecew/ozt37VFa8YyrBJrr2oLqgxQkAAKoMBzgZeaWDJl1FxRrRIpWZX3bQpGvO1rP0dfRVSr5zV9u1pztQ3VSLFkBlocUJAACqjNwM4v/oGU4DWwVRQnoeTVx3stz9/0zNpVlbzoqlhqNDhZ6dx115R65kUkLybQrPczI5ixBAFwInAACoMnIziHdpXFckzQz3r1VuJvO6tVzppW6NaN/5DDp29TY9KKOZSn7XXqLJweoAutBVBwAAVaa8TOO8nrfzfnIzmc8d2oLe6BFOP7zeheY/EynrPOb/eo7+u+si7TqXRuuOXEPXHlQYAicAALCZR7qYm8k8tG4tWecRfzOHFu+5RK9+fZxmbIqv0Kw9AIauOgAAqFKmMo0HlNE1VlaCTXMfUuzj4UJvPNmYzt3MpaOJmXT9zj2zu/YAJAicAACgyskNhOQk2DTch4Mvnj3HR9INnqQjc3eeFJxxjqdJ62MtNqgd1AeBEwAAVAs5gVBVt2jJHawudz9QHwROAABg9yzVtSf55cxNatugNrk5O1X5uYN9QeAEAACKYImuPen92iNJItXBkpFtqVmAl1jHA8bN6Wo0d3+wDwicAABAVcrr2nN3qUH//OE0XUzLo8FLD9E7TzUnf09XmvOL/v5l5X0y9xEwCLLsBwInAABQbdfekSu3KCE5g8Lr++llDt8++XGauuE07b2QIR46bIz0SBfDFAnSI2A0ZuxvTpAF1oU8TgAAoEocJHUO86W+zXzEq243Gmcn/2rsozTz6eYmv18371PRgxIqfFBMOfeKRKAlN08UnrNnf9DiBAAAYISDgwNFBHqXeW2kvE9N3t0m6xpK+8/YFEcdG9Wh+b+dr/Bz9jB+yjoQOAEAAFRzPqfvj10XS1nkP2cPXXuq6apbsGABPfroo+Tp6Un+/v40dOhQunDhQpnf06NHD/FXgOEycOBA7T5jx44ttb1///7VUCIAAFASufmcVrzYjk7P7ktrXnpU1v6PN6lLTfzlPS7mgx3n6evoq3T2Zjb9dubh+CndoEl3/BQHVaDgFqf9+/fTxIkTRfD04MEDmjFjBvXt25fOnTtHHh4eRr/np59+ovv372vfZ2ZmUuvWrWn48OF6+3GgtGrVKu17V1fXKiwJAAAokZxHuvBsvN5/dad1a+Ina//VL3UULUkjV8aUew4nk7LEIn1/Rbr2QCGB0/bt2/Xer169WrQ8nThxgp544gmj3+Pj8/AJ2pL169dTzZo1SwVOHCgFBARUwVkDAIBayHmki+5Dis3ZX05QVsfDhf5f54Z0MumOyCtVUFRi8lzxnD0VzqrLzs42GhyV5csvv6QRI0aUaqHat2+fCMKaNm1KEyZMEC1TAAAAFc37xC1Fuvi9YWoBc/aXgixm2D4kvf/PM5H0Vp9H6JtxnWjBsFayzhfP2VPJ4PCSkhKaPHkyPfbYYxQZGSnre44ePUrx8fEieDLsphs2bBiFhobS5cuXRRfggAEDKDo6mpycSqfPLywsFIskJydHe068WBIfT6PRWPy4tkpN5UVZlQt1q1xy67ZvRD3q1cxftPqk5xaKhJiPNnqYCdzY98rdn/f79IW2NOeXPyk1Rz8Z58yBzcV2af96ni6yyrT19E1q36A2BdV215uFd+RKJl2+mUmNcxypk0H6BbX/3paYcY9y0PBRbQC3Cm3bto3++OMPql+/vqzvee2110QwdObMmTL3u3LlCjVu3Jh2795NvXr1KrX9vffeo6ioqFLrL168KAauWxJXDreseXt7k6OjTTX4VQk1lRdlVS7UrXLZSt1yYBN7I48y84vI18OZ2gTXMpqC4Jmv4ig9r6jc4zk7OdDgFnVp9KMBdC4tn/6777re9/nXcqa3eoTQk+F1SKlKzKjb3NxceuSRR8T+Xl4PH7Fj04HTm2++SVu2bKEDBw6IViI58vPzKSgoiObMmUOTJk0qd38/Pz+aN2+eCLbktDiFhITQnTt3yr2AFanIjIwMcT5KDyTUVl6UVblQt8plb3W7PT6VJq47Jb42Nn5qUq9wirlym2ISb4v3NRyJHhhpTJH259au/pGVHw9cXKIx2rpmL3XL9/06derICpys2lXHMdvf//532rRpkxiTJDdoYhs2bBDBzosvvljuvsnJyWKMU2Cg8dT1PJDc2Kw7vtBV8YvE6RGq6ti2SE3lRVmVC3WrXPZUt0+1CqJljg4mn7MnjZ+KvpxJH+++QEcS7xg9jjQLb+6vf1K/yMBKBTnbbTivlNy6NafurRo4cSqCdevWidYm7hJLTU0V67lZzd39Yd/s6NGjKTg4WOR80sXjmjjvk6+v/pOw8/LyRLfbs88+K2bV8RinadOmUXh4OPXr168aSwcAAFB1z9krK3P4w4SZTctMd1DWLDy5mcm3m/lcPiWwauC0bNkybVJLXZx/iZNYsqSkpFKRICfJ5LFQO3fuLHVMHvzNY57WrFlDWVlZojuPc0PNnTsXuZwAAEAROIgxDHYqOrvuTHJWhTKTF5doxH5qe2SM1bvqysNdeIY4xYCp7+WWqh07dljk/AAAAJSe9XzBtvP0W3wqDW9fn9xdnOhfP5yW1YK070J6qQzmanhkjM2kIwAAAADLKS/BJnOt4UgPikvo9PUssZgiff/UjWdEuoOzN3PoauZd2Y+MeaZtMD0a6kNX0vNp4jr77tpD4AQAAKBAcrKYLx7Rhto39KEtsTdo9eFESr5TdvdebsED+jXu4XhkuSzxyBhb6tpD4AQAAKBQUhbz8mbhvfJ4GPnVcqVJ38eWe8ynWwXS8x1CqHmgFw1e+keZj4zx4UfGdGlIx68+fGRMobHcCHbWtYfACQAAQAWz8I5cuUUJyRkUXt+POoXVLdVi4+8lb0zUqE4NtYFNeS1a85+J1AY3P51Mpik/nC73+DO3xNHAlkHUOcyXMnILaNL6WJvq2rP9pBUAAABQKRwkcSDSt5mPeDXWzSWNiTLVAcbrA70fdpNV5Dl+gd7/ewRMWRLS82nxnksilcI/jARNTFrHLVHcjVed0OIEAAAAssZEzR4UUSrokpNXSs5gdd67rqcr/b1nOB27eocOXsygrHtFFcpDVZXQ4gQAAABmtyAZyys1pE2weDXWoiUFZsxwq/R+7pAWNLpLI/pkZFuKGtyC5JCbr8pS0OIEAAAAZrcgVeVgdXPGXMnNV2UpCJwAAADA7MzkFWXJrr0AgzFX1QGBEwAAANhcYOZUwTFXVQ1jnAAAAEBRY66qElqcAAAAQJVjrioCgRMAAACodsyVudBVBwAAACATAicAAAAAmRA4AQAAACBwAgAAALAstDgBAAAAyITACQAAAEAmpCMwQqN5mJ80JyeHLK2kpIRyc3PJzc2NHB2VH7eqqbwoq3KhbpULdatcJWbcf6T7vXT/LwsCJyP4QrOQkJCK1RYAAADY5f3f29u7zH0cNHLCK5XhKPXmzZvk6elJDg6WzUzKUS0HZNevXycvLy9SOjWVF2VVLtStcqFulSvHjPsPh0IcNAUFBZXbOoUWJyP4otWvX5+qElei0gMJtZYXZVUu1K1yoW6Vy0vm/ae8liaJsgedAAAAAFgQAicAAAAAmRA4VTNXV1eaPXu2eFUDNZUXZVUu1K1yoW6Vy7WK7j8YHA4AAAAgE1qcAAAAAGRC4AQAAAAgEwInAAAAAJkQOFWzTz/9lBo1aiRSwHfq1ImOHj1KSvPee++JxKG6S7NmzUgpDhw4QIMGDRKJ0rhsmzdvLpVIbdasWRQYGEju7u7Uu3dvunTpEimxrGPHji1V1/379yd7tGDBAnr00UdF4lt/f38aOnQoXbhwQW+fgoICmjhxIvn6+lKtWrXo2WefpbS0NFJqeXv06FGqfl9//XWyN8uWLaNWrVpp8/l06dKFtm3bpsh6lVNepdSrMQsXLhTlmTx5cpXVLwKnavT999/TlClTxCj/kydPUuvWralfv36Unp5OStOiRQtKSUnRLn/88QcpRX5+vqg7DoKNWbRoES1ZsoSWL19OR44cIQ8PD1HP/MurtLIyDpR06/q7774je7R//37xn2tMTAzt2rWLioqKqG/fvuIaSN566y3aunUrbdiwQezPTxgYNmwYKbW87NVXX9WrX/582xtOaMw31BMnTtDx48epZ8+eNGTIEDp79qzi6lVOeZVSr4aOHTtGK1asEEGjLovXLz9yBapHx44dNRMnTtS+Ly4u1gQFBWkWLFigqCqYPXu2pnXr1ho14F+hTZs2ad+XlJRoAgICNB988IF2XVZWlsbV1VXz3XffaZRUVjZmzBjNkCFDNEqUnp4uyrx//35tPTo7O2s2bNig3efPP/8U+0RHR2uUVl7WvXt3zaRJkzRKVKdOHc0XX3yh+Ho1LK9S6zU3N1fTpEkTza5du/TKVxX1ixananL//n0R/XO3je6jXfh9dHQ0KQ13TXH3TlhYGI0aNYqSkpJIDRITEyk1NVWvnjmNP3fLKrGe2b59+0RXT9OmTWnChAmUmZlJSpCdnS1efXx8xCv//nKrjG7dchd0gwYNFFG3huWVrF27lurWrUuRkZE0ffp0unv3Ltmz4uJiWr9+vWhZ4y4spderYXmVWq8TJ06kgQMH6tUjq4r6xbPqqsmtW7fEB7hevXp66/n9+fPnSUk4SFi9erW4kXITcFRUFD3++OMUHx8vxlMoGQdNzFg9S9uUhLvpuMk7NDSULl++TDNmzKABAwaI/5CcnJzInh/0zWMkHnvsMXFjYVx/Li4uVLt2bcXVrbHyshdeeIEaNmwo/gg6c+YMvf3222Ic1E8//UT2Ji4uTgQO3GXO41w2bdpEERERFBsbq8h6NVVepdUr48CQh79wV52hqvi9ReAEFsc3Tgn3NXMgxb+kP/zwA40bNw5XXEFGjBih/bply5aivhs3bixaoXr16kX2/NcrB/pKGptXkfKOHz9er355wgPXKwfJXM/2hP+Q4yCJW9Y2btxIY8aMEeNdlMpUeTl4UlK9Xr9+nSZNmiTG6fGkq+qArrpqwk2i/Be44Uh+fh8QEEBKxpH+I488QgkJCaR0Ul2qsZ4Zd83yZ92e6/rNN9+kX375hfbu3SsG2Uq4/rjLPSsrS1F1a6q8xvAfQcwe65dbHcLDw6l9+/ZiRiFPeli8eLFi69VUeZVWrydOnBATrNq1a0c1atQQCweIPEGHv+aWJUvXLwKnavwQ8wd4z549es3j/F6331mJ8vLyxF8y/FeN0nGXFf8y6tZzTk6OmF2n9HpmycnJYoyTPdY1j3/nIIK7NH7//XdRl7r499fZ2Vmvbrl7g8fv2WPdlldeY7gFg9lj/Rri/38LCwsVV6/llVdp9dqrVy/RLcllkJYOHTqIsbXS1xavX4sNaYdyrV+/XsyuWr16tebcuXOa8ePHa2rXrq1JTU1V1NX75z//qdm3b58mMTFRc+jQIU3v3r01devWFbN2lDJ749SpU2LhX6H/+7//E19fu3ZNbF+4cKGo1y1btmjOnDkjZp2FhoZq7t27p1FSWXnbv/71LzEzhet69+7dmnbt2omZLQUFBRp7M2HCBI23t7f47KakpGiXu3fvavd5/fXXNQ0aNND8/vvvmuPHj2u6dOkiFntUXnkTEhI0c+bMEeXk+uXPc1hYmOaJJ57Q2Jt///vfYrYgl4N/J/m9g4ODZufOnYqr1/LKq6R6NcVw1qCl6xeBUzX75JNPRAW6uLiI9AQxMTEapfnb3/6mCQwMFGUMDg4W7/mXVSn27t0rggjDhafmSykJZs6cqalXr54IlHv16qW5cOGCRmll5Rts3759NX5+fmK6b8OGDTWvvvqq3f4hYKycvKxatUq7Dwe/b7zxhpjaXbNmTc0zzzwjgg0lljcpKUncTH18fMTnODw8XDN16lRNdna2xt68/PLL4vPJ/yfx55V/J6WgSWn1Wl55lVSvcgMnS9evA/9juUYzAAAAAOXCGCcAAAAAmRA4AQAAAMiEwAkAAABAJgROAAAAADIhcAIAAACQCYETAAAAgEwInAAAAABkQuAEAAAAIBMCJwAAAACZEDgBgCJlZGTQhAkTqEGDBuTq6ioevtyvXz86dOiQ2O7g4ECbN2+29mkCgJ2pYe0TAACoCs8++yzdv3+f1qxZQ2FhYZSWliaekJ6ZmYkLDgAVhmfVAYDiZGVlUZ06dWjfvn3UvXv3UtsbNWpE165d075v2LAhXb16VXy9ZcsWioqKonPnzlFQUBCNGTOG3nnnHapRo4a2peqzzz6jn3/+WRw/MDCQFi1aRM8991w1lhAArAVddQCgOLVq1RILd8UVFhaW2n7s2DHxumrVKkpJSdG+P3jwII0ePZomTZokAqcVK1bQ6tWraf78+XrfP3PmTNGidfr0aRo1ahSNGDGC/vzzz2oqHQBYE1qcAECRfvzxR3r11Vfp3r171K5dO9HyxAFOq1attC1HmzZtoqFDh2q/p3fv3tSrVy+aPn26dt23335L06ZNo5s3b2q/7/XXX6dly5Zp9+ncubP4GdwSBQDKhhYnAFAkbhHiYIe71Pr37y+61Ti44RYkU7gFac6cOdoWK144+OJWqbt372r369Kli9738Xu0OAGoAwaHA4Biubm5UZ8+fcTC3WuvvPIKzZ49m8aOHWt0/7y8PDG+adiwYUaPBQCAFicAUI2IiAjKz88XXzs7O1NxcbHedm6RunDhAoWHh5daHB3/999lTEyM3vfx++bNm1dTKQDAmtDiBACKwykHhg8fTi+//LIY0+Tp6UnHjx8Xs9+GDBminVnH6Qkee+wxkeeJZ+HNmjWLnn76aZH7iWfJcbDE3Xfx8fE0b9487fE3bNhAHTp0oG7dutHatWvp6NGj9OWXX1qxxABQXTA4HAAUh2fSvffee7Rz5066fPkyFRUVUUhIiAimZsyYQe7u7rR161aaMmWKSEMQHBysTUewY8cOMc7p1KlTolWqWbNmoouPxzpJg8M//fRTMWPvwIEDIh3B+++/T88//7yVSw0A1QGBEwCAOf9pGpmNBwDqgTFOAAAAADIhcAIAAACQCYPDAQDMoNFocL0AVAwtTgAAAAAyIXACAAAAkAmBEwAAAIBMCJwAAAAAZELgBAAAACATAicAAAAAmRA4AQAAAMiEwAkAAABAJgROAAAAACTP/wc6oGJeVvRimgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved metrics to /Users/mark/git/learning-ml-to-llm/projects/phase2_transformers/project15_analysis/analysis_artifacts/comparison_metrics.json\n",
      "Saved warm-up losses array.\n"
     ]
    }
   ],
   "source": [
    "# 7) Warm-up training of random model (short) and comparison\n",
    "import time\n",
    "\n",
    "WARMUP_STEPS = 40\n",
    "LR = 3e-4\n",
    "optimizer = torch.optim.AdamW(random_model.parameters(), lr=LR)\n",
    "random_model.train()\n",
    "\n",
    "warmup_losses = []\n",
    "start = time.time()\n",
    "for step in range(WARMUP_STEPS):\n",
    "    xb, yb = get_batch_from_ids(val_ids, batch_size, block_size)\n",
    "    logits, loss = random_model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(random_model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    warmup_losses.append(loss.item())\n",
    "    if (step+1) % 10 == 0 or step == 0:\n",
    "        print(f'Step {step+1}/{WARMUP_STEPS} loss: {loss.item():.3f}')\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f'Warm-up completed in {elapsed:.1f}s')\n",
    "\n",
    "# Evaluate perplexity after warm-up\n",
    "post_loss = estimate_loss_for(random_model, val_ids, iters=EVAL_ITERS)\n",
    "post_ppl = float(np.exp(min(post_loss, 20)))\n",
    "print(f'Random after warm-up - loss: {post_loss:.3f}, ppl: {post_ppl:.1f}')\n",
    "\n",
    "# Plot warm-up curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(warmup_losses, marker='o')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Random Model Warm-up Loss')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save artifacts\n",
    "artifact_dir = cur / 'analysis_artifacts'\n",
    "artifact_dir.mkdir(exist_ok=True)\n",
    "\n",
    "import json\n",
    "results = {\n",
    "    'uniform_loss': uniform_loss,\n",
    "    'uniform_perplexity': uniform_ppl,\n",
    "    'random_initial_loss': random_loss,\n",
    "    'random_initial_perplexity': random_ppl,\n",
    "    'random_post_warmup_loss': post_loss,\n",
    "    'random_post_warmup_perplexity': post_ppl,\n",
    "    'pretrained_loss': pretrain_loss if pretrain_loss is not None else None,\n",
    "    'pretrained_perplexity': pretrain_ppl if pretrain_ppl is not None else None,\n",
    "    'warmup_steps': WARMUP_STEPS,\n",
    "    'learning_rate': LR,\n",
    "}\n",
    "with open(artifact_dir / 'comparison_metrics.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print('Saved metrics to', artifact_dir / 'comparison_metrics.json')\n",
    "\n",
    "# Save warm-up losses\n",
    "np.save(artifact_dir / 'warmup_losses.npy', np.array(warmup_losses))\n",
    "print('Saved warm-up losses array.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
