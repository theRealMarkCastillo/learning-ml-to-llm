# Phase 2: Transformers & Pretraining

## Overview
Build and pretrain a tiny transformer to understand what happens inside base models.

## Projects (Weeks 13-17)

### Project 12: Transformer Architecture (Week 13-14)
Build decoder-only transformer from scratch:
- Multi-head self-attention
- Feed-forward networks
- Positional embeddings
- Stacking transformer blocks

### Project 13: Tokenization (Week 14)
Understand text preprocessing:
- Character-level vs subword tokenization
- Byte-pair encoding (BPE)
- Special tokens and vocabulary
- Data loader creation

### Project 14: Pretraining (Week 15-16) ⭐ CORE PROJECT
Pretrain tiny transformer on Shakespeare:
- Next-token prediction loss
- Training loop and optimization
- Loss dynamics observation
- Text generation from trained model
- **Runtime**: 4-12 hours on M4

### Project 15: Analysis (Week 17)
Compare pretrained vs random models:
- Transfer learning effectiveness
- Why pretraining matters
- Fine-tuning vs training from scratch

## Learning Outcomes
- Deep understanding of transformer architecture
- Experience with pretraining process
- Insight into what base models learn
- Foundation for understanding Mistral fine-tuning

## Hardware Requirements
- M4 Mac with 64GB RAM: ✓ Perfect
- Training time: 4-12 hours for tiny model
- Memory usage: ~3GB during training

## Time Estimate
4-6 weeks of focused learning
