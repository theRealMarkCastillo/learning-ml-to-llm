{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9192df0",
   "metadata": {},
   "source": [
    "# Project 13: Tokenization and Text Preprocessing\n",
    "\n",
    "## Goal\n",
    "Understand how text becomes data for transformers.\n",
    "\n",
    "## Learning Objectives\n",
    "- Character-level vs subword tokenization\n",
    "- Byte-pair encoding (BPE)\n",
    "- Special tokens (BOS, EOS, PAD, UNK)\n",
    "- Creating data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89c55c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to explore tokenization!\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(\"Ready to explore tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b451cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample corpus:\n",
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "Machine learning models require lots of data.\n",
      "Transformers use attention mechanisms to process sequences.\n",
      "Natural language processing is fascinating.\n",
      "Deep learning has revolutionized AI.\n",
      "\n",
      "\n",
      "Corpus length: 233 characters\n",
      "Unique characters: 35\n"
     ]
    }
   ],
   "source": [
    "# 1) Sample text corpus\n",
    "corpus = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Machine learning models require lots of data.\n",
    "Transformers use attention mechanisms to process sequences.\n",
    "Natural language processing is fascinating.\n",
    "Deep learning has revolutionized AI.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample corpus:\")\n",
    "print(corpus)\n",
    "print(f\"\\nCorpus length: {len(corpus)} characters\")\n",
    "print(f\"Unique characters: {len(set(corpus))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a13091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character vocabulary size: 35\n",
      "Vocabulary: ['\\n', ' ', '.', 'A', 'D', 'I', 'M', 'N', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']...\n",
      "\n",
      "Original: Hello transformers!\n",
      "Encoded: [13, 20, 20, 23, 1, 28, 26, 9, 22, 27, 14, 23, 26, 21, 13, 26, 27]\n",
      "Decoded: ello transformers\n"
     ]
    }
   ],
   "source": [
    "# 2) Character-Level Tokenization\n",
    "class CharTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        # Build vocabulary from unique characters\n",
    "        chars = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.id_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to list of token IDs\"\"\"\n",
    "        return [self.char_to_id[ch] for ch in text if ch in self.char_to_id]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back to text\"\"\"\n",
    "        return ''.join([self.id_to_char[i] for i in ids])\n",
    "\n",
    "# Create tokenizer\n",
    "char_tokenizer = CharTokenizer(corpus)\n",
    "print(f\"Character vocabulary size: {char_tokenizer.vocab_size}\")\n",
    "print(f\"Vocabulary: {list(char_tokenizer.char_to_id.keys())[:20]}...\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_text = \"Hello transformers!\"\n",
    "encoded = char_tokenizer.encode(test_text)\n",
    "decoded = char_tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nOriginal: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad34afeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge 1: ('<', '/') → </ (freq: 33)\n",
      "Merge 2: ('</', 'w') → </w (freq: 33)\n",
      "Merge 3: ('</w', '>') → </w> (freq: 33)\n",
      "Merge 4: ('s', '</w>') → s</w> (freq: 8)\n",
      "Merge 5: ('e', '</w>') → e</w> (freq: 6)\n",
      "\n",
      "Total vocabulary size: 60\n",
      "\n",
      "Top 20 tokens by frequency:\n",
      "  'w': 34\n",
      "  '<': 33\n",
      "  '/': 33\n",
      "  '>': 33\n",
      "  '</': 33\n",
      "  '</w': 33\n",
      "  '</w>': 33\n",
      "  'e': 23\n",
      "  'n': 17\n",
      "  's': 17\n",
      "  'a': 17\n",
      "  'i': 14\n",
      "  'o': 14\n",
      "  'r': 13\n",
      "  't': 12\n",
      "  'u': 8\n",
      "  'l': 8\n",
      "  's</w>': 8\n",
      "  'c': 7\n",
      "  'g': 7\n"
     ]
    }
   ],
   "source": [
    "# 3) Byte-Pair Encoding (BPE) from Scratch\n",
    "class SimpleBPE:\n",
    "    def __init__(self, num_merges=50):\n",
    "        self.num_merges = num_merges\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def get_stats(self, words):\n",
    "        \"\"\"Count frequency of adjacent pairs\"\"\"\n",
    "        pairs = Counter()\n",
    "        for word in words:\n",
    "            for i in range(len(word) - 1):\n",
    "                pairs[word[i], word[i+1]] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair(self, pair, words):\n",
    "        \"\"\"Merge most frequent pair in all words\"\"\"\n",
    "        new_words = []\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        \n",
    "        for word in words:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(replacement)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_words.append(tuple(new_word))\n",
    "        return new_words\n",
    "    \n",
    "    def train(self, text):\n",
    "        \"\"\"Train BPE on text corpus\"\"\"\n",
    "        # Start with character-level tokens\n",
    "        words = [tuple(word + '</w>') for word in text.split()]\n",
    "        \n",
    "        # Build initial vocabulary\n",
    "        self.vocab = Counter()\n",
    "        for word in words:\n",
    "            for char in word:\n",
    "                self.vocab[char] = self.vocab.get(char, 0) + 1\n",
    "        \n",
    "        # Perform merges\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self.get_stats(words)\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            self.merges[best_pair] = ''.join(best_pair)\n",
    "            words = self.merge_pair(best_pair, words)\n",
    "            \n",
    "            # Update vocab\n",
    "            merged_token = ''.join(best_pair)\n",
    "            self.vocab[merged_token] = pairs[best_pair]\n",
    "            \n",
    "            if i < 5:  # Show first few merges\n",
    "                print(f\"Merge {i+1}: {best_pair} → {merged_token} (freq: {pairs[best_pair]})\")\n",
    "        \n",
    "        print(f\"\\nTotal vocabulary size: {len(self.vocab)}\")\n",
    "        return self.vocab\n",
    "\n",
    "# Train BPE\n",
    "bpe = SimpleBPE(num_merges=30)\n",
    "vocab = bpe.train(corpus.lower())\n",
    "\n",
    "print(f\"\\nTop 20 tokens by frequency:\")\n",
    "for token, freq in sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(f\"  '{token}': {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a04ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (with specials): 39\n",
      "Special tokens:\n",
      "  PAD: 0, UNK: 1\n",
      "  BOS: 2, EOS: 3\n",
      "\n",
      "Text: Hello!\n",
      "Encoded (with BOS/EOS): [2, 1, 17, 24, 24, 27, 1, 3]\n",
      "Decoded: <UNK>ello<UNK>\n",
      "Padded to 15: [2, 1, 17, 24, 24, 27, 1, 3, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 4) Special Tokens\n",
    "class TokenizerWithSpecials:\n",
    "    def __init__(self, corpus):\n",
    "        # Special tokens\n",
    "        self.BOS = '<BOS>'  # Beginning of sequence\n",
    "        self.EOS = '<EOS>'  # End of sequence\n",
    "        self.PAD = '<PAD>'  # Padding\n",
    "        self.UNK = '<UNK>'  # Unknown token\n",
    "        \n",
    "        # Build vocabulary with special tokens first\n",
    "        special_tokens = [self.PAD, self.UNK, self.BOS, self.EOS]\n",
    "        chars = sorted(list(set(corpus)))\n",
    "        all_tokens = special_tokens + chars\n",
    "        \n",
    "        self.vocab_size = len(all_tokens)\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(all_tokens)}\n",
    "        self.id_to_token = {i: tok for i, tok in enumerate(all_tokens)}\n",
    "        \n",
    "        # Special token IDs\n",
    "        self.pad_id = self.token_to_id[self.PAD]\n",
    "        self.unk_id = self.token_to_id[self.UNK]\n",
    "        self.bos_id = self.token_to_id[self.BOS]\n",
    "        self.eos_id = self.token_to_id[self.EOS]\n",
    "        \n",
    "    def encode(self, text, add_special=True):\n",
    "        \"\"\"Encode text with optional special tokens\"\"\"\n",
    "        ids = [self.token_to_id.get(ch, self.unk_id) for ch in text]\n",
    "        if add_special:\n",
    "            ids = [self.bos_id] + ids + [self.eos_id]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids, skip_special=True):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        special_ids = {self.pad_id, self.bos_id, self.eos_id}\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            if skip_special and i in special_ids:\n",
    "                continue\n",
    "            tokens.append(self.id_to_token.get(i, self.UNK))\n",
    "        return ''.join(tokens)\n",
    "    \n",
    "    def pad_sequence(self, ids, max_len):\n",
    "        \"\"\"Pad sequence to max_len\"\"\"\n",
    "        if len(ids) < max_len:\n",
    "            ids = ids + [self.pad_id] * (max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "        return ids\n",
    "\n",
    "# Test tokenizer with special tokens\n",
    "tokenizer = TokenizerWithSpecials(corpus)\n",
    "print(f\"Vocabulary size (with specials): {tokenizer.vocab_size}\")\n",
    "print(f\"Special tokens:\")\n",
    "print(f\"  PAD: {tokenizer.pad_id}, UNK: {tokenizer.unk_id}\")\n",
    "print(f\"  BOS: {tokenizer.bos_id}, EOS: {tokenizer.eos_id}\")\n",
    "\n",
    "# Test encoding with special tokens\n",
    "text = \"Hello!\"\n",
    "encoded = tokenizer.encode(text, add_special=True)\n",
    "decoded = tokenizer.decode(encoded, skip_special=True)\n",
    "\n",
    "print(f\"\\nText: {text}\")\n",
    "print(f\"Encoded (with BOS/EOS): {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Test padding\n",
    "padded = tokenizer.pad_sequence(encoded, max_len=15)\n",
    "print(f\"Padded to 15: {padded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98e72b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Character-Level Tokenization ===\n",
      "Original text length: 71\n",
      "Number of tokens: 71\n",
      "Vocabulary size: 35\n",
      "Compression ratio: 1.00\n",
      "\n",
      "=== Subword (BPE) Tokenization ===\n",
      "Vocabulary size: 60\n",
      "Note: BPE creates multi-character tokens, reducing sequence length\n",
      "Example BPE tokens: 'the', 'er', 'ing', 'tion' (common subwords)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5) Comparison: Character vs BPE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compare on sample text\n",
    "sample = \"The transformer architecture revolutionized natural language processing\"\n",
    "\n",
    "# Character tokenization\n",
    "char_tokens = char_tokenizer.encode(sample)\n",
    "char_compression = len(sample) / len(char_tokens)\n",
    "\n",
    "print(\"=== Character-Level Tokenization ===\")\n",
    "print(f\"Original text length: {len(sample)}\")\n",
    "print(f\"Number of tokens: {len(char_tokens)}\")\n",
    "print(f\"Vocabulary size: {char_tokenizer.vocab_size}\")\n",
    "print(f\"Compression ratio: {char_compression:.2f}\")\n",
    "\n",
    "print(\"\\n=== Subword (BPE) Tokenization ===\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"Note: BPE creates multi-character tokens, reducing sequence length\")\n",
    "print(\"Example BPE tokens: 'the', 'er', 'ing', 'tion' (common subwords)\")\n",
    "\n",
    "# Visualize vocabulary sizes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Vocab size comparison\n",
    "methods = ['Character', 'BPE']\n",
    "sizes = [char_tokenizer.vocab_size, len(vocab)]\n",
    "ax1.bar(methods, sizes, color=['#3498db', '#e74c3c'])\n",
    "ax1.set_ylabel('Vocabulary Size')\n",
    "ax1.set_title('Vocabulary Size Comparison')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Token frequency distribution (BPE)\n",
    "top_tokens = sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "tokens, freqs = zip(*top_tokens)\n",
    "ax2.barh(range(len(tokens)), freqs, color='#2ecc71')\n",
    "ax2.set_yticks(range(len(tokens)))\n",
    "ax2.set_yticklabels([t[:10] for t in tokens])\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_title('Top 15 BPE Tokens')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ce70e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPT-2 Tokenizer ===\n",
      "Vocabulary size: 50257\n",
      "Special tokens: {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n",
      "\n",
      "Original text: The transformer architecture revolutionized natural language processing!\n",
      "Tokens: ['The', 'Ġtransformer', 'Ġarchitecture', 'Ġrevolution', 'ized', 'Ġnatural', 'Ġlanguage', 'Ġprocessing', '!']\n",
      "Token IDs: [464, 47385, 10959, 5854, 1143, 3288, 3303, 7587, 0]\n",
      "Number of tokens: 9\n",
      "\n",
      "Subword breakdown:\n",
      "  'transformer' → ['trans', 'former']\n",
      "  'attention' → ['att', 'ention']\n",
      "  'language' → ['language']\n",
      "  'revolutionized' → ['revolution', 'ized']\n",
      "\n",
      "Decoded: The transformer architecture revolutionized natural language processing!\n"
     ]
    }
   ],
   "source": [
    "# 6) Real-World Tokenizer: HuggingFace GPT-2 BPE\n",
    "try:\n",
    "    from transformers import GPT2Tokenizer\n",
    "    \n",
    "    # Load pretrained GPT-2 tokenizer\n",
    "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    print(\"=== GPT-2 Tokenizer ===\")\n",
    "    print(f\"Vocabulary size: {len(gpt2_tokenizer)}\")\n",
    "    print(f\"Special tokens: {gpt2_tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    # Test tokenization\n",
    "    text = \"The transformer architecture revolutionized natural language processing!\"\n",
    "    tokens = gpt2_tokenizer.tokenize(text)\n",
    "    token_ids = gpt2_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"\\nOriginal text: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    \n",
    "    # Show encoding of specific words\n",
    "    words = [\"transformer\", \"attention\", \"language\", \"revolutionized\"]\n",
    "    print(f\"\\nSubword breakdown:\")\n",
    "    for word in words:\n",
    "        word_tokens = gpt2_tokenizer.tokenize(word)\n",
    "        print(f\"  '{word}' → {word_tokens}\")\n",
    "    \n",
    "    # Decode back\n",
    "    decoded = gpt2_tokenizer.decode(token_ids)\n",
    "    print(f\"\\nDecoded: {decoded}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"transformers library not installed. Install with: pip install transformers\")\n",
    "    print(\"\\nGPT-2 uses BPE with ~50K vocab size\")\n",
    "    print(\"Special tokens: <|endoftext|> for BOS/EOS\")\n",
    "    print(\"Handles rare words by breaking into subwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6414d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in corpus: 233\n",
      "Dataset size: 217 sequences\n",
      "Batch size: 4\n",
      "Number of batches: 55\n",
      "\n",
      "Sample batch shapes:\n",
      "  Input (x): torch.Size([4, 16])\n",
      "  Target (y): torch.Size([4, 16])\n",
      "\n",
      "First sample in batch:\n",
      "  Input tokens: [33, 25, 28, 31, 5, 27, 34, 17, 30, 5, 32, 20, 17, 5, 24, 13]\n",
      "  Target tokens: [25, 28, 31, 5, 27, 34, 17, 30, 5, 32, 20, 17, 5, 24, 13, 38]\n",
      "  Input text: umps over the la\n",
      "  Target text: mps over the laz\n"
     ]
    }
   ],
   "source": [
    "# 7) Create PyTorch Dataset for Language Modeling\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, block_size=32):\n",
    "        \"\"\"\n",
    "        Create dataset for next-token prediction\n",
    "        \n",
    "        Args:\n",
    "            text: Input text corpus\n",
    "            tokenizer: Tokenizer with encode method\n",
    "            block_size: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Tokenize entire corpus\n",
    "        self.tokens = tokenizer.encode(text, add_special=False)\n",
    "        print(f\"Total tokens in corpus: {len(self.tokens)}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Number of possible blocks\n",
    "        return max(0, len(self.tokens) - self.block_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get block of tokens\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        # Input and target (shifted by 1)\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TextDataset(corpus, tokenizer, block_size=16)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} sequences\")\n",
    "print(f\"Batch size: 4\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Show sample batch\n",
    "batch_x, batch_y = next(iter(dataloader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "print(f\"  Input (x): {batch_x.shape}\")\n",
    "print(f\"  Target (y): {batch_y.shape}\")\n",
    "\n",
    "# Decode first sample\n",
    "print(f\"\\nFirst sample in batch:\")\n",
    "print(f\"  Input tokens: {batch_x[0].tolist()}\")\n",
    "print(f\"  Target tokens: {batch_y[0].tolist()}\")\n",
    "print(f\"  Input text: {tokenizer.decode(batch_x[0].tolist(), skip_special=True)}\")\n",
    "print(f\"  Target text: {tokenizer.decode(batch_y[0].tolist(), skip_special=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1201af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batching with dynamic padding:\n",
      "  Input shape: torch.Size([4, 16])\n",
      "  Target shape: torch.Size([4, 16])\n",
      "  Attention mask shape: torch.Size([4, 16])\n",
      "\n",
      "Attention mask (1=real token, 0=padding):\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 8) Batching with Padding (Variable Length Sequences)\n",
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to handle variable-length sequences\n",
    "    Pads sequences to max length in batch\n",
    "    \"\"\"\n",
    "    xs, ys = zip(*batch)\n",
    "    \n",
    "    # Find max length in batch\n",
    "    max_len = max(len(x) for x in xs)\n",
    "    \n",
    "    # Pad sequences\n",
    "    pad_id = tokenizer.pad_id\n",
    "    xs_padded = torch.stack([\n",
    "        torch.cat([x, torch.full((max_len - len(x),), pad_id, dtype=torch.long)])\n",
    "        if len(x) < max_len else x\n",
    "        for x in xs\n",
    "    ])\n",
    "    ys_padded = torch.stack([\n",
    "        torch.cat([y, torch.full((max_len - len(y),), pad_id, dtype=torch.long)])\n",
    "        if len(y) < max_len else y\n",
    "        for y in ys\n",
    "    ])\n",
    "    \n",
    "    # Create attention mask (1 for real tokens, 0 for padding)\n",
    "    attention_mask = (xs_padded != pad_id).long()\n",
    "    \n",
    "    return xs_padded, ys_padded, attention_mask\n",
    "\n",
    "# Test collate function\n",
    "dataloader_padded = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_batch)\n",
    "batch_x, batch_y, mask = next(iter(dataloader_padded))\n",
    "\n",
    "print(\"Batching with dynamic padding:\")\n",
    "print(f\"  Input shape: {batch_x.shape}\")\n",
    "print(f\"  Target shape: {batch_y.shape}\")\n",
    "print(f\"  Attention mask shape: {mask.shape}\")\n",
    "print(f\"\\nAttention mask (1=real token, 0=padding):\")\n",
    "print(mask.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd8cebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenizer vocabulary to: data/processed/tokenizer_vocab.json\n",
      "Vocabulary size: 39\n",
      "\n",
      "To load:\n",
      "```python\n",
      "import json\n",
      "with open('data/processed/tokenizer_vocab.json') as f:\n",
      "    vocab = json.load(f)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 9) Save Tokenizer\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path('data/processed')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save vocabulary\n",
    "vocab_dict = {\n",
    "    'token_to_id': tokenizer.token_to_id,\n",
    "    'id_to_token': tokenizer.id_to_token,\n",
    "    'special_tokens': {\n",
    "        'PAD': tokenizer.pad_id,\n",
    "        'UNK': tokenizer.unk_id,\n",
    "        'BOS': tokenizer.bos_id,\n",
    "        'EOS': tokenizer.eos_id\n",
    "    },\n",
    "    'vocab_size': tokenizer.vocab_size\n",
    "}\n",
    "\n",
    "with open(out_dir / 'tokenizer_vocab.json', 'w') as f:\n",
    "    json.dump(vocab_dict, f, indent=2)\n",
    "\n",
    "print(f\"Saved tokenizer vocabulary to: {out_dir / 'tokenizer_vocab.json'}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nTo load:\")\n",
    "print(\"```python\")\n",
    "print(\"import json\")\n",
    "print(\"with open('data/processed/tokenizer_vocab.json') as f:\")\n",
    "print(\"    vocab = json.load(f)\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c457908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Summary: Tokenization for Transformers\n",
       "\n",
       "### What We Learned\n",
       "\n",
       "**1. Character-Level Tokenization**\n",
       "- Simplest approach: each character is a token\n",
       "- Small vocabulary (26 letters + punctuation + space ≈ 50-100 tokens)\n",
       "- Long sequences (every character is a token)\n",
       "- Good for: character-level tasks, small alphabets\n",
       "- Bad for: long sequences, large vocabularies\n",
       "\n",
       "**2. Byte-Pair Encoding (BPE)**\n",
       "- Iteratively merges most frequent character pairs\n",
       "- Creates subword vocabulary (common prefixes/suffixes)\n",
       "- Balances vocab size vs sequence length\n",
       "- Used by: GPT-2, GPT-3, RoBERTa\n",
       "- Advantages:\n",
       "  - Handles rare words by breaking into subwords\n",
       "  - No unknown tokens (can always fall back to characters)\n",
       "  - Compression: fewer tokens than character-level\n",
       "\n",
       "**3. Special Tokens**\n",
       "- `<PAD>`: Padding for batch processing\n",
       "- `<UNK>`: Unknown/out-of-vocabulary tokens\n",
       "- `<BOS>`: Beginning of sequence\n",
       "- `<EOS>`: End of sequence\n",
       "- Model learns to recognize and use these appropriately\n",
       "\n",
       "**4. Data Preparation**\n",
       "- **Encoding**: Text → Token IDs (integers)\n",
       "- **Decoding**: Token IDs → Text\n",
       "- **Padding**: Ensure equal length in batches\n",
       "- **Attention Mask**: Mark real vs padded tokens\n",
       "\n",
       "### Tokenization Comparison\n",
       "\n",
       "| Method | Vocab Size | Seq Length | Use Case |\n",
       "|--------|-----------|------------|----------|\n",
       "| Character | ~50-100 | Very Long | Simple, small data |\n",
       "| BPE | ~5K-50K | Moderate | General purpose (GPT) |\n",
       "| WordPiece | ~30K | Moderate | BERT, Transformers |\n",
       "| SentencePiece | ~32K | Moderate | Multilingual (T5) |\n",
       "\n",
       "### BPE Algorithm Steps\n",
       "\n",
       "1. Start with character vocabulary\n",
       "2. Count all adjacent character pairs\n",
       "3. Merge most frequent pair into new token\n",
       "4. Update vocabulary\n",
       "5. Repeat for N iterations\n",
       "6. Result: vocabulary of characters + common subwords\n",
       "\n",
       "### PyTorch DataLoader Pattern\n",
       "\n",
       "```python\n",
       "# 1. Create Dataset\n",
       "dataset = TextDataset(text, tokenizer, block_size=512)\n",
       "\n",
       "# 2. Create DataLoader\n",
       "dataloader = DataLoader(\n",
       "    dataset,\n",
       "    batch_size=32,\n",
       "    shuffle=True,\n",
       "    collate_fn=collate_batch  # handles padding\n",
       ")\n",
       "\n",
       "# 3. Training loop\n",
       "for batch_x, batch_y, mask in dataloader:\n",
       "    logits = model(batch_x, mask)\n",
       "    loss = F.cross_entropy(logits.view(-1, vocab_size), batch_y.view(-1))\n",
       "    loss.backward()\n",
       "```\n",
       "\n",
       "### Best Practices\n",
       "\n",
       "1. **Choose tokenizer based on task**:\n",
       "   - Character: very small data, character-level modeling\n",
       "   - BPE/WordPiece: most NLP tasks\n",
       "   - SentencePiece: multilingual\n",
       "\n",
       "2. **Vocabulary size trade-offs**:\n",
       "   - Larger vocab → shorter sequences → faster\n",
       "   - Smaller vocab → longer sequences → more context\n",
       "   - Typical: 5K-50K tokens\n",
       "\n",
       "3. **Handle special tokens consistently**:\n",
       "   - Always reserve IDs 0-3 for PAD, UNK, BOS, EOS\n",
       "   - Include special tokens during training\n",
       "   - Model learns their meaning\n",
       "\n",
       "4. **Batching strategies**:\n",
       "   - Fixed-length: pad to max_len (wasteful but simple)\n",
       "   - Dynamic: pad to max in batch (efficient)\n",
       "   - Bucketing: group similar-length sequences\n",
       "\n",
       "5. **Attention masks**:\n",
       "   - 1 for real tokens, 0 for padding\n",
       "   - Model ignores padded positions\n",
       "   - Essential for variable-length sequences\n",
       "\n",
       "### Real-World Tokenizers\n",
       "\n",
       "- **GPT-2/3**: BPE with ~50K vocab\n",
       "- **BERT**: WordPiece with ~30K vocab\n",
       "- **T5**: SentencePiece with ~32K vocab\n",
       "- **LLaMA**: SentencePiece with ~32K vocab\n",
       "\n",
       "### Key Takeaways\n",
       "\n",
       "✅ Tokenization converts text to integers for model input  \n",
       "✅ BPE balances vocab size and sequence length  \n",
       "✅ Special tokens handle edge cases (padding, unknown)  \n",
       "✅ DataLoaders handle batching and padding automatically  \n",
       "✅ Attention masks distinguish real vs padded tokens  \n",
       "✅ Same tokenizer must be used for training and inference  \n",
       "\n",
       "### Next Steps\n",
       "\n",
       "- **Project 14**: Train transformer on tokenized data\n",
       "- **Project 15**: Compare random vs pretrained models\n",
       "- Experiment with different vocab sizes\n",
       "- Try multilingual tokenization (SentencePiece)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10) Summary & Key Concepts\n",
    "from IPython.display import Markdown, display\n",
    "display(Markdown(r'''\n",
    "## Summary: Tokenization for Transformers\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "**1. Character-Level Tokenization**\n",
    "- Simplest approach: each character is a token\n",
    "- Small vocabulary (26 letters + punctuation + space ≈ 50-100 tokens)\n",
    "- Long sequences (every character is a token)\n",
    "- Good for: character-level tasks, small alphabets\n",
    "- Bad for: long sequences, large vocabularies\n",
    "\n",
    "**2. Byte-Pair Encoding (BPE)**\n",
    "- Iteratively merges most frequent character pairs\n",
    "- Creates subword vocabulary (common prefixes/suffixes)\n",
    "- Balances vocab size vs sequence length\n",
    "- Used by: GPT-2, GPT-3, RoBERTa\n",
    "- Advantages:\n",
    "  - Handles rare words by breaking into subwords\n",
    "  - No unknown tokens (can always fall back to characters)\n",
    "  - Compression: fewer tokens than character-level\n",
    "\n",
    "**3. Special Tokens**\n",
    "- `<PAD>`: Padding for batch processing\n",
    "- `<UNK>`: Unknown/out-of-vocabulary tokens\n",
    "- `<BOS>`: Beginning of sequence\n",
    "- `<EOS>`: End of sequence\n",
    "- Model learns to recognize and use these appropriately\n",
    "\n",
    "**4. Data Preparation**\n",
    "- **Encoding**: Text → Token IDs (integers)\n",
    "- **Decoding**: Token IDs → Text\n",
    "- **Padding**: Ensure equal length in batches\n",
    "- **Attention Mask**: Mark real vs padded tokens\n",
    "\n",
    "### Tokenization Comparison\n",
    "\n",
    "| Method | Vocab Size | Seq Length | Use Case |\n",
    "|--------|-----------|------------|----------|\n",
    "| Character | ~50-100 | Very Long | Simple, small data |\n",
    "| BPE | ~5K-50K | Moderate | General purpose (GPT) |\n",
    "| WordPiece | ~30K | Moderate | BERT, Transformers |\n",
    "| SentencePiece | ~32K | Moderate | Multilingual (T5) |\n",
    "\n",
    "### BPE Algorithm Steps\n",
    "\n",
    "1. Start with character vocabulary\n",
    "2. Count all adjacent character pairs\n",
    "3. Merge most frequent pair into new token\n",
    "4. Update vocabulary\n",
    "5. Repeat for N iterations\n",
    "6. Result: vocabulary of characters + common subwords\n",
    "\n",
    "### PyTorch DataLoader Pattern\n",
    "\n",
    "```python\n",
    "# 1. Create Dataset\n",
    "dataset = TextDataset(text, tokenizer, block_size=512)\n",
    "\n",
    "# 2. Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch  # handles padding\n",
    ")\n",
    "\n",
    "# 3. Training loop\n",
    "for batch_x, batch_y, mask in dataloader:\n",
    "    logits = model(batch_x, mask)\n",
    "    loss = F.cross_entropy(logits.view(-1, vocab_size), batch_y.view(-1))\n",
    "    loss.backward()\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Choose tokenizer based on task**:\n",
    "   - Character: very small data, character-level modeling\n",
    "   - BPE/WordPiece: most NLP tasks\n",
    "   - SentencePiece: multilingual\n",
    "\n",
    "2. **Vocabulary size trade-offs**:\n",
    "   - Larger vocab → shorter sequences → faster\n",
    "   - Smaller vocab → longer sequences → more context\n",
    "   - Typical: 5K-50K tokens\n",
    "\n",
    "3. **Handle special tokens consistently**:\n",
    "   - Always reserve IDs 0-3 for PAD, UNK, BOS, EOS\n",
    "   - Include special tokens during training\n",
    "   - Model learns their meaning\n",
    "\n",
    "4. **Batching strategies**:\n",
    "   - Fixed-length: pad to max_len (wasteful but simple)\n",
    "   - Dynamic: pad to max in batch (efficient)\n",
    "   - Bucketing: group similar-length sequences\n",
    "\n",
    "5. **Attention masks**:\n",
    "   - 1 for real tokens, 0 for padding\n",
    "   - Model ignores padded positions\n",
    "   - Essential for variable-length sequences\n",
    "\n",
    "### Real-World Tokenizers\n",
    "\n",
    "- **GPT-2/3**: BPE with ~50K vocab\n",
    "- **BERT**: WordPiece with ~30K vocab\n",
    "- **T5**: SentencePiece with ~32K vocab\n",
    "- **LLaMA**: SentencePiece with ~32K vocab\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "✅ Tokenization converts text to integers for model input  \n",
    "✅ BPE balances vocab size and sequence length  \n",
    "✅ Special tokens handle edge cases (padding, unknown)  \n",
    "✅ DataLoaders handle batching and padding automatically  \n",
    "✅ Attention masks distinguish real vs padded tokens  \n",
    "✅ Same tokenizer must be used for training and inference  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Project 14**: Train transformer on tokenized data\n",
    "- **Project 15**: Compare random vs pretrained models\n",
    "- Experiment with different vocab sizes\n",
    "- Try multilingual tokenization (SentencePiece)\n",
    "'''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
