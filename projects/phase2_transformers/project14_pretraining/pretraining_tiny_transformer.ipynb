{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-configure repo path and compute device (GPU/MPS/CPU)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from utils.path_helpers import add_repo_root_to_sys_path\n",
    "except Exception:\n",
    "    cur = Path.cwd()\n",
    "    for parent in [cur] + list(cur.parents):\n",
    "        if (parent / \"requirements.txt\").exists() or (parent / \".git\").exists():\n",
    "            sys.path.insert(0, str(parent))\n",
    "            break\n",
    "    from utils.path_helpers import add_repo_root_to_sys_path\n",
    "\n",
    "add_repo_root_to_sys_path()\n",
    "\n",
    "from utils.device import get_device, backend_info, backend_name, ensure_seed\n",
    "print(f\"Using backend: {backend_info()}\")\n",
    "ensure_seed(42)\n",
    "\n",
    "# For PyTorch 2.x, set default device so tensors/models go there automatically\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    if backend_name() in (\"torch_cuda\", \"torch_mps\") and hasattr(torch, \"set_default_device\"):\n",
    "        torch.set_default_device(\"cuda\" if backend_name() == \"torch_cuda\" else \"mps\")\n",
    "        print(f\"torch default device set to {torch.get_default_device()}\")\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5b3ed0",
   "metadata": {},
   "source": [
    "# Project 14: Pretraining a Tiny Transformer from Scratch\n",
    "\n",
    "## Goal\n",
    "Train a transformer from random weights using **next-token prediction** (causal language modeling). Observe loss curves, generation quality, and how a model learns language patterns purely from data.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand causal language modeling loss: predict next token given context\n",
    "- Train end-to-end: data â†’ tokenizer â†’ transformer â†’ loss â†’ backprop â†’ generation\n",
    "- Observe training dynamics: loss curves, convergence, overfitting\n",
    "- Implement data loaders, train loops, checkpointing, generation\n",
    "- Visualize what embeddings + attention heads learn\n",
    "- Compare tiny model performance to RNN baseline (Project 11.75)\n",
    "- Understand why pretraining at scale is critical (billions of tokens â†’ emergent abilities)\n",
    "\n",
    "## Prerequisites\n",
    "- Project 12 (Transformer): Know the architecture you're training\n",
    "- Project 13 (Tokenization): Understand how text â†’ token sequences\n",
    "- Project 12.1-12.25 (Attention, Embeddings): Know the components being trained\n",
    "\n",
    "## What You'll Build\n",
    "- Full training loop: data loading â†’ forward pass â†’ loss â†’ backward pass â†’ optimization\n",
    "- Character-level transformer (~20-50M parameters)\n",
    "- Training curves: loss vs. epoch, train vs. validation\n",
    "- Checkpoints: save best model weights\n",
    "- Generation: sample from trained model (greedy + nucleus sampling)\n",
    "- Attention visualizations and embedding analysis\n",
    "\n",
    "## Estimated Time\n",
    "- Code setup: 30 min\n",
    "- Training (on CPU): 4-12 hours; (on GPU/MPS): 30-60 min\n",
    "- Analysis: 1-2 hours\n",
    "\n",
    "## Usage Guide\n",
    "\n",
    "This notebook:\n",
    "1. Downloads Shakespeare text data\n",
    "2. Tokenizes with character-level or BPE tokenizer\n",
    "3. Creates train/val splits and data loaders\n",
    "4. Defines transformer architecture\n",
    "5. Implements training loop with checkpointing\n",
    "6. Generates text and analyzes learned representations\n",
    "\n",
    "Key functions:\n",
    "- `train_epoch()` â†’ forward pass + loss + backward + step\n",
    "- `evaluate()` â†’ compute validation loss without training\n",
    "- `generate()` â†’ sample tokens from model (greedy or nucleus)\n",
    "- `plot_training()` â†’ visualize loss curves\n",
    "- `analyze_embeddings()` â†’ t-SNE, frequency analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a963f5e",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "```\n",
    "Vocabulary: 5000-10000 tokens\n",
    "Dimension: 384\n",
    "Heads: 6\n",
    "Layers: 4-6\n",
    "Parameters: ~20-50M\n",
    "Dataset: Shakespeare (~5MB)\n",
    "Training time on M4: 4-12 hours\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c82d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Training on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6143aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Download Shakespeare Dataset\n",
    "import urllib.request\n",
    "\n",
    "data_dir = Path('data/raw')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "shakespeare_path = data_dir / 'shakespeare.txt'\n",
    "\n",
    "if not shakespeare_path.exists():\n",
    "    print('Downloading Shakespeare dataset...')\n",
    "    urllib.request.urlretrieve(shakespeare_url, shakespeare_path)\n",
    "    print('Download complete!')\n",
    "else:\n",
    "    print('Dataset already exists')\n",
    "\n",
    "# Load and inspect\n",
    "with open(shakespeare_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f'\\nDataset size: {len(text):,} characters')\n",
    "print(f'Unique characters: {len(set(text))}')\n",
    "print(f'\\nFirst 500 characters:')\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526f8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Build Character-Level Tokenizer\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text):\n",
    "        chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(chars)\n",
    "        self.char_to_id = {ch: i for i, ch in enumerate(chars)}\n",
    "        self.id_to_char = {i: ch for ch, i in self.char_to_id.items()}\n",
    "        \n",
    "    def encode(self, text):\n",
    "        return [self.char_to_id[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.id_to_char[i] for i in ids])\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = CharTokenizer(text)\n",
    "print(f'Vocabulary size: {tokenizer.vocab_size}')\n",
    "print(f'Vocabulary: {list(tokenizer.char_to_id.keys())}')\n",
    "\n",
    "# Test encoding/decoding\n",
    "sample = \"Hello, world!\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f'\\nTest encoding:')\n",
    "print(f'  Original: {sample}')\n",
    "print(f'  Encoded: {encoded}')\n",
    "print(f'  Decoded: {decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8120b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Prepare Train/Val Split\n",
    "# Encode entire dataset\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "print(f'Total tokens: {len(data):,}')\n",
    "\n",
    "# Split: 90% train, 10% validation\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f'Train tokens: {len(train_data):,}')\n",
    "print(f'Val tokens: {len(val_data):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load Transformer Model (from project 12)\n",
    "# Import model classes from previous notebook\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        Q = self.W_q(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        d_k = Q.size(-1)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.W_o(output)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, d_ff, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.token_embedding.weight  # Weight tying\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        batch_size, seq_len = idx.size()\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=idx.device)).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Embeddings\n",
    "        x = self.token_embedding(idx) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "# Create model\n",
    "config = {\n",
    "    'vocab_size': tokenizer.vocab_size,\n",
    "    'd_model': 256,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 4,\n",
    "    'd_ff': 1024,\n",
    "    'max_len': 256,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "model = GPTModel(**config).to(device)\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(f'Model size: ~{sum(p.numel() for p in model.parameters()) * 4 / 1e6:.1f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Create Data Loaders\n",
    "def get_batch(data, batch_size, block_size):\n",
    "    \"\"\"Get a batch of data for training\"\"\"\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test batch generation\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "x_batch, y_batch = get_batch(train_data, batch_size, block_size)\n",
    "print(f'Input batch shape: {x_batch.shape}')\n",
    "print(f'Target batch shape: {y_batch.shape}')\n",
    "print(f'\\nExample: first sequence (first 50 chars)')\n",
    "print(f'Input:  {tokenizer.decode(x_batch[0, :50].tolist())!r}')\n",
    "print(f'Target: {tokenizer.decode(y_batch[0, :50].tolist())!r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca58234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Training Loop (enhanced: versioned checkpoints, CSV logging, early stopping)\n",
    "from tqdm import tqdm\n",
    "import csv, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Hyperparameters\n",
    "max_iters = 5000  # adjust as needed\n",
    "learning_rate = 3e-4\n",
    "eval_interval = 250\n",
    "eval_iters = 100\n",
    "patience = 6              # number of evaluation windows without improvement before stopping\n",
    "save_every = 500          # versioned checkpoint interval\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Paths for artifacts\n",
    "cur = Path().resolve()\n",
    "ckpt_dir = cur / 'checkpoints'\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "latest_ckpt = cur / 'shakespeare_transformer.pt'\n",
    "best_ckpt = cur / 'shakespeare_transformer_best.pt'\n",
    "log_csv = cur / 'training_log.csv'\n",
    "\n",
    "# CSV logging helper\n",
    "def log_row(step, train_loss, val_loss, is_best):\n",
    "    write_header = not log_csv.exists()\n",
    "    with open(log_csv, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\n",
    "            'timestamp','step','train_loss','val_loss','is_best','lr'\n",
    "        ])\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow({\n",
    "            'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n",
    "            'step': step,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'is_best': int(is_best),\n",
    "            'lr': learning_rate\n",
    "        })\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    \"\"\"Estimate average loss on train and val sets\"\"\"\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split, data in [('train', train_data), ('val', val_data)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data, batch_size, block_size)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = float(losses.mean())\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "print('Starting enhanced training...')\n",
    "best_val = float('inf')\n",
    "no_improve_count = 0\n",
    "for iter in tqdm(range(max_iters), desc='Training'):\n",
    "    # Evaluate loss periodically\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        tr_loss = losses['train']\n",
    "        va_loss = losses['val']\n",
    "        train_losses.append(tr_loss)\n",
    "        val_losses.append(va_loss)\n",
    "        improved = va_loss < best_val - 1e-6\n",
    "        if improved:\n",
    "            best_val = va_loss\n",
    "            no_improve_count = 0\n",
    "        else:\n",
    "            no_improve_count += 1\n",
    "        log_row(iter, tr_loss, va_loss, improved)\n",
    "        print(f'\\nStep {iter}: train {tr_loss:.4f}, val {va_loss:.4f} | best {best_val:.4f} | no_improve {no_improve_count}')\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'train_loss': tr_loss,\n",
    "            'val_loss': va_loss,\n",
    "            'global_step': iter\n",
    "        }, latest_ckpt)\n",
    "        \n",
    "        # Save best checkpoint\n",
    "        if improved:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'config': config,\n",
    "                'train_loss': tr_loss,\n",
    "                'val_loss': va_loss,\n",
    "                'global_step': iter\n",
    "            }, best_ckpt)\n",
    "            print(f'âœ… New best checkpoint saved (val {va_loss:.4f})')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if no_improve_count >= patience:\n",
    "            print(f'ðŸ›‘ Early stopping triggered at step {iter} (no val improvement for {patience} evals).')\n",
    "            break\n",
    "\n",
    "    # Get batch and compute loss\n",
    "    xb, yb = get_batch(train_data, batch_size, block_size)\n",
    "    _, loss = model(xb, yb)\n",
    "    \n",
    "    # Backprop\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Versioned checkpointing\n",
    "    if (iter % save_every == 0) and iter != 0:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'global_step': iter\n",
    "        }, ckpt_dir / f'shakespeare_transformer_step{iter}.pt')\n",
    "\n",
    "print('\\nâœ… Training complete or stopped.')\n",
    "print(f'Latest checkpoint: {latest_ckpt.name}')\n",
    "print(f'Best checkpoint: {best_ckpt.name if best_ckpt.exists() else \"(none)\"}')\n",
    "print(f'Log file: {log_csv}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0457d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Visualize Training Progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "steps = [i * eval_interval for i in range(len(train_losses))]\n",
    "plt.plot(steps, train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(steps, val_losses, label='Val Loss', marker='s')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress: Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Final train loss: {train_losses[-1]:.4f}')\n",
    "print(f'Final val loss: {val_losses[-1]:.4f}')\n",
    "print(f'Loss reduction: {train_losses[0] - train_losses[-1]:.4f} ({(train_losses[0] - train_losses[-1])/train_losses[0]*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3300a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Generate Text from Trained Model\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=200, temperature=0.8):\n",
    "    \"\"\"Generate text from a prompt\"\"\"\n",
    "    model.eval()\n",
    "    context = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context if too long\n",
    "            context_crop = context if context.size(1) <= config['max_len'] else context[:, -config['max_len']:]\n",
    "            # Get predictions\n",
    "            logits, _ = model(context_crop)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            # Append to context\n",
    "            context = torch.cat([context, next_token], dim=1)\n",
    "    \n",
    "    return tokenizer.decode(context[0].tolist())\n",
    "\n",
    "# Generate samples with different prompts\n",
    "prompts = [\n",
    "    \"ROMEO:\",\n",
    "    \"To be or not to be\",\n",
    "    \"What is\"\n",
    "]\n",
    "\n",
    "print('Generated text samples:\\n')\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f'--- Sample {i}: Prompt = \"{prompt}\" ---')\n",
    "    generated = generate(model, tokenizer, prompt, max_new_tokens=150, temperature=0.8)\n",
    "    print(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b78fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Save Model and Tokenizer\n",
    "# Save model checkpoint\n",
    "model_path = cur / 'shakespeare_transformer.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': config,\n",
    "    'train_loss': train_losses[-1],\n",
    "    'val_loss': val_losses[-1]\n",
    "}, model_path)\n",
    "print(f'âœ… Model saved to {model_path}')\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "tokenizer_path = cur / 'char_tokenizer.pkl'\n",
    "with open(tokenizer_path, 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(f'âœ… Tokenizer saved to {tokenizer_path}')\n",
    "\n",
    "print(f'\\nModel size on disk: {model_path.stat().st_size / 1e6:.2f} MB')\n",
    "print(f'Tokenizer size on disk: {tokenizer_path.stat().st_size / 1e3:.2f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8ba10",
   "metadata": {},
   "source": [
    "# Exercises & Extensions\n",
    "\n",
    "## Warm-up\n",
    "\n",
    "1. **Loss Curve Analysis**: Train for 5 epochs (quick run). Plot train vs. validation loss. At what epoch does validation loss stop improving? (This is overfitting point.)\n",
    "2. **Temperature Sensitivity**: Generate text with temperatures [0.5, 1.0, 1.5, 2.0]. How does generation quality change? Lower = more conservative, higher = more random.\n",
    "3. **Convergence Speed**: Train with learning rates [1e-4, 3e-4, 1e-3]. Plot loss curves. Which converges fastest without diverging?\n",
    "\n",
    "## Intermediate\n",
    "\n",
    "4. **Embedding Analysis**: Extract the embedding matrix after training. Compute t-SNE reduction (2D or 3D). Do semantically similar characters cluster together?\n",
    "5. **Attention Head Specialization**: Extract attention weights from layer 0 and layer N-1. Visualize 2-3 heads per layer. What patterns do you see? (Early layers: local context? Later layers: rare tokens?)\n",
    "6. **Generation Length Dependence**: Generate sequences of length 50, 100, 200, 500. Does quality degrade with length? Why?\n",
    "\n",
    "## Advanced\n",
    "\n",
    "7. **Scaling Laws**: Train models with different hidden dimensions (128, 256, 384, 512). Plot final loss vs. parameter count. Does loss follow power-law scaling? (Empirically true: loss âˆ N^{-Î±})\n",
    "8. **Data Efficiency**: Train on 1%, 10%, 50%, 100% of Shakespeare. Plot final loss vs. data size. Estimate how much data is needed to reach a target loss.\n",
    "9. **Transfer Learning**: Save the best pretrained model. On a tiny downstream task (e.g., classify lines as Hamlet or Romeo), train only the output layer (freeze embeddings+transformer). Compare to random initialization.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary & Bridge Forward\n",
    "\n",
    "## What You Learned\n",
    "\n",
    "- **Causal Language Modeling**: Predict next token given previous tokens; this is the foundation of all autoregressive language models\n",
    "- **Training Loop**: Data â†’ tokenization â†’ batching â†’ forward â†’ loss â†’ backward â†’ optimize â†’ evaluate\n",
    "- **Loss Dynamics**: Training loss â†“, validation loss â†“ initially, then can â†‘ (overfitting); early stopping prevents this\n",
    "- **Generation**: Sample from model using greedy, multinomial, or nucleus sampling\n",
    "- **Scaling**: More parameters + more data = lower loss; this is the basis for scaling laws (GPT-3, GPT-4)\n",
    "- **Checkpointing**: Save best model; resume training from checkpoints; track hyperparameters and metrics\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Pretraining is the key step in modern language models:\n",
    "\n",
    "1. **Representation Learning**:\n",
    "   - Embeddings learn to encode semantic information\n",
    "   - Attention heads learn syntactic patterns, co-reference, etc.\n",
    "   - Deeper layers capture high-level concepts\n",
    "\n",
    "2. **Transfer Learning**:\n",
    "   - Pretrained models fine-tune quickly to downstream tasks\n",
    "   - Few-shot examples are often sufficient\n",
    "   - This is why companies like OpenAI invest billions in pretraining\n",
    "\n",
    "3. **Scaling Laws** (Chinchilla, Compute Optimal):\n",
    "   - Loss ~ (N^{-Î±}) Ã— (D^{-Î²}) where N = parameters, D = data tokens\n",
    "   - Optimal ratio: parameters â‰ˆ data tokens (e.g., 70B params â‰ˆ 1.4T tokens for LLaMA)\n",
    "   - Larger models enable in-context learning (few-shot, chain-of-thought)\n",
    "\n",
    "## Bridge to Next Projects\n",
    "\n",
    "- **Project 15 (Analysis)**: Compare random vs. pretrained models\n",
    "  - Pretrained: trained on billions of tokens, knows language\n",
    "  - Random: untrained weights, generates nonsense\n",
    "  - Analyze: what embeddings/attention learned during pretraining\n",
    "\n",
    "- **Project 16 (Mistral Tuning)**: Fine-tune a production pretrained model\n",
    "  - Mistral: already pretrained on 8T+ tokens\n",
    "  - Fine-tuning: train on instruction-response pairs (much smaller dataset)\n",
    "  - This is why fine-tuning is practical and affordable\n",
    "\n",
    "## Your Takeaway\n",
    "\n",
    "> **Pretraining is unsupervised learning at scale.** Next-token prediction on massive corpora teaches models language structure implicitly. This learned knowledge transfers to downstream tasks, enabling practical deployment of large language models.\n",
    "\n",
    "---\n",
    "\n",
    "# Performance Notes\n",
    "\n",
    "- **Training Time**: Character-level on 1MB text â‰ˆ 2-12 hours (CPU); 30-60 min (GPU/MPS)\n",
    "- **Batch Size**: 32-128 typical; larger batches â†’ better gradient estimates â†’ slower convergence to same loss\n",
    "- **Learning Rate**: 3e-4 is standard; 1e-5 Ã— sqrt(d_model) as rule of thumb for warm start\n",
    "- **Warmup**: 10-15% of total steps; helps early training stability\n",
    "- **Checkpointing**: Save every 1000-10000 steps to disk (can be 100MB+ for 50M param model)\n",
    "- **Generation Speed**: ~50-100 tokens/sec on CPU; 1000+ tokens/sec on GPU (depending on model size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416543b",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Summary: What We Learned\n",
    "\n",
    "### What Happened During Pretraining\n",
    "\n",
    "We successfully **pretrained a transformer from scratch** on Shakespeare's complete works (~1MB text):\n",
    "\n",
    "1. **Architecture**: 4-layer decoder-only transformer with 8 attention heads (256 dim, ~7M parameters)\n",
    "2. **Data**: Character-level tokenization (vocab size: 65) on train/val split\n",
    "3. **Training**: 5000 iterations with AdamW optimizer, learning rate 3e-4\n",
    "4. **Objective**: Next-token prediction (language modeling)\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **Loss decreased significantly** from initial ~4.0 to <2.0, showing the model learned patterns\n",
    "- **Generated text** exhibits Shakespearean style: proper character names, dialogue format, archaic language\n",
    "- The model learned:\n",
    "  - Character-level patterns (spelling, punctuation)\n",
    "  - Word boundaries and common words\n",
    "  - Shakespeare-specific vocabulary (thee, thou, thy)\n",
    "  - Dramatic structure (character names followed by colons)\n",
    "\n",
    "### Why Pretraining Matters\n",
    "\n",
    "**Pretraining is expensive but powerful** because:\n",
    "\n",
    "1. **Learns general patterns** from large unlabeled data (we used ~1M chars, real LLMs use trillions of tokens)\n",
    "2. **Captures language structure** without task-specific labels\n",
    "3. **Creates reusable representations** that transfer to downstream tasks\n",
    "4. **Enables few-shot learning** after pretraining on diverse data\n",
    "\n",
    "### Real-World Scale\n",
    "\n",
    "Our tiny experiment vs. production LLMs:\n",
    "\n",
    "| Metric | Our Model | GPT-3 | Modern LLMs |\n",
    "|--------|-----------|-------|-------------|\n",
    "| Parameters | ~7M | 175B | 7B-70B+ |\n",
    "| Training Data | ~1MB | ~500GB | ~1-10TB |\n",
    "| Training Time | ~5 min | Months | Weeks-Months |\n",
    "| Cost | $0 | ~$5M | $1M-$100M+ |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "After pretraining, you would typically:\n",
    "\n",
    "1. **Fine-tune** on task-specific data (instruction following, dialogue, etc.)\n",
    "2. **Evaluate** on benchmarks (perplexity, downstream tasks)\n",
    "3. **Align** with human preferences (RLHF, DPO)\n",
    "4. **Deploy** for inference with optimizations\n",
    "\n",
    "**Key Takeaway**: Pretraining teaches the model \"what language looks like\" so downstream tasks can focus on \"what to do with it\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75989ab",
   "metadata": {},
   "source": [
    "## ðŸ”„ Resume Training\n",
    "If you previously trained and saved a checkpoint (`shakespeare_transformer.pt`) and tokenizer (`char_tokenizer.pkl`), you can resume training from where you left off without starting over. The next cell will:\n",
    "\n",
    "1. Load the saved checkpoint and tokenizer.\n",
    "2. Rebuild the model with the original config.\n",
    "3. Continue training for a specified number of additional iterations.\n",
    "4. Append new loss values and re-save an updated checkpoint.\n",
    "\n",
    "Adjust `extra_iters` for how long you want to continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2527cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume training from checkpoint with versioned saves, CSV logging, and early stopping\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import csv, datetime\n",
    "\n",
    "cur = Path().resolve()\n",
    "ckpt_path = cur / 'shakespeare_transformer.pt'\n",
    "tok_path = cur / 'char_tokenizer.pkl'\n",
    "ckpt_dir = cur / 'checkpoints'\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "log_csv_path = cur / 'training_log.csv'\n",
    "\n",
    "assert tok_path.exists(), f\"Tokenizer not found at {tok_path}. Run tokenizer cell above first.\"\n",
    "assert ckpt_path.exists(), f\"Checkpoint not found at {ckpt_path}. Train & save first.\"\n",
    "\n",
    "# Load tokenizer\n",
    "with open(tok_path, 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Rebuild model using saved config\n",
    "ckpt = torch.load(ckpt_path, map_location=device)\n",
    "config = ckpt['config']\n",
    "model = GPTModel(**config).to(device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.train()\n",
    "\n",
    "print('Resumed model with config:', config)\n",
    "print('Params:', sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "# Recreate data tensors if needed (assumes `train_data`, `val_data` exist)\n",
    "try:\n",
    "    _ = train_data.shape, val_data.shape\n",
    "except NameError:\n",
    "    # Build from text\n",
    "    text_path = cur / 'shakespeare.txt'\n",
    "    if not text_path.exists():\n",
    "        import urllib.request\n",
    "        url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "        urllib.request.urlretrieve(url, text_path)\n",
    "    text = text_path.read_text(encoding='utf-8')\n",
    "    ids = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "    n = int(0.9 * len(ids))\n",
    "    train_data = ids[:n]\n",
    "    val_data = ids[n:]\n",
    "\n",
    "# Batch helper (reuse if already defined)\n",
    "if 'get_batch' not in globals():\n",
    "    def get_batch(data, batch_size, block_size):\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "        return x.to(device), y.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "extra_iters = 1000\n",
    "batch_size = 32\n",
    "block_size = min(config.get('max_len', 256), 256)\n",
    "learning_rate = 3e-4\n",
    "eval_interval = 200\n",
    "eval_iters = 50\n",
    "patience = 5               # early stopping patience on val loss\n",
    "save_every = 200           # save versioned checkpoint every N steps\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Logging helpers\n",
    "def append_csv(row_dict):\n",
    "    write_header = not log_csv_path.exists()\n",
    "    with open(log_csv_path, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(row_dict.keys()))\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row_dict)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(data):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data, batch_size, block_size)\n",
    "        _, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return float(losses.mean())\n",
    "\n",
    "# Initial metrics\n",
    "best_val = ckpt.get('val_loss', float('inf'))\n",
    "start_step = ckpt.get('global_step', 0)\n",
    "print(f'Starting from step={start_step}, best_val={best_val}')\n",
    "\n",
    "print('Continuing training...')\n",
    "train_curve = []\n",
    "val_curve = []\n",
    "for t in tqdm(range(extra_iters), desc='Resume'):\n",
    "    global_step = start_step + t\n",
    "\n",
    "    # Train step\n",
    "    xb, yb = get_batch(train_data, batch_size, block_size)\n",
    "    _, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Periodic eval/logging\n",
    "    if (t % eval_interval == 0) or (t == extra_iters - 1):\n",
    "        tr = estimate_loss(train_data)\n",
    "        va = estimate_loss(val_data)\n",
    "        train_curve.append(tr)\n",
    "        val_curve.append(va)\n",
    "        is_best = va < best_val - 1e-6\n",
    "        if is_best:\n",
    "            best_val = va\n",
    "\n",
    "        # CSV log\n",
    "        append_csv({\n",
    "            'timestamp': datetime.datetime.now().isoformat(timespec='seconds'),\n",
    "            'mode': 'resume',\n",
    "            'step': int(global_step),\n",
    "            'train_loss': float(tr),\n",
    "            'val_loss': float(va),\n",
    "            'is_best': int(is_best),\n",
    "            'lr': float(learning_rate)\n",
    "        })\n",
    "\n",
    "        print(f'\\n[resume] step {global_step}: train {tr:.4f} | val {va:.4f} | best {best_val:.4f}')\n",
    "\n",
    "        # Save latest and best\n",
    "        latest_payload = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'train_loss': tr,\n",
    "            'val_loss': va,\n",
    "            'global_step': global_step\n",
    "        }\n",
    "        torch.save(latest_payload, ckpt_path)\n",
    "        if is_best:\n",
    "            torch.save(latest_payload, cur / 'shakespeare_transformer_best.pt')\n",
    "\n",
    "    # Versioned checkpointing\n",
    "    if (global_step % save_every) == 0 and global_step > start_step:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'global_step': global_step\n",
    "        }, ckpt_dir / f'shakespeare_transformer_step{global_step}.pt')\n",
    "\n",
    "    # Early stopping\n",
    "    if len(val_curve) >= patience:\n",
    "        recent = val_curve[-patience:]\n",
    "        if all(v >= best_val - 1e-6 for v in recent):\n",
    "            print(f'\\nEarly stopping at step {global_step} (no val improvement for {patience} evals).')\n",
    "            break\n",
    "\n",
    "print(f'âœ… Checkpoints: latest -> {ckpt_path.name}, best -> shakespeare_transformer_best.pt, versions -> {ckpt_dir}')\n",
    "print(f'ðŸ“ˆ CSV log: {log_csv_path}')\n",
    "\n",
    "# Plot resume curves\n",
    "plt.figure(figsize=(8,4))\n",
    "steps = [start_step + i*eval_interval for i in range(len(train_curve))]\n",
    "plt.plot(steps, train_curve, label='Train (resume)', marker='o')\n",
    "plt.plot(steps, val_curve, label='Val (resume)', marker='s')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Resume Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
